<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN" "JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">DS</journal-id>
<journal-title-group><journal-title>Data Science</journal-title></journal-title-group>
<issn pub-type="epub">2451-8492</issn><issn pub-type="ppub">2451-8484</issn><issn-l>2451-8484</issn-l>
<publisher>
<publisher-name>IOS Press</publisher-name><publisher-loc>Nieuwe Hemweg 6B, 1013 BG Amsterdam, The Netherlands</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">DS200028</article-id>
<article-id pub-id-type="doi">10.3233/DS-200028</article-id>
<article-categories><subj-group subj-group-type="heading">
<subject>Review Article</subject></subj-group></article-categories>
<title-group>
<article-title>Reinforcement learning for personalization: A systematic literature review</article-title>
</title-group>
<contrib-group content-type="Editor">
<contrib contrib-type="editor">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0370-6749</contrib-id>
<name><surname>Moise</surname><given-names>Izabela</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2092-9904</contrib-id>
<name><surname>den Hengst</surname><given-names>Floris</given-names></name><xref ref-type="aff" rid="affa">a</xref><xref ref-type="corresp" rid="cor2">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5471-4338</contrib-id>
<name><surname>Grua</surname><given-names>Eoin Martino</given-names></name><xref ref-type="aff" rid="affb">b</xref><xref ref-type="fn" rid="thanks3">**</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0919-8861</contrib-id>
<name><surname>el Hassouni</surname><given-names>Ali</given-names></name><xref ref-type="aff" rid="affc">c</xref><xref ref-type="fn" rid="thanks3">**</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3356-3574</contrib-id>
<name><surname>Hoogendoorn</surname><given-names>Mark</given-names></name><xref ref-type="aff" rid="affd">d</xref><xref ref-type="fn" rid="thanks3">**</xref>
</contrib>
<aff id="affa"><label>a</label>Dept. of Computer Science, Faculty of Science, <institution>Vrije Universiteit Amsterdam</institution>, De Boelelaan 1111, 1081 HV, Amsterdam, <country>The Netherlands</country>. E-mail: <email>f.den.hengst@vu.nl</email></aff>
<aff id="affb"><label>b</label>Dept. of Computer Science, Faculty of Science, <institution>Vrije Universiteit Amsterdam</institution>, De Boelelaan 1111, 1081 HV, Amsterdam, <country>The Netherlands</country>. E-mail: <email>e.m.grua@vu.nl</email></aff>
<aff id="affc"><label>c</label>Dept. of Computer Science, Faculty of Science, <institution>Vrije Universiteit Amsterdam</institution>, De Boelelaan 1111, 1081 HV, Amsterdam, <country>The Netherlands</country>. E-mail: <email>a.el.hassouni@vu.nl</email></aff>
<aff id="affd"><label>d</label>Dept. of Computer Science, Faculty of Science, <institution>Vrije Universiteit Amsterdam</institution>, De Boelelaan 1111, 1081 HV, Amsterdam, <country>The Netherlands</country>. E-mail: <email>m.hoogendoorn@vu.nl</email></aff>
</contrib-group>
<author-notes>
<corresp id="cor2"><label>*</label>Corresponding author. E-mail: <email>f.den.hengst@vu.nl</email>.</corresp><fn id="thanks3"><label>**</label>
<p>Authors contributed equally.</p></fn>
</author-notes>
<pub-date date-type="preprint" publication-format="electronic"><day>10</day><month>4</month><year>2020</year></pub-date><pub-date date-type="pub" publication-format="electronic"><day>11</day><month>11</month><year>2020</year></pub-date><pub-date date-type="collection" publication-format="electronic"><year>2020</year></pub-date><volume>3</volume><issue>2</issue><fpage>107</fpage><lpage>147</lpage><history><date date-type="received"><day>2</day><month>02</month><year>2020</year></date><date date-type="accepted"><day>3</day><month>03</month><year>2020</year></date></history>
<permissions><copyright-statement>© 2020 – IOS Press and the authors.</copyright-statement><copyright-year>2020</copyright-year>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/" license-type="open-access" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution (CC BY 4.0) License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions>
<abstract>
<p>The major application areas of reinforcement learning (RL) have traditionally been game playing and continuous control. In recent years, however, RL has been increasingly applied in systems that interact with humans. RL can personalize digital systems to make them more relevant to individual users. Challenges in personalization settings may be different from challenges found in traditional application areas of RL. An overview of work that uses RL for personalization, however, is lacking. In this work, we introduce a framework of personalization settings and use it in a systematic literature review. Besides setting, we review solutions and evaluation strategies. Results show that RL has been increasingly applied to personalization problems and realistic evaluations have become more prevalent. RL has become sufficiently robust to apply in contexts that involve humans and the field as a whole is growing. However, it seems not to be maturing: the ratios of studies that include a comparison or a realistic evaluation are not showing upward trends and the vast majority of algorithms are used only once. This review can be used to find related work across domains, provides insights into the state of the field and identifies opportunities for future work.</p>
</abstract>
<kwd-group>
<label>Keywords</label>
<kwd>Reinforcement learning</kwd>
<kwd>contextual bandits</kwd>
<kwd>personalization</kwd>
<kwd>adaptive systems</kwd>
<kwd>recommender systems</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="x1-1000-1">
<label>1.</label>
<title>Introduction</title>
<p>For several decades, both academia and commerce have sought to develop tailored products and services at low cost in various application domains. These reach far and wide, including medicine [<xref ref-type="bibr" rid="ref005">5</xref>,<xref ref-type="bibr" rid="ref071">71</xref>,<xref ref-type="bibr" rid="ref077">77</xref>,<xref ref-type="bibr" rid="ref084">84</xref>,<xref ref-type="bibr" rid="ref178">178</xref>,<xref ref-type="bibr" rid="ref179">179</xref>], human-computer interaction [<xref ref-type="bibr" rid="ref061">61</xref>,<xref ref-type="bibr" rid="ref100">100</xref>,<xref ref-type="bibr" rid="ref118">118</xref>], product, news, music and video recommendations [<xref ref-type="bibr" rid="ref163">163</xref>,<xref ref-type="bibr" rid="ref164">164</xref>,<xref ref-type="bibr" rid="ref217">217</xref>] and even manufacturing [<xref ref-type="bibr" rid="ref039">39</xref>,<xref ref-type="bibr" rid="ref154">154</xref>]. When products and services are adapted to individual tastes, they become more appealing, desirable, informative, e.g. <italic>relevant</italic> to the intended user than one-size-fits all alternatives. Such adaptation is referred to as <italic>personalization</italic> [<xref ref-type="bibr" rid="ref055">55</xref>].</p>
<p>Digital systems enable personalization on a grand scale. The key enabler is data. While the software on these systems is identical for all users, the behavior of these systems can be tailored based on experiences with individual users. For example, Netflix’s<xref ref-type="fn" rid="fn-1">1</xref><fn id="fn-1"><label><sup>1</sup></label>
<p><uri>https://www.netflix.com</uri></p></fn> digital video delivery mechanism includes tracking of views and ratings. These ease the gratification of diverse entertainment needs as they enable Netflix to offer instantaneous personalized content recommendations. The ability to adapt system behavior to individual tastes is becoming increasingly valuable as digital systems permeate our society.</p>
<p>Recently, reinforcement learning (RL) has been attracting substantial attention as an elegant paradigm for personalization based on data. For any particular environment or user state, this technique strives to determine the sequence of actions to maximize a reward. These actions are not necessarily selected to yield the highest reward <italic>now</italic>, but are typically selected to achieve a high reward in the long term. Returning to the Netflix example, the company may not be interested in having a user watch a single recommended video instantly, but rather aim for users to prolong their subscription after having enjoyed many recommended videos. Besides the focus on long-term goals in RL, rewards can be formulated in terms of user feedback so that no explicit definition of desired behavior is required [<xref ref-type="bibr" rid="ref012">12</xref>,<xref ref-type="bibr" rid="ref079">79</xref>].</p>
<p>RL has seen successful applications to personalization in a wide variety of domains. Some of the earliest work, such as [<xref ref-type="bibr" rid="ref122">122</xref>,<xref ref-type="bibr" rid="ref174">174</xref>,<xref ref-type="bibr" rid="ref175">175</xref>] and [<xref ref-type="bibr" rid="ref231">231</xref>] focused on web services. More recently, [<xref ref-type="bibr" rid="ref107">107</xref>] showed that adding personalization to an existing online news recommendation engine increased click-through rates by 12.5%. Applications are not limited to web services, however. As an example from the health domain, [<xref ref-type="bibr" rid="ref234">234</xref>] achieve optimal per-patient treatment plans to address advanced metastatic stage IIIB/IV non-small cell lung cancer in simulation. They state that ‘there is significant potential of the proposed methodology for developing personalized treatment strategies in other cancers, in cystic fibrosis, and in other life-threatening diseases’. An early example of tailoring intelligent tutor behavior using RL can be found in [<xref ref-type="bibr" rid="ref124">124</xref>]. A more recent example in this domain, [<xref ref-type="bibr" rid="ref074">74</xref>], compared the effect of personalized and non-personalized affective feedback in language learning with a social robot for children and found that personalization significantly impacts psychological valence.</p>
<p>Although the aforementioned applications span various domains, they are similar in solution: they all use traits of users to achieve personalization, and all rely on implicit feedback from users. Furthermore, the use of RL in contexts that involve humans poses challenges unique to this setting. In traditional RL subfields such as game-playing and robotics, for example, simulators can be used for rapid prototyping and <italic>in-silico</italic> benchmarks are well established [<xref ref-type="bibr" rid="ref014">14</xref>,<xref ref-type="bibr" rid="ref021">21</xref>,<xref ref-type="bibr" rid="ref050">50</xref>,<xref ref-type="bibr" rid="ref097">97</xref>]. Contexts with humans, however, may be much harder to simulate and the deployment of autonomous agents in these contexts may come with different concerns regarding for example safety. When using RL for a personalization problem, similar issues may arise across different application domains. An overview of RL for personalization across domains, however, is lacking. We believe this is not to be attributed to fundamental differences in setting, solution or methodology, but stems from application domains working in isolation for cultural and historical reasons.</p>
<p>This paper provides an overview and categorization of RL applications for personalization across a variety of application domains. It thus aids researchers and practictioners in identifying related work relevant to a specific personalization setting, promotes the understanding of how RL is used for personalization and identifies challenges across domains. We first provide a brief introduction of the RL framework and formally introduce how it can be used for personalization. We then present a framework to classify personalization settings by. The purpose of this framework is for researchers with a specific setting to identify relevant related work across domains. We then use this framework in a systematic literature review (SLR). We investigate in which settings RL is used, which solutions are common and how they are evaluated: Section <xref rid="x1-18000-5">5</xref> details the SLR protocol, results and analysis are described in Section <xref rid="x1-24000-6">6</xref>. All data collected has been made available digitally [<xref ref-type="bibr" rid="ref046">46</xref>]. Finally, we conclude with current trends challenges in Section <xref rid="x1-28000-7">7</xref>.</p>
</sec>
<sec id="x1-2000-2">
<label>2.</label>
<title>Reinforcement learning for personalization</title>
<p>RL considers problems in the framework of <italic>Markov decision processes</italic> or MDPs. In this framework, an agent collects rewards over time by performing actions in an environment as depicted in Fig. <xref rid="x1-2001-1">1</xref>. The goal of the agent is to maximize the total amount of collected rewards over time. In this section, we formally introduce the core concepts of MDPs and RL and include some strategies to personalization without aiming to provide an in depth introduction to RL. Following [<xref ref-type="bibr" rid="ref188">188</xref>], we consider the related <italic>multi-armed</italic> and <italic>contextual bandit</italic> problems as special cases of the full RL problem where actions do not affect the environment and where observations of the environment are absent or present respectively. We refer the reader to [<xref ref-type="bibr" rid="ref188">188</xref>,<xref ref-type="bibr" rid="ref220">220</xref>] and [<xref ref-type="bibr" rid="ref190">190</xref>] for a full introduction.</p>
<fig id="x1-2001-1">
<label>Fig. 1.</label>
<caption>
<p>The agent-environment in RL for personalization from [<xref ref-type="bibr" rid="ref188">188</xref>].</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g001.jpg"/>
</fig>
<p>An MDP is defined as a tuple <inline-formula><mml:math id="math001">
<mml:mo fence="true" stretchy="false">⟨</mml:mo>
<mml:mi mathvariant="italic">S</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">A</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">T</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">R</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">γ</mml:mi>
<mml:mo fence="true" stretchy="false">⟩</mml:mo></mml:math></inline-formula> where <inline-formula><mml:math id="math002">
<mml:mi mathvariant="italic">S</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="true" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">}</mml:mo></mml:math></inline-formula> is a finite set of states, <inline-formula><mml:math id="math003">
<mml:mi mathvariant="italic">A</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="true" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">m</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">}</mml:mo></mml:math></inline-formula> a finite set of system actions, <inline-formula><mml:math id="math004">
<mml:mi mathvariant="italic">T</mml:mi>
<mml:mo>:</mml:mo>
<mml:mi mathvariant="italic">S</mml:mi>
<mml:mo>×</mml:mo>
<mml:mi mathvariant="italic">A</mml:mi>
<mml:mo>×</mml:mo>
<mml:mi mathvariant="italic">S</mml:mi>
<mml:mo stretchy="false">→</mml:mo>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo fence="true" stretchy="false">]</mml:mo></mml:math></inline-formula> a probabilistic transition function, <inline-formula><mml:math id="math005">
<mml:mi mathvariant="italic">R</mml:mi>
<mml:mo>:</mml:mo>
<mml:mi mathvariant="italic">S</mml:mi>
<mml:mo>×</mml:mo>
<mml:mi mathvariant="italic">A</mml:mi>
<mml:mo stretchy="false">→</mml:mo>
<mml:mi mathvariant="double-struck">R</mml:mi></mml:math></inline-formula> a reward function and <inline-formula><mml:math id="math006">
<mml:mi mathvariant="italic">γ</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo fence="true" stretchy="false">]</mml:mo></mml:math></inline-formula> a factor to discount future rewards. At each time step <italic>t</italic>, the system is confronted with some state <inline-formula><mml:math id="math007">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula>, performs some action <inline-formula><mml:math id="math008">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> which yields a reward <inline-formula><mml:math id="math009">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>:</mml:mo>
<mml:mi mathvariant="italic">R</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> and some state <inline-formula><mml:math id="math010">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub></mml:math></inline-formula> following the probability distribution <inline-formula><mml:math id="math011">
<mml:mi mathvariant="italic">T</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula>. A series of these states, actions and rewards from the onset to some terminal state <italic>T</italic> is called a trajectory <inline-formula><mml:math id="math012">
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mi mathvariant="italic">r</mml:mi>
<mml:mo>:</mml:mo>
<mml:mo fence="true" stretchy="false">⟨</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">⟩</mml:mo></mml:math></inline-formula>. These trajectories typically contain the interaction histories for users with the system. A single trajectory can describe a single session of the user interacting with the system or can contain many different separate sessions. Multiple trajectories may be available in a data set <inline-formula><mml:math id="math013">
<mml:mi mathvariant="italic">D</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="true" stretchy="false">{</mml:mo>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ℓ</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">}</mml:mo></mml:math></inline-formula>. The goal is to find a policy <inline-formula><mml:math id="math014">
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup></mml:math></inline-formula> out of all <inline-formula><mml:math id="math015">
<mml:mi mathvariant="normal">Π</mml:mi>
<mml:mo>:</mml:mo>
<mml:mi mathvariant="italic">S</mml:mi>
<mml:mo>×</mml:mo>
<mml:mi mathvariant="italic">A</mml:mi>
<mml:mo stretchy="false">→</mml:mo>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo fence="true" stretchy="false">]</mml:mo></mml:math></inline-formula> that maximizes the sum of future rewards at any <italic>t</italic>, given an end time <italic>T</italic>: 
<disp-formula>
<mml:math display="block" id="math016">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(1)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">G</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>:</mml:mo>
<mml:munderover accentunder="false" accent="false">
<mml:mrow>
<mml:mstyle displaystyle="true">
<mml:mo largeop="true" movablelimits="false">∑</mml:mo></mml:mstyle>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">k</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:munderover>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">γ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">k</mml:mi>
<mml:mo>−</mml:mo>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msup>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">k</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula> 
If some expectation <inline-formula><mml:math id="math017">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="double-struck">E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> over the future reward for some policy <italic>π</italic> can be formulated, a value can be assigned to some state <italic>s</italic> given that policy: 
<disp-formula>
<mml:math display="block" id="math018">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(2)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="double-struck">E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">G</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo fence="true" stretchy="false">]</mml:mo>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula> 
Similarly, a value can be assigned to an action <italic>a</italic> in a state <italic>s</italic>: 
<disp-formula>
<mml:math display="block" id="math019">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(3)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="double-struck">E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">G</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo fence="true" stretchy="false">]</mml:mo>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula> 
Now the optimal policy <inline-formula><mml:math id="math020">
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup></mml:math></inline-formula> should satisfy <inline-formula><mml:math id="math021">
<mml:mo>∀</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi mathvariant="italic">S</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mo>∀</mml:mo>
<mml:mi mathvariant="italic">π</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi mathvariant="normal">Π</mml:mi>
<mml:mo>:</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>⩾</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="math022">
<mml:mo>∀</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi mathvariant="italic">S</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi mathvariant="italic">A</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mo>∀</mml:mo>
<mml:mi mathvariant="italic">π</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi mathvariant="normal">Π</mml:mi>
<mml:mo>:</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>⩾</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula>. Assuming a suitable <inline-formula><mml:math id="math023">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="double-struck">E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:mi mathvariant="italic">G</mml:mi>
<mml:mo fence="true" stretchy="false">]</mml:mo></mml:math></inline-formula>, <inline-formula><mml:math id="math024">
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup></mml:math></inline-formula> consists of selecting the action that is expected to yield the highest sum of rewards: 
<disp-formula>
<mml:math display="block" id="math025">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd id="x1-2005-4">
<mml:mtext>(4)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:munder>
<mml:mrow>
<mml:mo movablelimits="false">arg</mml:mo>
<mml:mo movablelimits="false">max</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:munder>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mspace width="1em"/>
<mml:mo>∀</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi mathvariant="italic">S</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi mathvariant="italic">A</mml:mi>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula>
</p>
<p>With these definitions in place, we now turn to methods of finding <inline-formula><mml:math id="math026">
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup></mml:math></inline-formula>. Such methods can be categorized by considering which elements of the MDP are known. Generally, <italic>S</italic>, <italic>A</italic> and <italic>γ</italic> are determined upfront and known. <italic>T</italic> and <italic>R</italic>, on the other hand, may or may not be known. If they are both known, the expectation <inline-formula><mml:math id="math027">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="double-struck">E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:mi mathvariant="italic">G</mml:mi>
<mml:mo fence="true" stretchy="false">]</mml:mo></mml:math></inline-formula> is directly available and a corresponding <inline-formula><mml:math id="math028">
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup></mml:math></inline-formula> can be found analytically. In some settings, however, <italic>T</italic> and <italic>R</italic> may be unknown and <inline-formula><mml:math id="math029">
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup></mml:math></inline-formula> must be found empirically. This can be done by estimating <italic>T</italic>, <italic>R</italic>, <italic>V</italic>, <italic>Q</italic> and finally <inline-formula><mml:math id="math030">
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup></mml:math></inline-formula> or a combination thereof using data set <italic>D</italic>. Thus, if we include approximations in Eq. (<xref rid="x1-2005-4">4</xref>), we get: 
<disp-formula>
<mml:math display="block" id="math031">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd id="x1-2006-5">
<mml:mtext>(5)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:mover accent="true">
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi mathvariant="italic">D</mml:mi>
<mml:mo>=</mml:mo>
<mml:munder>
<mml:mrow>
<mml:mo movablelimits="false">arg</mml:mo>
<mml:mo movablelimits="false">max</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:munder>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi mathvariant="italic">D</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mspace width="1em"/>
<mml:mo>∀</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi mathvariant="italic">S</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi mathvariant="italic">A</mml:mi>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula> 
As <italic>D</italic> may lack the required trajectories for a reasonable <inline-formula><mml:math id="math032">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="double-struck">E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:mi mathvariant="italic">G</mml:mi>
<mml:mo fence="true" stretchy="false">]</mml:mo></mml:math></inline-formula> and may even be empty initially, <italic>exploratory</italic> actions can be selected to enrich <italic>D</italic>. Such actions need not follow <inline-formula><mml:math id="math033"><mml:mover accent="true">
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover></mml:math></inline-formula> as in Eq. (<xref rid="x1-2006-5">5</xref>) but may be selected through some other mechanism such as sampling from the full action set <italic>A</italic> randomly.</p>
<p>Having introduced RL briefly, we continue by exploring some strategies in applying this framework to the problem of personalizing systems. We return to our earlier example of a video recommendation task and consider a set of <italic>n</italic> users <inline-formula><mml:math id="math034">
<mml:mi mathvariant="italic">U</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="true" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">u</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">u</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">}</mml:mo></mml:math></inline-formula>. A first way to adapt software systems to an individual users’ needs is to define a separate environment, corresponding MDP and RL agent for each user. The overall goal becomes to find a set of optimal policies <inline-formula><mml:math id="math035">
<mml:mo fence="true" stretchy="false">{</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo fence="true" stretchy="false">}</mml:mo></mml:math></inline-formula> for a set of environments formalized as MDPs <inline-formula><mml:math id="math036">
<mml:mi mathvariant="italic">M</mml:mi>
<mml:mo>:</mml:mo>
<mml:mo fence="true" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>:</mml:mo>
<mml:mo fence="true" stretchy="false">⟨</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">S</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">A</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">γ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">⟩</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>:</mml:mo>
<mml:mo fence="true" stretchy="false">⟨</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">S</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">A</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">γ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">⟩</mml:mo>
<mml:mo fence="true" stretchy="false">}</mml:mo></mml:math></inline-formula>. In the case of approximations as in Eq. (<xref rid="x1-2006-5">5</xref>), these are made per MDP based on data set <inline-formula><mml:math id="math037">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> with trajectories only involving that environment. In the running example, videos would be recommended to a user based on previous video recommendations and selections of that particular user. The benefit of isolated MDPs is that differences between <inline-formula><mml:math id="math038">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="math039">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> or between <inline-formula><mml:math id="math040">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="math041">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> for MDPs <inline-formula><mml:math id="math042">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">≠</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> are handled naturally, e.g. such differences do not make <inline-formula><mml:math id="math043">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="double-struck">E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:mi mathvariant="italic">G</mml:mi>
<mml:mo fence="true" stretchy="false">]</mml:mo></mml:math></inline-formula> incorrect. On the other hand, similarities between <inline-formula><mml:math id="math044">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="math045">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> cannot be used. For example, consider a video recommendation task with <inline-formula><mml:math id="math046">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">S</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mo fence="true" stretchy="false">{</mml:mo>
<mml:mi mathvariant="italic">morning</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">afternoon</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">night</mml:mi>
<mml:mo fence="true" stretchy="false">}</mml:mo></mml:math></inline-formula>. If two users <inline-formula><mml:math id="math047">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">u</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">≠</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">u</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> are both using a video service in the <inline-formula><mml:math id="math048">
<mml:mi mathvariant="italic">morning</mml:mi></mml:math></inline-formula> state, they may both like to watch a breakfast news broadcast whereas in the <inline-formula><mml:math id="math049">
<mml:mi mathvariant="italic">night</mml:mi></mml:math></inline-formula> state they may both prefer a talk show. Learning such patterns for each environment individually may require a substantial number of trajectories and may be infeasible in some settings, such as those where users cannot be identified across trajectories or those where each user is expected to contribute only one trajectory to <inline-formula><mml:math id="math050">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula>.</p>
<p>An alternative approach is to define is a single agent and MDP with user-specific information in the state space <italic>S</italic> and learn a single <inline-formula><mml:math id="math051">
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup></mml:math></inline-formula> for all users [<xref ref-type="bibr" rid="ref047">47</xref>]. In some settings, users can be described using a function that returns a vector representation of the <italic>l</italic> features that characterize a user <inline-formula><mml:math id="math052">
<mml:mi mathvariant="italic">ϕ</mml:mi>
<mml:mo>:</mml:mo>
<mml:mi mathvariant="italic">U</mml:mi>
<mml:mo stretchy="false">→</mml:mo>
<mml:mo fence="true" stretchy="false">⟨</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">U</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">l</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">U</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo fence="true" stretchy="false">⟩</mml:mo></mml:math></inline-formula>. Such a vector could for example contain age, favourite genre and viewing history. If two users <inline-formula><mml:math id="math053">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">u</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">≠</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">u</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> have both enjoyed the first “Lord of the Rings” movie and viewer <inline-formula><mml:math id="math054">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">u</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> has followed up on a recommendation of its sequel by the system then this sequel may be a suitable recommendation for the other viewer <inline-formula><mml:math id="math055">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">u</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> as well. Generally, this approach can be valuable when it is unclear which elements of trajectories of users <inline-formula><mml:math id="math056">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">u</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> should be used in determining <inline-formula><mml:math id="math057">
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msubsup></mml:math></inline-formula>. Conceptually, finding <inline-formula><mml:math id="math058">
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup></mml:math></inline-formula> now includes determining <inline-formula><mml:math id="math059">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">u</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula>’s preference for actions given a state and determining the relationship between user preferences. This approach should therefore be able to overcome the negative transfer problem described below when enough trajectories are available. The growth in state space size, on the other hand, may require an exorbitant number of trajectories in <italic>D</italic> due to the curse of dimensionality [<xref ref-type="bibr" rid="ref015">15</xref>]. Thus, <italic>ϕ</italic> is to be carefully designed or dimensionality reduction techniques are to be used in approaches following this strategy. As a closing remark on this approach to personalization, we note that the distinction between task-related and user-specific information is somewhat artificial as <italic>S</italic> may already contain <inline-formula><mml:math id="math060">
<mml:mi mathvariant="italic">ϕ</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">U</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> in many practical settings and we stress that the distinction is made for illustrative purposes here.</p>
<p>A third category of approaches can be considered as a middle ground between learning a single <inline-formula><mml:math id="math061">
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup></mml:math></inline-formula> and learning a <inline-formula><mml:math id="math062">
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msubsup></mml:math></inline-formula> per user. It is motivated by the idea that users and corresponding environments may be similar. If this is the case, then trajectories <inline-formula><mml:math id="math063">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> from some similar environment <inline-formula><mml:math id="math064">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">≠</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> may prove useful in estimating <inline-formula><mml:math id="math065">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="double-struck">E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:mi mathvariant="italic">G</mml:mi>
<mml:mo fence="true" stretchy="false">]</mml:mo></mml:math></inline-formula>. One such an approach is based on clustering [<xref ref-type="bibr" rid="ref054">54</xref>,<xref ref-type="bibr" rid="ref075">75</xref>,<xref ref-type="bibr" rid="ref124">124</xref>,<xref ref-type="bibr" rid="ref191">191</xref>]. Formally, it requires <inline-formula><mml:math id="math066">
<mml:mi mathvariant="italic">q</mml:mi>
<mml:mo>⩽</mml:mo>
<mml:mi mathvariant="italic">n</mml:mi></mml:math></inline-formula> groups <inline-formula><mml:math id="math067">
<mml:mi mathvariant="italic">G</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="true" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">g</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">g</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">q</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">}</mml:mo></mml:math></inline-formula> and a mapping function <inline-formula><mml:math id="math068">
<mml:mi mathvariant="normal">Φ</mml:mi>
<mml:mo>:</mml:mo>
<mml:mi mathvariant="italic">M</mml:mi>
<mml:mo stretchy="false">→</mml:mo>
<mml:mi mathvariant="italic">G</mml:mi></mml:math></inline-formula>. In practice, this mapping function is typically defined on the level of users <italic>U</italic> or the feature representation <inline-formula><mml:math id="math069">
<mml:mi mathvariant="italic">ϕ</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">U</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula>. An RL agent is defined for every <inline-formula><mml:math id="math070">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">g</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">p</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> and interacts with all environments <inline-formula><mml:math id="math071">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="normal">Φ</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="normal">Φ</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">g</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">p</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula>. Trajectories in <inline-formula><mml:math id="math072">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="math073">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> are concatenated or <italic>pooled</italic> to form a single <inline-formula><mml:math id="math074">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">p</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> which is used to approximate <inline-formula><mml:math id="math075">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="double-struck">E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">p</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:mi mathvariant="italic">G</mml:mi>
<mml:mo fence="true" stretchy="false">]</mml:mo></mml:math></inline-formula> for all <inline-formula><mml:math id="math076">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula>. A combined <inline-formula><mml:math id="math077">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">p</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> may be orders of magnitude bigger than an isolated <inline-formula><mml:math id="math078">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula>, which may result in a much better approximation <inline-formula><mml:math id="math079">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="double-struck">E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">p</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:mi mathvariant="italic">G</mml:mi>
<mml:mo fence="true" stretchy="false">]</mml:mo>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">p</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> and a resulting <inline-formula><mml:math id="math080"><mml:mover accent="true">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">p</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">p</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> that yields a higher reward in all environments. For example, users of the video recommendation service may be clustered by age and users in the ‘infant’ cluster may generally prefer children’s movies over history documentaries. A related approach similarly uses trajectories <inline-formula><mml:math id="math081">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> of other environments <inline-formula><mml:math id="math082">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">≠</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> but still aims to find environment-specific <inline-formula><mml:math id="math083">
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msubsup></mml:math></inline-formula>. Trajectories in <inline-formula><mml:math id="math084">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> are weighted during estimation of <inline-formula><mml:math id="math085">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="double-struck">E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:mi mathvariant="italic">G</mml:mi>
<mml:mo fence="true" stretchy="false">]</mml:mo></mml:math></inline-formula> using some weighting scheme. This can be understood as a generalization of the pooling approach. First, recall that <inline-formula><mml:math id="math086">
<mml:mi mathvariant="normal">Φ</mml:mi>
<mml:mo>:</mml:mo>
<mml:mi mathvariant="italic">M</mml:mi>
<mml:mo stretchy="false">→</mml:mo>
<mml:mi mathvariant="italic">G</mml:mi></mml:math></inline-formula> for the pooling approach and note that it can be rewritten to <inline-formula><mml:math id="math087">
<mml:mi mathvariant="normal">Φ</mml:mi>
<mml:mo>:</mml:mo>
<mml:mi mathvariant="italic">M</mml:mi>
<mml:mo>×</mml:mo>
<mml:mi mathvariant="italic">M</mml:mi>
<mml:mo stretchy="false">→</mml:mo>
<mml:mo fence="true" stretchy="false">{</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo fence="true" stretchy="false">}</mml:mo></mml:math></inline-formula>. The weighting scheme, now, is a generalization where <inline-formula><mml:math id="math088">
<mml:mi mathvariant="normal">Φ</mml:mi>
<mml:mo>:</mml:mo>
<mml:mi mathvariant="italic">M</mml:mi>
<mml:mo>×</mml:mo>
<mml:mi mathvariant="italic">M</mml:mi>
<mml:mo stretchy="false">→</mml:mo>
<mml:mi mathvariant="double-struck">R</mml:mi></mml:math></inline-formula>. Finding a suitable Φ can be challenging in itself and depends on the availability of user features, trajectories and the task at hand. Typical strategies are to define Φ in terms of similarity of feature representations of users <inline-formula><mml:math id="math089">
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:mi mathvariant="italic">ϕ</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">u</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">ϕ</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">u</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo fence="true" stretchy="false">]</mml:mo></mml:math></inline-formula> or similarity of <inline-formula><mml:math id="math090">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula>. The two previous approaches work under the assumption that <inline-formula><mml:math id="math091">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="math092">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> are similar and that Φ is suitable. If either of these assumptions is not met, pooling data may result in a policy that is suboptimal for both <inline-formula><mml:math id="math093">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="math094">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula>. This phenomenon is typically referred to as the <italic>negative transfer problem</italic> [<xref ref-type="bibr" rid="ref146">146</xref>].</p>
</sec>
<sec id="x1-3000-3">
<label>3.</label>
<title>Algorithms</title>
<p>In this section we provide an overview of specific RL techniques and algorithms used for personalization. This overview is the result of our systematic literature review as can be seen in Table <xref rid="x1-26003-4">4</xref>. Figure <xref rid="x1-3001-2">2</xref> contains a diagram of the discussed techniques. We start with a subset of the full RL problem known as <italic>k</italic>-armed bandits. We bridge the gap towards the full RL setting with contextual bandits approaches. Then, value-based and policy-gradient RL methods are discussed.</p>
<fig id="x1-3001-2">
<label>Fig. 2.</label>
<caption>
<p>Overview of types of RL algorithms discussed in this section and the number of uses in publications included in this survey. See Table <xref rid="x1-26003-4">4</xref> for a list of all (families of) algorithms used by more than one publication.</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g002.jpg"/>
</fig>
<sec id="x1-4000-3.1">
<label>3.1.</label>
<title>Multi-armed bandits</title>
<p>Multi-armed bandits is a simplified setting of RL. As a result, it is often used to introduce basic learning methods that can be extended to full RL algorithms [<xref ref-type="bibr" rid="ref188">188</xref>]. In the non-associative setting, the objective is to learn how to act optimally in a single situation. Formally, this setting is equivalent to an MDP with a single state. In the associative or <italic>contextual</italic> version of this setting, actions are taken in more than one situation. This setting is closer to the full RL problem yet it lacks an important trait of full RL, namely that the selected action affects the situation. Both associative and non-associative multi-armed bandit approaches do not take into account temporal separation of actions and related rewards.</p>
<p>In general, multi-armed bandit solutions are not suitable when success is achieved by sequences of actions. Non-associative <italic>k</italic>-armed bandits solutions are only applicable when context is not important. This makes them generally unsuitable for personalizaton as it typically utilizes different personal contexts for different users by offering a different functionality. In some niche areas, however, <italic>k</italic>-armed bandits are applicable and can be very attractive due to formal guarantees on their performance. If context is of importance, contextual bandit approaches provide a good starting point for personalizing an application. These approaches hold a middle ground between non-associative multi-armed bandits and full RL solutions in terms of modeling power and ease of implementation. Their theoretical guarantees on optimality are less strong than their <italic>k</italic>-armed counterparts but they are easier to implement, evaluate and maintain than full RL solutions.</p>
<sec id="x1-5000-3.1.1">
<label>3.1.1.</label>
<title><italic>k</italic>-Armed bandits</title>
<p>In a <italic>k</italic>-armed bandit setting, one is constantly faced with the choice between <italic>k</italic> different actions [<xref ref-type="bibr" rid="ref188">188</xref>]. Depending on the selected action, a scalar reward is obtained. This reward is drawn from a stationary probability distribution. It is assumed that an independent probability distribution exists for every action. The goal is to maximize the expected total reward over a certain period of time. Still considering the <italic>k</italic>-armed bandit setting, we assign a value <inline-formula><mml:math id="math095">
<mml:mi mathvariant="italic">Q</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> to each of the <italic>k</italic> actions and define this value as the expected reward given that the action was selected. The expected reward given that an action a is selected is defined as follows: 
<disp-formula>
<mml:math display="block" id="math096">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(6)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:mi mathvariant="italic">Q</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="double-struck">E</mml:mi>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo fence="true" stretchy="false">]</mml:mo>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula>
</p>
<p>In a trivial problem setting, one knows the exact value of each action and selecting the action with the highest value would constitute the optimal policy. In more realistic problems, it is fair to assume that one cannot know the values of the actions exactly. In this case, one can estimate the value of an action. We denote this estimated value with <inline-formula><mml:math id="math097"><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> and our goal is to have estimate <inline-formula><mml:math id="math098"><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> as close to the true <inline-formula><mml:math id="math099">
<mml:mi mathvariant="italic">Q</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> as possible.</p>
<p>At each time step <italic>t</italic>, estimates of the values of actions are obtained. Always selecting the actions with the highest estimated value is called greedy action selection. In this case we are exploiting the knowledge we have built about the values of the actions. When we select actions with a lower expected value, we say we are exploring. In this case we are improving the estimates of values for these actions. In the balancing act of exploration and exploitation, we opt for exploitation to maximize the expected total reward for the next step, while opting for exploration could results in higher expected total reward in the long run.</p>
</sec>
<sec id="x1-6000-3.1.2">
<label>3.1.2.</label>
<title>Action-value methods for multi-armed bandits</title>
<p>Action-value methods [<xref ref-type="bibr" rid="ref188">188</xref>] denote a collections of methods used for estimating the values of actions. The most natural way of estimating the action-values is to average the rewards that were observed. This method is called the sample-average method. The value estimate <inline-formula><mml:math id="math100">
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> is then defined as: 
<disp-formula>
<mml:math display="block" id="math101">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(7)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo><mml:mstyle displaystyle="true">
<mml:mfrac>
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mo largeop="false" movablelimits="false">∑</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>·</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mn mathvariant="double-struck">1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mo largeop="false" movablelimits="false">∑</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:msub>
<mml:mrow>
<mml:mn mathvariant="double-struck">1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfrac>
</mml:mstyle>
<mml:mo mathvariant="normal">,</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula> 
where <inline-formula><mml:math id="math102">
<mml:msub>
<mml:mrow>
<mml:mn mathvariant="double-struck">1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> is 1 when <inline-formula><mml:math id="math103">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi></mml:math></inline-formula> is true and 0 otherwise. A default value is assigned to <inline-formula><mml:math id="math104"><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> when the denominator is zero. As the denominator approaches infinity, the estimate <inline-formula><mml:math id="math105"><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> converges to the true <inline-formula><mml:math id="math106">
<mml:mi mathvariant="italic">Q</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula>. Again, the most basic way of selecting actions is the greedy action selection method. Here the action with the highest value is selected. In the case of a tie, one action is selected using tie-breaking methods such as random selection. Greedy action selection is defined as follows for any time point <italic>t</italic>: 
<disp-formula>
<mml:math display="block" id="math107">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(8)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:munder>
<mml:mrow>
<mml:mo movablelimits="false">arg</mml:mo>
<mml:mo movablelimits="false">max</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:munder><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula>
</p>
<p>Greedy action selection only exploits knowledge built up using the action-value method and only maximizes the immediate reward. This can lead to incorrect action-value approximations because actions with e.g. low <italic>estimated</italic> but high <italic>actual</italic> values are not sampled. An improvement over this greedy action selection is to randomly explore with a small probability <italic>ϵ</italic>. This method is named the <italic>ϵ</italic>-greedy action selection. A benefit of this method is that, while it is relatively simple, in the limit <inline-formula><mml:math id="math108"><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> will converge to <inline-formula><mml:math id="math109">
<mml:mi mathvariant="italic">Q</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> [<xref ref-type="bibr" rid="ref188">188</xref>]. This indicates that the probability of selecting the optimal action is then greater than <inline-formula><mml:math id="math110">
<mml:mn>1</mml:mn>
<mml:mo>−</mml:mo>
<mml:mi mathvariant="italic">ϵ</mml:mi></mml:math></inline-formula> which is near certainty.</p>
</sec>
<sec id="x1-7000-3.1.3">
<label>3.1.3.</label>
<title>Incremental implementation</title>
<p>In Section <xref rid="x1-6000-3.1.2">3.1.2</xref> we discussed a method to estimate action-values using sample-averaging. To ensure the usability of these method in real-world applications, we need to be able to compute these values in an efficient way. Assume a setting with one action. At each iteration <italic>j</italic> a reward <inline-formula><mml:math id="math111">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub></mml:math></inline-formula> is obtained after selecting an action. Let <inline-formula><mml:math id="math112">
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> denote the estimate value of the action after <inline-formula><mml:math id="math113">
<mml:mi mathvariant="italic">n</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn></mml:math></inline-formula> iterations. We can then define: 
<disp-formula>
<mml:math display="block" id="math114">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(9)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo><mml:mstyle displaystyle="true">
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>3</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mo stretchy="false">⋯</mml:mo>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:mfrac>
</mml:mstyle>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula>
</p>
<p>Using this approach would mean storing the values of all the rewards to recalculate <inline-formula><mml:math id="math115">
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> from scratch at every iteration. There is however a more efficient way for calculating <inline-formula><mml:math id="math116">
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> that is constant in memory and computation time. Rewriting it yields the following update rule: 
<disp-formula>
<mml:math display="block" id="math117">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(10)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo><mml:mstyle displaystyle="true">
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:mfrac>
</mml:mstyle>
<mml:mo fence="true" maxsize="1.19em" minsize="1.19em">[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo fence="true" maxsize="1.19em" minsize="1.19em">]</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula> 
where the term <inline-formula><mml:math id="math118">
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> represents the old estimate, <inline-formula><mml:math id="math119">
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo fence="true" stretchy="false">]</mml:mo></mml:math></inline-formula> the error in the estimate we made of the reward and <inline-formula><mml:math id="math120"><mml:mstyle displaystyle="false">
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:mfrac>
</mml:mstyle></mml:math></inline-formula> the learning rate.</p>
</sec>
<sec id="x1-8000-3.1.4">
<label>3.1.4.</label>
<title>UCB: Upper-confidence bound</title>
<p>The greedy and <italic>ϵ</italic>-greedy action selection methods were discussed in Section <xref rid="x1-6000-3.1.2">3.1.2</xref> and it was introduced that exploration is required to establish good action-value estimates. Although <italic>ϵ</italic>-greedy explores all actions eventually, it does so randomly. A better way of exploration would take into account the action-value’s proximity to the optimal value and the uncertainty in the value estimations. Intuitively, we want a selected action <italic>a</italic> to either provide a good immediate reward or else some very useful information in updating <inline-formula><mml:math id="math121"><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula>. An approach that uses this idea is the upper confidence bound action selection (UCB) method [<xref ref-type="bibr" rid="ref007">7</xref>,<xref ref-type="bibr" rid="ref065">65</xref>,<xref ref-type="bibr" rid="ref188">188</xref>]. UCB is defined as follows at time step <italic>t</italic>: 
<disp-formula>
<mml:math display="block" id="math122">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(11)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:munder>
<mml:mrow>
<mml:mo movablelimits="false">arg</mml:mo>
<mml:mo movablelimits="false">max</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:munder>
<mml:mo fence="true" maxsize="2.03em" minsize="2.03em">[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="italic">c</mml:mi>
<mml:mo>·</mml:mo>
<mml:msqrt>
<mml:mrow>
<mml:mstyle displaystyle="true">
<mml:mfrac>
<mml:mrow>
<mml:mo movablelimits="false">ln</mml:mo>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">N</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfrac>
</mml:mstyle>
</mml:mrow>
</mml:msqrt>
<mml:mo fence="true" maxsize="2.03em" minsize="2.03em">]</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula> 
where <inline-formula><mml:math id="math123">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">N</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> is how often action <italic>a</italic> was chosen up to time <italic>t</italic> and <inline-formula><mml:math id="math124">
<mml:mi mathvariant="italic">c</mml:mi>
<mml:mo mathvariant="normal">&gt;</mml:mo>
<mml:mn>0</mml:mn></mml:math></inline-formula> is a parameter to control the rate of exploration. The square root term denotes the level of uncertainty in the approximation of the value of action <italic>a</italic>. Hence, UCB provides an upper bound for the true value of the action <italic>a</italic>. Here, <italic>c</italic> is used to define the confidence level. When the action <italic>a</italic> is selected often, <inline-formula><mml:math id="math125">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">N</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> will become larger which leads the uncertainty term to decrease. On the other hand, if the action <italic>a</italic> is not selected very often, <italic>t</italic> increases and so does the uncertainty term.</p>
<p><italic>k</italic>-Armed bandit approaches address the trade-off between exploitation and exploration directly. It has been shown that the difference between the obtained rewards and optimal rewards, or the <italic>regret</italic>, is at best logarithmic in the number of iterations <italic>n</italic> in the absence of prior knowledge of the action value distributions and in the absence of context [<xref ref-type="bibr" rid="ref102">102</xref>]. UCB algorithms with a regret logarithmic in and uniformly distributed over <italic>n</italic> exist [<xref ref-type="bibr" rid="ref007">7</xref>]. This makes them a very interesting choice when strong theoretical guarantees on performance are required.</p>
<p>Whether these algorithms are suitable, however, depends on the setting at hand. If there is a large number of actions to choose from or when the task is not stationary <italic>k</italic>-armed bandits are typically too simplistic. In a news recommendation task, for example, exploration may take longer than an item stays relevant. Additionally, <italic>k</italic>-armed bandits are not suitable when action values are conditioned on the situation at hand, that is: when a single action results in a different reward based on e.g. time-of-day or user-specific information such as in Section <xref rid="x1-2000-2">2</xref>. In these scenarios, the problem formalization of contextual bandits and the use of function approximation are of interest.</p>
</sec>
<sec id="x1-9000-3.1.5">
<label>3.1.5.</label>
<title>Contextual bandits</title>
<p>In the previous sections, action-values where not associated with different situations. In this section we extend the non-associative bandit setting to the associative setting of contextual bandits. Assume a setting with <italic>n k</italic>-armed bandits problems. At each time step <italic>t</italic> one encounters a situation with a randomly selected <italic>k</italic>-armed bandits problem. We can use some of the approaches that were discussed to estimate the action values. However, this is only possible if the true action-values change slowly between the different <italic>n</italic> problems [<xref ref-type="bibr" rid="ref188">188</xref>]. Add to this setting the fact that now at each time <italic>t</italic> a distinctive piece of information is provided about the underlying <italic>k</italic>-armed bandit which is not the actual action value. Using this information we can now learn a policy that uses the distinctive information to associate the <italic>k</italic>-armed bandit with the best action to take. This approach is called contextual bandits and uses trial-and-error to search for the optimal actions and associates these actions with situation in which they perform optimally. This type of algorithm is positioned between <italic>k</italic>-armed bandits and full RL. The similarity with RL lies in the fact that a policy is learned while the association with <italic>k</italic>-armed bandits stems from the fact that actions only affect immediate rewards. When actions are allowed to affect the next situation as well then we are dealing with RL.</p>
</sec>
<sec id="x1-10000-3.1.6">
<label>3.1.6.</label>
<title>Function approximation: LinUCB and CLUB</title>
<p>Despite the good theoretical characteristics of the UCB algorithm, it is not often used in the contextual setting in practice. The reason is that in practice, state and action spaces may be very large and although UCB is optimal in the uninformed case, we may do better if we use obtained information across actions and situations. Instead of maintaining isolated sample-average estimates per action or per state-action pair such as in Sections <xref rid="x1-6000-3.1.2">3.1.2</xref> and <xref rid="x1-9000-3.1.5">3.1.5</xref>, we can estimate a parametric payoff function approximated from data. The parametric function takes some feature description of actions for <italic>k</italic>-armed bandit settings and state-action pairs for the contextual bandit setting and output some estimated <inline-formula><mml:math id="math126"><mml:mover accent="true">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">θ</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover></mml:math></inline-formula>. Here, we focus on the contextual-bandit algorithms LinUCB and CLUB.</p>
<p>LinUCB (Linear Upper-Confidence Bound) uses linear function approximation to calculate the confidence interval efficiently in closed form [<xref ref-type="bibr" rid="ref107">107</xref>]. Define the expected payoff for action <italic>a</italic> with the <italic>d</italic>-dimensional featurized state <inline-formula><mml:math id="math127">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="math128">
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msubsup></mml:math></inline-formula> a vector of unknown parameters as follows: 
<disp-formula>
<mml:math display="block" id="math129">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(12)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:mi mathvariant="double-struck">E</mml:mi>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">]</mml:mo>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
</mml:msubsup>
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula> 
Using ridge regression, an estimate of <inline-formula><mml:math id="math130">
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> can be obtained [<xref ref-type="bibr" rid="ref107">107</xref>]. Consequently, it can be shows that for any <inline-formula><mml:math id="math131">
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mo mathvariant="normal">&gt;</mml:mo>
<mml:mn>0</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="math132">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">∈</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="double-struck">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">d</mml:mi>
</mml:mrow>
</mml:msup></mml:math></inline-formula> with <inline-formula><mml:math id="math133">
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>+</mml:mo>
<mml:msqrt>
<mml:mrow>
<mml:mo movablelimits="false">ln</mml:mo>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo><mml:mstyle displaystyle="false">
<mml:mfrac>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mfrac>
</mml:mstyle>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo mathvariant="normal" stretchy="false">/</mml:mo>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msqrt></mml:math></inline-formula> a reasonably tight estimate for the expected payoff of arm <italic>a</italic> can be obtained as follows: 
<disp-formula>
<mml:math display="block" id="math134">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(13)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:munder>
<mml:mrow>
<mml:mo movablelimits="false">arg</mml:mo>
<mml:mo movablelimits="false">max</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:munder>
<mml:mo fence="true" maxsize="1.19em" minsize="1.19em">[</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
</mml:msubsup>
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="italic">α</mml:mi>
<mml:msqrt>
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
</mml:msubsup>
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="italic">A</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:msqrt>
<mml:mspace width="2.75pt"/>
<mml:mo fence="true" maxsize="1.19em" minsize="1.19em">]</mml:mo>
<mml:mo mathvariant="normal">,</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula> 
where <inline-formula><mml:math id="math135">
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="italic">A</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">T</mml:mi>
</mml:mrow>
</mml:msubsup>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">I</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">d</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="math136">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> a design matrix of dimension <italic>m</italic> x <italic>d</italic> whose rows are the <italic>m</italic> contexts that are observed, <inline-formula><mml:math id="math137">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">b</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">∈</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="double-struck">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">m</mml:mi>
</mml:mrow>
</mml:msup></mml:math></inline-formula> the corresponding response vector and <inline-formula><mml:math id="math138">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">I</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">d</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> the <italic>d</italic> x <italic>d</italic> identity matrix [<xref ref-type="bibr" rid="ref107">107</xref>].</p>
<p>Similar to LinUCB, CLUB (Clustering of bandits) utilizes the linear bandit algorithm for payoff estimation [<xref ref-type="bibr" rid="ref069">69</xref>]. In contrast to LinUCB, CLUB uses adaptive clustering in order to speed up the learning process. The main idea is to use confidence balls of user models estimate user similarity and share feedback across similar users. CLUB can thus be understood as a cluster-based alternative (see Section <xref rid="x1-2000-2">2</xref>) to LinUCB algorithm.</p>
</sec>
</sec>
<sec id="x1-11000-3.2">
<label>3.2.</label>
<title>Value-based RL</title>
<p>In value based RL, we learn an estimate <italic>V</italic> of the optimal value function <inline-formula><mml:math id="math139">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:msub></mml:math></inline-formula> for a given policy <italic>π</italic>. We do this with the aim of finding <inline-formula><mml:math id="math140">
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup></mml:math></inline-formula>. Temporal-difference (TD) prediction is a method that learns from raw experiences without having to build a model of the environment the policy is interacting with [<xref ref-type="bibr" rid="ref188">188</xref>,<xref ref-type="bibr" rid="ref199">199</xref>]. In this section, we discuss various RL algorithms based on TD prediction.</p>
<sec id="x1-12000-3.2.1">
<label>3.2.1.</label>
<title>Sarsa: On-policy temporal-difference RL</title>
<p>Sarsa is an on-policy temporal-difference method that learns an action-value function [<xref ref-type="bibr" rid="ref181">181</xref>,<xref ref-type="bibr" rid="ref188">188</xref>]. Given the current behaviour policy <italic>π</italic>, we estimate <inline-formula><mml:math id="math141"><mml:mover accent="true">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> <inline-formula><mml:math id="math142">
<mml:mo>∀</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi></mml:math></inline-formula>, and <italic>a</italic>. This is done using transitions from state-action pair to state-action pair. Events of the form <inline-formula><mml:math id="math143">
<mml:mo fence="true" stretchy="false">⟨</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo fence="true" stretchy="false">⟩</mml:mo></mml:math></inline-formula> are used in the following update rule to estimate the state-action values: 
<disp-formula>
<mml:math display="block" id="math144">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(14)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo fence="true" maxsize="1.19em" minsize="1.19em">[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="italic">γ</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo fence="true" maxsize="1.19em" minsize="1.19em">]</mml:mo>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula>
</p>
<p>This update rule is applied after every transition from <inline-formula><mml:math id="math145">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="math146">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub></mml:math></inline-formula>. In case <inline-formula><mml:math id="math147">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub></mml:math></inline-formula> is a terminal state, a value of zero is assigned. By doing this we are ensuring that the estimate <inline-formula><mml:math id="math148"><mml:mover accent="true">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover></mml:math></inline-formula> for a behaviour policy <italic>π</italic> while resulting in changes in <italic>π</italic> given <inline-formula><mml:math id="math149">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula>. Sarsa will converge to an optimal action-value function <inline-formula><mml:math id="math150">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:msub></mml:math></inline-formula> and hence an optimal policy <inline-formula><mml:math id="math151">
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup></mml:math></inline-formula> in the limit given that all possible state-action pairs are visited an infinite amount of time [<xref ref-type="bibr" rid="ref188">188</xref>]. Consequently, Sarsa converges to the greedy policy in the limit. Algorithm <xref rid="x1-12003r-1">1</xref> shows Sarsa in more detail.</p>
<fig id="x1-12003r-1">
<label>Algorithm 1:</label>
<caption>
<p>Sarsa – An on-policy temporal-difference RL algorithm</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g003.jpg"/>
</fig>
</sec>
<sec id="x1-13000-3.2.2">
<label>3.2.2.</label>
<title>Q-learning: Off-policy temporal-difference RL</title>
<p>Q-learning was one of the breakthroughs in the field of RL [<xref ref-type="bibr" rid="ref188">188</xref>,<xref ref-type="bibr" rid="ref219">219</xref>]. Q-learning is classified as an off-policy temporal-difference algorithm for control. Similar to Sarsa, Q-learning approximates the optimal action-value function <inline-formula><mml:math id="math152">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:msub></mml:math></inline-formula> by learning <inline-formula><mml:math id="math153"><mml:mover accent="true">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover></mml:math></inline-formula>. Differently from Sarsa, Q-learning learns <inline-formula><mml:math id="math154"><mml:mover accent="true">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∗</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover></mml:math></inline-formula> independently of the policy being followed. The policy being followed still has an effect on the learning process, but only by determining which state-action pairs are visited and consequently updated. Algorithm <xref rid="x1-13003r-2">2</xref> shows Q-learning in more detail. The update rule for Q-learning is defined as follows: 
<disp-formula>
<mml:math display="block" id="math155">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(15)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo fence="true" maxsize="1.19em" minsize="1.19em">[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="italic">γ</mml:mi>
<mml:munder>
<mml:mrow>
<mml:mo movablelimits="false">max</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:munder>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo fence="true" maxsize="1.19em" minsize="1.19em">]</mml:mo>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula>
</p>
<fig id="x1-13003r-2">
<label>Algorithm 2:</label>
<caption>
<p>Q-Learning – An off-policy temporal-difference RL algorithm</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g004.jpg"/>
</fig>
</sec>
<sec id="x1-14000-3.2.3">
<label>3.2.3.</label>
<title>Value-function approximation</title>
<p>In Sections <xref rid="x1-13000-3.2.2">3.2.2</xref> and <xref rid="x1-12000-3.2.1">3.2.1</xref> we discussed tabular algorithms for value-based RL. In this section we discuss function approximation in RL for estimating state-value functions from a known policy <italic>π</italic> (i.e. on-policy RL). The difference with the tabular approach is that we represent <inline-formula><mml:math id="math156">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">v</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> as a parameterized function with a weight vector <inline-formula><mml:math id="math157">
<mml:mi mathvariant="italic">w</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="double-struck">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">d</mml:mi>
</mml:mrow>
</mml:msup></mml:math></inline-formula> where <inline-formula><mml:math id="math158"><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">v</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">w</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo stretchy="false">≈</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">v</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> is the approximated value of state <italic>s</italic> given the learned weights <italic>w</italic>. Different function approximators can be used to estimate <inline-formula><mml:math id="math159"><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">v</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover></mml:math></inline-formula>. For instance, <inline-formula><mml:math id="math160"><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">v</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover></mml:math></inline-formula> can be a deep neural network with <italic>w</italic> representing the weights of the network. In the tabular version of value-based RL, states and their estimated values are isolated from each other while in function approximation adjusting one weight in the network can lead to changes in the estimated values of many states. This form of learning is powerful due its ability to generalize across different states, but at the same time may lead to more complex models that are harder to understand and to tune. An example of value-function approximation is the deep Q-network (DQN) algorithm [<xref ref-type="bibr" rid="ref133">133</xref>]. This algorithm combines deep (convolutional) neural network and Q-learning. Using DQN, it was shown that RL agents can achieve state-of-the-art performances on many problems without relying on engineered features. DNQ learns directly from raw (pixel) data instead. The following update rule is an alteration of the Q-learning (semi-gradient of Q-learning [<xref ref-type="bibr" rid="ref188">188</xref>]) update rule for estimating the weights of the network: 
<disp-formula>
<mml:math display="block" id="math161">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(16)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo fence="true" maxsize="1.61em" minsize="1.61em">[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="italic">γ</mml:mi>
<mml:mo>·</mml:mo>
<mml:munder>
<mml:mrow>
<mml:mo movablelimits="false">max</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
</mml:munder>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo fence="true" maxsize="1.61em" minsize="1.61em">]</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mo>∇</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">w</mml:mi>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">Q</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula>
</p>
</sec>
</sec>
<sec id="x1-15000-3.3">
<label>3.3.</label>
<title>Policy-gradient RL</title>
<p>In value-based RL values of actions are approximated and then a policy is derived by selecting actions using a certain selection strategy. In policy-gradient RL we learn a parameterized policy directly [<xref ref-type="bibr" rid="ref188">188</xref>,<xref ref-type="bibr" rid="ref189">189</xref>]. Consequently, we can select actions without the need for an explicit value function. Let <inline-formula><mml:math id="math162">
<mml:mi mathvariant="normal">Θ</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="double-struck">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">d</mml:mi>
</mml:mrow>
</mml:msup></mml:math></inline-formula> where <italic>d</italic> is the dimension of the parameter vector Θ. For policy-based methods that also rely on a value function, we denote the function’s weight vector denoted by <inline-formula><mml:math id="math163">
<mml:mi mathvariant="italic">w</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="double-struck">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="italic">d</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>′</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:msup></mml:math></inline-formula> as <inline-formula><mml:math id="math164"><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">v</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">w</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula>. Define the probability of selecting action <italic>a</italic> at time step <italic>t</italic> given state <italic>s</italic> with policy parameters Θ as: 
<disp-formula>
<mml:math display="block" id="math165">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(17)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:mi mathvariant="italic">π</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="normal">Θ</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="italic">P</mml:mi>
<mml:mo fence="true" stretchy="false">[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="italic">s</mml:mi>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="normal">Θ</mml:mi>
<mml:mo fence="true" stretchy="false">]</mml:mo>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula>
</p>
<p>Consider a function <inline-formula><mml:math id="math166">
<mml:mi mathvariant="italic">J</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="normal">Θ</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> that quantifies the performance of the policy <italic>π</italic> with respect to parameter vector Θ. The goal is to optimize Θ such that <inline-formula><mml:math id="math167">
<mml:mi mathvariant="italic">J</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="normal">Θ</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> is maximized. We use the following update rule to approximate gradient ascent in <italic>J</italic> where the term <inline-formula><mml:math id="math168"><mml:mover accent="false">
<mml:mrow>
<mml:mo>∇</mml:mo>
<mml:mi mathvariant="italic">J</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mo stretchy="true">ˆ</mml:mo></mml:mover>
<mml:mo stretchy="false">∈</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="double-struck">R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">d</mml:mi>
</mml:mrow>
</mml:msup></mml:math></inline-formula> approximates the gradient of <inline-formula><mml:math id="math169">
<mml:mi mathvariant="italic">J</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="normal">Θ</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo></mml:math></inline-formula> at <italic>t</italic>: 
<disp-formula>
<mml:math display="block" id="math170">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(18)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="italic">α</mml:mi><mml:mover accent="false">
<mml:mrow>
<mml:mo>∇</mml:mo>
<mml:mi mathvariant="italic">J</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mo stretchy="true">ˆ</mml:mo></mml:mover>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula>
</p>
<fig id="x1-15004r-3">
<label>Algorithm 3:</label>
<caption>
<p>One-step episodic actor-critic</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g005.jpg"/>
</fig>
</sec>
<sec id="x1-16000-3.4">
<label>3.4.</label>
<title>Actor-critic</title>
<p>In actor-critic methods [<xref ref-type="bibr" rid="ref098">98</xref>,<xref ref-type="bibr" rid="ref188">188</xref>] both the value and policy functions are approximated. The actor in actor-critic is the learned policy while the critic approximates the value function. Algorithm <xref rid="x1-15004r-3">3</xref> shows the one-step episodic actor-critic algorithm in more detail. The update rule for the parameter vector Θ is defined as follows: 
<disp-formula>
<mml:math display="block" id="math171">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(19)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="italic">α</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">δ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub><mml:mstyle displaystyle="true">
<mml:mfrac>
<mml:mrow>
<mml:mo>∇</mml:mo>
<mml:mi mathvariant="italic">π</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">π</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">a</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">Θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfrac>
</mml:mstyle>
<mml:mo mathvariant="normal">,</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula> 
where <inline-formula><mml:math id="math172">
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">δ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub></mml:math></inline-formula> is defined as follows: 
<disp-formula>
<mml:math display="block" id="math173">
<mml:mtable displaystyle="true"><mml:mlabeledtr>
<mml:mtd>
<mml:mtext>(20)</mml:mtext>
</mml:mtd>
<mml:mtd>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">δ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="italic">γ</mml:mi><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">v</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">w</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo><mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="italic">v</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">ˆ</mml:mo></mml:mover>
<mml:mo mathvariant="normal" fence="true" stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo mathvariant="normal">,</mml:mo>
<mml:mi mathvariant="italic">w</mml:mi>
<mml:mo mathvariant="normal" fence="true" stretchy="false">)</mml:mo>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mlabeledtr></mml:mtable></mml:math>
</disp-formula>
</p>
</sec>
</sec>
<sec id="x1-17000-4">
<label>4.</label>
<title>A classification of personalization settings</title>
<p>Personalization has many different definitions [<xref ref-type="bibr" rid="ref030">30</xref>,<xref ref-type="bibr" rid="ref055">55</xref>,<xref ref-type="bibr" rid="ref165">165</xref>]. We adopt the definition proposed in [<xref ref-type="bibr" rid="ref055">55</xref>] as it is based on 21 existing definitions found in literature and suits a variety of application domains: “personalization is a process that changes the functionality, interface, information access and content, or distinctiveness of a system to increase its personal relevance to an individual or a category of individuals”. This definition identifies personalization as a process and mentions an existing system subject to that process. We include aspects of both the desired process of change and existing system in our framework. Section <xref rid="x1-22000-5.4">5.4</xref> further details how this framework was used in a SLR.</p>
<p>Table <xref rid="x1-17001-1">1</xref> provides an overview of the framework. On a high level, we distinguish three categories. The first category contains aspects of suitability of system behavior. We differentiate settings in which suitability of system behavior is determined explicitly by users and settings in which it is inferred by the system after observing user behavior [<xref ref-type="bibr" rid="ref172">172</xref>]. For example, a user can explicitly rate suitability of a video recommendation; a system can also infer suitability by observing whether the user decides to watch the video. Whether implicit or explicit feedback is preferable depends on availability and quality of feedback signals [<xref ref-type="bibr" rid="ref089">89</xref>,<xref ref-type="bibr" rid="ref143">143</xref>,<xref ref-type="bibr" rid="ref172">172</xref>]. Besides suitability, we consider safety of system behavior. Unaltered RL algorithms use trial-and-error style exploration to optimize their behavior yet this may not suit a particular domain [<xref ref-type="bibr" rid="ref078">78</xref>,<xref ref-type="bibr" rid="ref092">92</xref>,<xref ref-type="bibr" rid="ref136">136</xref>,<xref ref-type="bibr" rid="ref153">153</xref>]. For example, tailoring the insulin delivery policy of an artificial pancreas to the metabolism of an individual requires trial insulin delivery action but these should only be sampled when their outcome is within safe certainty bounds [<xref ref-type="bibr" rid="ref044">44</xref>]. If safety is a significant concern in the systems’ application domain, specifically designed safety-aware RL techniques may be required, see [<xref ref-type="bibr" rid="ref149">149</xref>] and [<xref ref-type="bibr" rid="ref064">64</xref>] for overviews of such techniques.</p>
<table-wrap id="x1-17001-1">
<label>Table 1</label>
<caption>
<p>Framework to categorize personalization setting by</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<td valign="top" align="left">Category</td>
<td valign="top" align="center">A#</td>
<td valign="top" align="center">Aspect</td>
<td valign="top" align="center">Description</td>
<td valign="top" align="center">Range</td>
</tr>
</thead>
<tbody>
<tr>
<td valign="top" align="left" rowspan="2">Suitability outcome</td>
<td valign="top" align="center">A1</td>
<td valign="top" align="left">Control</td>
<td valign="top" align="left">The extent to which the user defines the suitability of behavior explicitly.</td>
<td valign="top" align="left">Explicit – implicit</td>
</tr>
<tr>
<td valign="top" align="center">A2</td>
<td valign="top" align="left">Safety</td>
<td valign="top" align="left">The extent to which safety is of importance.</td>
<td valign="top" align="left">Trivial – critical</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">Upfront knowledge</td>
<td valign="top" align="center">A3</td>
<td valign="top" align="left">User models</td>
<td valign="top" align="left">The a priori availability of models that describe user responses to system behavior.</td>
<td valign="top" align="left">Unavailable – unlimited</td>
</tr>
<tr>
<td valign="top" align="center">A4</td>
<td valign="top" align="left">Data availability</td>
<td valign="top" align="left">The a priori availability of human responses to system behavior.</td>
<td valign="top" align="left">Unavailable – unlimited</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="3">New Experiences</td>
<td valign="top" align="center">A5</td>
<td valign="top" align="left">Interaction availability</td>
<td valign="top" align="left">The availability of new samples of interactions with individuals.</td>
<td valign="top" align="left">Unavailable – unlimited</td>
</tr>
<tr>
<td valign="top" align="center">A6</td>
<td valign="top" align="left">Privacy sensitivity</td>
<td valign="top" align="left">The degree to which privacy is a concern.</td>
<td valign="top" align="left">Trivial – critical</td>
</tr>
<tr>
<td valign="top" align="center">A7</td>
<td valign="top" align="left">State observability</td>
<td valign="top" align="left">The degree to which all information to base personalization can be measured.</td>
<td valign="top" align="left">Partial – full</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Aspects in the second category deal with the availability of upfront knowledge. Firstly, knowledge of how users respond to system actions may be captured in user models. Such models open up a range of RL solutions that require less or no sampling of new interactions with users [<xref ref-type="bibr" rid="ref081">81</xref>]. As an example, user pain models are used to predict suitability of exercises in an adaptive physical rehabilitation curriculum manager a priori [<xref ref-type="bibr" rid="ref208">208</xref>]. Models can also be used to interact with the RL agent in simulation. For example, dialogue agent modules may be trained by interacting with a simulated chatbot user [<xref ref-type="bibr" rid="ref047">47</xref>,<xref ref-type="bibr" rid="ref095">95</xref>,<xref ref-type="bibr" rid="ref105">105</xref>]. Secondly, upfront knowledge may be available in the form of data on human responses to system behavior. This data can be used to derive user models and can be used to optimize policies directly and provide high-confidence evaluations of such policies [<xref ref-type="bibr" rid="ref111">111</xref>,<xref ref-type="bibr" rid="ref202">202</xref>–<xref ref-type="bibr" rid="ref204">204</xref>].</p>
<p>The third category details new experiences. Empirical RL approaches have proven capable of modelling extremely complex dynamics, however, this typically requires complex estimators that in turn need substantial amounts of training data. The availability of users to interact with is therefore a major consideration when designing an RL solution. A second aspect that relates to the use of new experiences is privacy sensitivity of the setting. Privacy sensitivity is of importance as it may restrict sharing, pooling or any other specific usage of data [<xref ref-type="bibr" rid="ref009">9</xref>,<xref ref-type="bibr" rid="ref076">76</xref>]. Finally, we identify the state observability as a relevant aspect. In some settings, the true environment state cannot be observed directly but must be estimated using available observations. This may be common as personalization exploits differences in mental [<xref ref-type="bibr" rid="ref022">22</xref>,<xref ref-type="bibr" rid="ref096">96</xref>,<xref ref-type="bibr" rid="ref217">217</xref>] and physical state [<xref ref-type="bibr" rid="ref067">67</xref>,<xref ref-type="bibr" rid="ref125">125</xref>]. For example, recommending appropriate music during running involves matching songs to the user emotional state and e.g. running pace. Both mental and physical state may be hard to measure accurately [<xref ref-type="bibr" rid="ref002">2</xref>,<xref ref-type="bibr" rid="ref017">17</xref>,<xref ref-type="bibr" rid="ref152">152</xref>].</p>
<p>Although aspects in Table <xref rid="x1-17001-1">1</xref> are presented separately, we explicitly note that they are not mutually independent. Settings where privacy is a major concern, for example, are expected to typically have less existing and new interactions available. Similarly, safety requirements will impact new interaction availability. Presence of upfront knowledge is mostly of interest in settings where control lies with the system as it may ease the control task. In contrast, user models may be marginally important if desired behavior is specified by the user in full. Finally, a lack of upfront knowledge and partial observability complicates adhering to safety requirements.</p>
</sec>
<sec id="x1-18000-5">
<label>5.</label>
<title>A systematic literature review</title>
<p>A SLR is ‘a form of secondary study that uses a well-defined methodology to identify, analyze and interpret all available evidence related to a specific research question in a way that is unbiased and (to a degree) repeatable’ [<xref ref-type="bibr" rid="ref023">23</xref>]. PRISMA is a standard for reporting on SLRs and details eligibility criteria, article collection, screening process, data extraction and data synthesis [<xref ref-type="bibr" rid="ref135">135</xref>]. This section contains a report on this SLR according to the PRISMA statement. This SLR was a collaborative work to which all authors contributed. We denote authors by abbreviation of their names, e.g. FDH, EG, AEH and MH.</p>
<sec id="x1-19000-5.1">
<label>5.1.</label>
<title>Inclusion criteria</title>
<p>Studies in this SLR were included on the basis of three eligibility criteria. To be included, articles had to be published in a peer-reviewed journal or conference proceedings in English. Secondly, the study had to address a problem fitting to our definition of personalization as described in Section <xref rid="x1-17000-4">4</xref>. Finally, the study had to use a RL algorithm to address such a personalization problem. Here, we view contextual bandit algorithms as a subset of RL algorithms and thus included them in our analysis. Additionally, we excluded studies in which a RL algorithm was used for purposes other than personalization.</p>
</sec>
<sec id="x1-20000-5.2">
<label>5.2.</label>
<title>Search strategy</title>
<fig id="x1-20001-3">
<label>Fig. 3.</label>
<caption>
<p>Overview of the SLR process.</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g006.jpg"/>
</fig>
<p>Figure <xref rid="x1-20001-3">3</xref> contains an overview of the SLR process. The first step is to run a query on a set of databases. For this SLR, a query was run on Scopus, IEEE Xplore, ACM’s full-text collection, DBLP and Google Scholar on June 6, 2018. These databases were selected as their combined index spans a wide range, and their combined result set was sufficiently large for this study. Scopus and IEEE Xplore support queries on title, keywords and abstract. ACM’s full-text collection, DBLP and Google scholar do not support queries on keywords and abstract content. We therefore ran two kinds of queries: we queried on title only for ACM’s full-text collection, DBLP and Google Scholar and we extended this query to keywords and abstract content for Scopus and IEEE Xplore. The query was constructed by combining techniques of interest and keywords for the personalization problem. For techniques of interest the terms ‘reinforcement learning’ and ‘contextual bandits’ were used. For the personalization problem, variations on the words ‘personalized’, ‘customized’, ‘individualized’ and ‘tailored’ were included in British and American spelling. All queries are listed in Appendix <xref rid="x1-31000-A">A</xref>. Query results were de-duplicated and stored in a spreadsheet.</p>
</sec>
<sec id="x1-21000-5.3">
<label>5.3.</label>
<title>Screening process</title>
<p>In the screening process, all query results are tested against the inclusion criteria from Section <xref rid="x1-19000-5.1">5.1</xref> in two phases. We used all criteria in both phases. In the first phase, we assessed eligibility based on keywords, abstract and title whereas we used full text of the article in the second phase. In the first phase, a spreadsheet with de-duplicated results was shared with all authors via Google Drive. Studies were assigned randomly to authors who scored each study by the eligibility criteria. The results of this screening were verified by one of the other authors, assigned randomly. Disagreements were settled in meetings involving those in disagreement and FDH if necessary. In addition to eligibility results, author preferences for full-text screening were recorded on a three-point scale. Studies that were not considered eligible were not taken into account beyond this point, all other studies were included in the second phase.</p>
<p>In the second phase, data on eligible studies was copied to a new spreadsheet. This sheet was again shared via Google Drive. Full texts were retrieved and evenly divided amongst authors according to preference. For each study, the assigned author then assessed eligibility based on full text and extracted the data items detailed below.</p>
</sec>
<sec id="x1-22000-5.4">
<label>5.4.</label>
<title>Data items</title>
<p>Data on setting, solution and methodology were collected. Table <xref rid="x1-22001-2">2</xref> contains all data items for this SLR. For data on setting, we operationalized our framework from Table <xref rid="x1-17001-1">1</xref> in Section <xref rid="x1-17000-4">4</xref>. To assess trends in solution, algorithms used, number of MDP models (see Section <xref rid="x1-2000-2">2</xref>) and training regime were recorded. Specifically, we noted whether training was performed by interacting with actual users (‘live’), using existing data and a simulator of user behavior. For the algorithms, we recorded the name as used by the authors. To gauge maturity of the proposed solutions and the field as a whole, data on the evaluation strategy and baselines used were extracted. Again, we listed whether evaluation included ‘live’ interaction with users, existing interactions between systems and users or using a simulator. Finally, publication year and application domain were registered to enable identification of trends over time and across domains. The list of domains was composed as follows: during phase one of the screening process, all authors recorded a domain for each included paper, yielding a highly inconsistent initial set of domains. This set was simplified into a more consistent set of domains which was used during full-text screening. For papers that did not fall into this consistent set of domains, two categories were added: a ‘Domain Independent’ and an ‘Other’ category. The actual domain was recorded for the five papers in the ‘Other’ category. These domains were not further consolidated as all five papers were assigned to unique domains not encountered before.</p>
<table-wrap id="x1-22001-2">
<label>Table 2</label>
<caption>
<p>Data items in SLR. The last column relates data items to aspects of setting from Table <xref rid="x1-17001-1">1</xref> where applicable</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<td valign="top" align="left">Category</td>
<td valign="top" align="center">#</td>
<td valign="top" align="center">Data item</td>
<td valign="top" align="center">Values</td>
<td valign="top" align="center">A#</td>
</tr>
</thead>
<tbody>
<tr>
<td valign="top" align="left" rowspan="8">Setting</td>
<td valign="top" align="char" char=".">1</td>
<td valign="top" align="left">User defines suitability of system behavior explicitly</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A1</td>
</tr>
<tr>
<td valign="top" align="char" char=".">2</td>
<td valign="top" align="left">Suitability of system behavior is derived</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A1</td>
</tr>
<tr>
<td valign="top" align="char" char=".">3</td>
<td valign="top" align="left">Safety is mentioned as a concern in the article</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A2</td>
</tr>
<tr>
<td valign="top" align="char" char=".">4</td>
<td valign="top" align="left">Privacy is mentioned as a concern in the article</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A6</td>
</tr>
<tr>
<td valign="top" align="char" char=".">5</td>
<td valign="top" align="left">Models of user responses to system behavior are available</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A3</td>
</tr>
<tr>
<td valign="top" align="char" char=".">6</td>
<td valign="top" align="left">Data on user responses to system behavior are available</td>
<td valign="top" align="left">Yes. No</td>
<td valign="top" align="center">A4</td>
</tr>
<tr>
<td valign="top" align="char" char=".">7</td>
<td valign="top" align="left">New interactions with users can be sampled with ease</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A5</td>
</tr>
<tr>
<td valign="top" align="char" char=".">8</td>
<td valign="top" align="left">All information to base personalization on can be measured</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A7</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="7">Solution</td>
<td valign="top" align="char" char=".">9</td>
<td valign="top" align="left">Algorithms</td>
<td valign="top" align="left">N/A</td>
<td valign="top" align="center">–</td>
</tr>
<tr>
<td valign="top" align="char" char=".">10</td>
<td valign="top" align="left">Number of learners</td>
<td valign="top" align="left">1, 1/user, 1/group, multiple</td>
<td valign="top" align="center">–</td>
</tr>
<tr>
<td valign="top" align="char" char=".">11</td>
<td valign="top" align="left">Usage of traits of the user</td>
<td valign="top" align="left">state, other, not used</td>
<td valign="top" align="center">–</td>
</tr>
<tr>
<td valign="top" align="char" char=".">12</td>
<td valign="top" align="left">Training mode</td>
<td valign="top" align="left">online, batch, other, unknown</td>
<td valign="top" align="center">–</td>
</tr>
<tr>
<td valign="top" align="char" char=".">13</td>
<td valign="top" align="left">Training in simulation</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A3</td>
</tr>
<tr>
<td valign="top" align="char" char=".">14</td>
<td valign="top" align="left">Training on a real-life dataset</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A4</td>
</tr>
<tr>
<td valign="top" align="char" char=".">15</td>
<td valign="top" align="left">Training in ‘live’ setting</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A5</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="5">Evaluation</td>
<td valign="top" align="char" char=".">16</td>
<td valign="top" align="left">Evaluation in simulation</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A3</td>
</tr>
<tr>
<td valign="top" align="char" char=".">17</td>
<td valign="top" align="left">Evaluation on a real-life dataset</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A4</td>
</tr>
<tr>
<td valign="top" align="char" char=".">18</td>
<td valign="top" align="left">Evaluation in ‘live’ setting</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">A5</td>
</tr>
<tr>
<td valign="top" align="char" char=".">19</td>
<td valign="top" align="left">Comparison with ‘no personalization’</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">–</td>
</tr>
<tr>
<td valign="top" align="char" char=".">20</td>
<td valign="top" align="left">Comparison with non-RL methods</td>
<td valign="top" align="left">Yes, No</td>
<td valign="top" align="center">–</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="x1-23000-5.5">
<label>5.5.</label>
<title>Synthesis and analysis</title>
<p>To facilitate analysis, reported algorithms were normalized using simple text normalization and key-collision methods. The resulting mappings are available in the dataset release [<xref ref-type="bibr" rid="ref046">46</xref>]. Data was summarized using descriptive statistics and figures with an accompanying narrative to gain insight into trends with respect to settings, solutions and evaluation over time and across domains.</p>
</sec>
</sec>
<sec id="x1-24000-6">
<label>6.</label>
<title>Results</title>
<p>The quantitative synthesis and analyses introduced in Section <xref rid="x1-23000-5.5">5.5</xref> were applied to the collected data. In this section, we present insights obtained. We focus on the major insights and encourage the reader to explore the tabular view in Appendix <xref rid="x1-32000-B">B</xref> or the collected data for further analysis [<xref ref-type="bibr" rid="ref046">46</xref>].</p>
<p>Before diving into the details of the study in light of the classification scheme we have proposed, let us first study some general trends. Figure <xref rid="x1-24001-4">4</xref> shows the number of publications addressing personalization using RL techniques over time. A clear increase can be seen. With over forty entries, the health domain contains by far the most articles, followed by entertainment, education and commerce with all approximately just over twenty five entries. Other domains contain less than twelve papers in total. Figure <xref rid="x1-24002-5">5</xref>(a) shows the popularity of domains for the five most recent years and seems to indicate that the number of articles in the health domain is steadily growing, in contrast with the other domains. Of course, these graphs are based on a limited number of publications, so drawing strong conclusions from these results is difficult. We do need to take into account that the popularity of RL for personalization is increasing in general. Therefore Fig. <xref rid="x1-24002-5">5</xref>(b) shows the relative distribution of studies over domains for the five most recent years. Now we see that the health domain is just following the overall trend, and is not becoming more popular within studies that use RL for personalization. We fail to identify clear trends for other domains from these figures.</p>
<fig id="x1-24001-4">
<label>Fig. 4.</label>
<caption>
<p>Distribution of included papers over time and over domains. Note that only studies published prior to the query date of June 6, 2018 were included.</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g007.jpg"/>
</fig>
<fig id="x1-24002-5">
<label>Fig. 5.</label>
<caption>
<p>Popularity of domains for the five most recent years.</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g008.jpg"/>
</fig>
<sec id="x1-25000-6.1">
<label>6.1.</label>
<title>Setting</title>
<p>Table <xref rid="x1-25001-3">3</xref> provides an overview of the data related to setting in which the studies were conducted. The table shows that user responses to system behavior are present in a minority of cases (66/166). Additionally, models of user behavior are only used in around one quarter of all publications. The suitability of system behavior is much more frequently derived from data (130/166) rather than explicitly collected by users (39/166). Privacy is clearly not within the scope of most articles, only in 9 out of 166 cases do we see this issue explicitly mentioned. Safety concerns, however, are mentioned in a reasonable proportion of studies (30/166). Interactions can generally be sampled with ease and the resulting information is frequently sufficient to base personalization of the system at hand on.</p>
<table-wrap id="x1-25001-3">
<label>Table 3</label>
<caption>
<p>Number of publications by aspects of setting</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<td valign="top" align="left">Aspect</td>
<td valign="top" align="center">#</td>
</tr>
</thead>
<tbody>
<tr>
<td valign="top" align="left">User defines suitability of system behavior explicitly</td>
<td valign="top" align="char" char=".">39</td>
</tr>
<tr>
<td valign="top" align="left">Suitability of system behavior is derived</td>
<td valign="top" align="char" char=".">130</td>
</tr>
<tr>
<td valign="top" align="left">Safety is mentioned as a concern in the article</td>
<td valign="top" align="char" char=".">30</td>
</tr>
<tr>
<td valign="top" align="left">Privacy is mentioned as a concern in the article</td>
<td valign="top" align="char" char=".">9</td>
</tr>
<tr>
<td valign="top" align="left">Models of user responses to system behavior are available</td>
<td valign="top" align="char" char=".">41</td>
</tr>
<tr>
<td valign="top" align="left">Data on user responses to system behavior are available</td>
<td valign="top" align="char" char=".">66</td>
</tr>
<tr>
<td valign="top" align="left">New interactions with users can be sampled with ease</td>
<td valign="top" align="char" char=".">97</td>
</tr>
<tr>
<td valign="top" align="left">All information to base personalization on can be measured</td>
<td valign="top" align="char" char=".">132</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Let us dive into some aspects in a bit more detail. A first trend we anticipate is an increase of the fraction of studies working with real data on human responses over the years, considering the digitization trend and associated data collection. Figure <xref rid="x1-25002-6">6</xref>(a) shows the fraction of papers for which data on user responses to system behavior is available over time. Surprisingly, we see that this fraction does not show any clear trend over time. Another aspect of interest relates to safety issues in particular domains. We hypothesize that in certain domains, such as health, safety is more frequently mentioned as a concern. Figure <xref rid="x1-25002-6">6</xref>(b) shows the fraction of papers of the different domains in which safety is mentioned. Indeed, we clearly see that certain domains mention safety much more frequently than other domains. Third, we explore the ease with which interactions with users can be sampled. Again, we expect to see substantial differences between domains. Figure <xref rid="x1-26001-7">7</xref> confirms our intuition. Interactions can be sampled with ease more frequently in studies in the commerce, entertainment, energy, and smart homes domains when compared to communication and health domains.</p>
<fig id="x1-25002-6">
<label>Fig. 6.</label>
<caption>
<p>Availability of user responses over time (a), and mentions of safety as a concern over domains (b).</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g009.jpg"/>
</fig>
<p>Finally, we investigate whether upfront knowledge is available. In our analysis, we explore both real data as well user models being available upfront. One would expect papers to have at least one of these two prior to starting experiments. User models and not real data were reported in 41 studies, while 53 articles used real data but no user model and 12 use both. We see that for 71 studies neither is available. In roughly half of these, simulators were used for both training (38/71) and evaluation (37/71). In a minority, training (15/71) and evaluation (17/71) were performed in a live setting, e.g. while collecting data.</p>
</sec>
<sec id="x1-26000-6.2">
<label>6.2.</label>
<title>Solution</title>
<p>In our investigation into solutions, we first explore the algorithms that were used. Figure <xref rid="x1-26002-8">8</xref> shows the distribution of usage frequency. A vast majority of the algorithms are used only once, some techniques are used a couple of times and one algorithm is used 60 times. Note again that we use the name of the algorithms used by the authors as a basis for this analysis. Table <xref rid="x1-26003-4">4</xref> lists the algorithms that were used more than once. A significant number of studies (60/166) use the Q-learning algorithm. At the same time, a substantial number of articles (18/166) reports the use of RL as the underlying algorithmic framework without specifying an actual algorithm. The contextual bandits, Sarsa, actor-critic and inverse RL (IRL) algorithms are used in respectively (18/166), (12/166), (8/166), (8/166) and (7/166) papers. We also observe some additional algorithms from the contextual bandits family, such as UCB and LinUCB. Furthermore, we find various mentions that indicate the usage of deep neural networks: deep reinforcement learning, DQN and DDQN. In general, we find that some publications refer to a specific algorithm whereas others only report generic techniques or families thereof.</p>
<fig id="x1-26001-7">
<label>Fig. 7.</label>
<caption>
<p>New interactions with users can be sampled with ease.</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g010.jpg"/>
</fig>
<fig id="x1-26002-8">
<label>Fig. 8.</label>
<caption>
<p>Distribution of algorithm usage frequencies.</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g011.jpg"/>
</fig>
<table-wrap id="x1-26003-4">
<label>Table 4</label>
<caption>
<p>Algorithm usage for all algorithms that were used in more than one publication</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<td valign="top" align="left">Algorithm</td>
<td valign="top" align="center"># of uses</td>
</tr>
</thead>
<tbody>
<tr>
<td valign="top" align="left">Q-learning [<xref ref-type="bibr" rid="ref219">219</xref>]</td>
<td valign="top" align="char" char=".">60</td>
</tr>
<tr>
<td valign="top" align="left">RL, not further specified</td>
<td valign="top" align="char" char=".">18</td>
</tr>
<tr>
<td valign="top" align="left">Contextual bandits</td>
<td valign="top" align="char" char=".">12</td>
</tr>
<tr>
<td valign="top" align="left">Sarsa [<xref ref-type="bibr" rid="ref187">187</xref>]</td>
<td valign="top" align="char" char=".">8</td>
</tr>
<tr>
<td valign="top" align="left">Actor-critic</td>
<td valign="top" align="char" char=".">8</td>
</tr>
<tr>
<td valign="top" align="left">Inverse reinforcement learning</td>
<td valign="top" align="char" char=".">7</td>
</tr>
<tr>
<td valign="top" align="left">UCB [<xref ref-type="bibr" rid="ref007">7</xref>]</td>
<td valign="top" align="char" char=".">5</td>
</tr>
<tr>
<td valign="top" align="left">Policy iteration</td>
<td valign="top" align="char" char=".">5</td>
</tr>
<tr>
<td valign="top" align="left">LinUCB [<xref ref-type="bibr" rid="ref037">37</xref>]</td>
<td valign="top" align="char" char=".">5</td>
</tr>
<tr>
<td valign="top" align="left">Deep reinforcement learning</td>
<td valign="top" align="char" char=".">4</td>
</tr>
<tr>
<td valign="top" align="left">Fitted Q-iteration [<xref ref-type="bibr" rid="ref166">166</xref>]</td>
<td valign="top" align="char" char=".">3</td>
</tr>
<tr>
<td valign="top" align="left">DQN [<xref ref-type="bibr" rid="ref133">133</xref>]</td>
<td valign="top" align="char" char=".">3</td>
</tr>
<tr>
<td valign="top" align="left">Interactive reinforcement learning</td>
<td valign="top" align="char" char=".">2</td>
</tr>
<tr>
<td valign="top" align="left">TD-learning</td>
<td valign="top" align="char" char=".">2</td>
</tr>
<tr>
<td valign="top" align="left">DYNA-Q [<xref ref-type="bibr" rid="ref186">186</xref>]</td>
<td valign="top" align="char" char=".">2</td>
</tr>
<tr>
<td valign="top" align="left">Policy gradient</td>
<td valign="top" align="char" char=".">2</td>
</tr>
<tr>
<td valign="top" align="left">CLUB [<xref ref-type="bibr" rid="ref069">69</xref>]</td>
<td valign="top" align="char" char=".">2</td>
</tr>
<tr>
<td valign="top" align="left">Monte Carlo</td>
<td valign="top" align="char" char=".">2</td>
</tr>
<tr>
<td valign="top" align="left">Thompson sampling</td>
<td valign="top" align="char" char=".">2</td>
</tr>
<tr>
<td valign="top" align="left">DDQN [<xref ref-type="bibr" rid="ref212">212</xref>]</td>
<td valign="top" align="char" char=".">2</td>
</tr>
</tbody>
</table>
</table-wrap>
<fig id="x1-26004-9">
<label>Fig. 9.</label>
<caption>
<p>Occurence of different solution architectures (a) and usage of simulators in training (b). For (a), publications that compare architectures are represented in the ‘multiple’ category.</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g012.jpg"/>
</fig>
<p>Figure <xref rid="x1-26004-9">9</xref>(a) lists the number of models used in the included publications. The majority of solutions relies on a single-model architecture. On the other end of the spectrum lies the architecture of using one model per person. This architecture comes second in usage frequency. The architecture that uses one model per group can be considered a middle ground between these former two. In this architecture, only experiences with relevant individuals can be shared. Comparisons between architectures are rare. We continue by investigating whether and where traits of the individual were used in relation to these architectures. Table <xref rid="x1-27001-5">5</xref> provides an overview. Out of all papers that use one model, 52.7% did not use the traits of the individuals and 41.7 % included traits in the state space. 47.5% of the papers include the traits of the individuals in the state representation while in 37.3% of the papers the traits were not included. In 15.3% of the cases this was not known.</p>
<p>Figure <xref rid="x1-26004-9">9</xref>(b) shows the popularity of using a simulator for training per domain. We see that a substantial percentage of publications use a simulator and that simulators are used in all domains. Simulators are used in the majority of publications for the energy, transport, communication and entertainment domains. In publications in the first three out of these domains, we typically find applications that require large-scale implementation and have a big impact on infrastructure, e.g. control of the entire energy grid or a fleet of taxis in a large city. This complicates the collection of useful realistic dataset and training in a live setting. This is not the case for the entertainment domain with 17 works using a simulator for training. Further investigation shows that nine out of these 17 also include training on real data or in a ‘live’ setting. It seems that training on a simulator is part of the validation of the algorithm rather than the prime contribution of the paper in the entertainment domain.</p>
</sec>
<sec id="x1-27000-6.3">
<label>6.3.</label>
<title>Evaluation</title>
<p>In investigating evaluation rigor, we first turn to the data on which evaluations are based. Figure <xref rid="x1-27002-10">10</xref> shows how many studies include an evaluation in a ‘live’ setting or using existing interactions with users. In the years up to 2007 few studies were done and most of these included realistic evaluations. In more recent years, the absolute number of studies shows a marked upward trend to which the relative number of articles that include a realistic evaluation fails to keep pace. Figure <xref rid="x1-27002-10">10</xref> also shows the number of realistic evaluations per domain. Disregarding the smart home domain, as it contains only four studies, the highest ratio of real evaluations can be found in the commerce and entertainment domains, followed by the health domain.</p>
<table-wrap id="x1-27001-5">
<label>Table 5</label>
<caption>
<p>Number of models and the inclusion of user traits</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<td valign="top" align="left" rowspan="3">Traits of users were used</td>
<td valign="top" align="center" colspan="4">Number of models</td>
</tr>
<tr>
<td valign="top" colspan="4"><hr/></td>
</tr>
<tr>
<td valign="top" align="center">1</td>
<td valign="top" align="center">1/group</td>
<td valign="top" align="center">1/person</td>
<td valign="top" align="center">Multiple</td>
</tr>
</thead>
<tbody>
<tr>
<td valign="top" align="left">In state representation</td>
<td valign="top" align="char" char=".">38</td>
<td valign="top" align="char" char=".">8</td>
<td valign="top" align="char" char=".">28</td>
<td valign="top" align="center">2</td>
</tr>
<tr>
<td valign="top" align="left">Other</td>
<td valign="top" align="char" char=".">5</td>
<td valign="top" align="char" char=".">0</td>
<td valign="top" align="char" char=".">9</td>
<td valign="top" align="center">3</td>
</tr>
<tr>
<td valign="top" align="left">Not used</td>
<td valign="top" align="char" char=".">48</td>
<td valign="top" align="char" char=".">3</td>
<td valign="top" align="char" char=".">22</td>
<td valign="top" align="center">0</td>
</tr>
<tr>
<td valign="top" align="left">Total</td>
<td valign="top" align="char" char=".">91</td>
<td valign="top" align="char" char=".">11</td>
<td valign="top" align="char" char=".">59</td>
<td valign="top" align="center">5</td>
</tr>
</tbody>
</table>
</table-wrap>
<fig id="x1-27002-10">
<label>Fig. 10.</label>
<caption>
<p>Number of papers with a ‘live’ evaluation or evaluation using data on user responses to system behavior.</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g013.jpg"/>
</fig>
<p>We look at possible reasons for a lack of realistic evaluation using our categorization of settings from Section <xref rid="x1-17000-4">4</xref>. Indeed, there are 63 studies with no realistic evaluation versus 104 with a realistic evaluation. Because these group sizes differ, we include ratios with respect to these totals in Table <xref rid="x1-27003-6">6</xref>. The biggest difference between ratios of studies with and without a realistic evaluation is in the upfront availability of data on interactions with users. This is not surprising, as it is natural to use existing interactions for evaluation when they are available already. The second biggest difference between the groups is whether safety is mentioned as a concern. Relatively, studies that refrain from a realistic evaluation mention safety concerns almost twice as often as studies that do a realistic evaluation. The third biggest difference can be found in availability of user models. If a model is available, user responses can be simulated more easily. Privacy concerns are not mentioned frequently, so little can be said on its contribution to a lacking realistic evaluation. Finally and surprisingly, the ease of sampling interactions is comparable between studies with a realistic and without realistic evaluation.</p>
<table-wrap id="x1-27003-6">
<label>Table 6</label>
<caption>
<p>Comparison of settings with realistic and other evaluation</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<td valign="top" align="left"/>
<td valign="top" align="center" colspan="2">Real-world evaluation</td>
<td valign="top" align="center" colspan="2">Other evaluation</td>
</tr>
<tr>
<td valign="top"/>
<td valign="top" colspan="2"><hr/></td>
<td valign="top" colspan="2"><hr/></td>
</tr>
<tr>
<td valign="top" align="left"/>
<td valign="top" align="center">Count</td>
<td valign="top" align="center">% of column total</td>
<td valign="top" align="center">Count</td>
<td valign="top" align="center">% of column total</td>
</tr>
</thead>
<tbody>
<tr>
<td valign="top" align="left">Total</td>
<td valign="top" align="char" char=".">104</td>
<td valign="top" align="char" char=".">100.0%</td>
<td valign="top" align="char" char=".">63</td>
<td valign="top" align="char" char=".">100.0%</td>
</tr>
<tr>
<td valign="top" align="left">Data on user responses to system behavior are available</td>
<td valign="top" align="char" char=".">57</td>
<td valign="top" align="char" char=".">54.8%</td>
<td valign="top" align="char" char=".">9</td>
<td valign="top" align="char" char=".">14.5%</td>
</tr>
<tr>
<td valign="top" align="left">Safety is mentioned as a concern in the article</td>
<td valign="top" align="char" char=".">14</td>
<td valign="top" align="char" char=".">13.5%</td>
<td valign="top" align="char" char=".">16</td>
<td valign="top" align="char" char=".">25.8%</td>
</tr>
<tr>
<td valign="top" align="left">Models of user responses to system behavior are available</td>
<td valign="top" align="char" char=".">21</td>
<td valign="top" align="char" char=".">20.2%</td>
<td valign="top" align="char" char=".">20</td>
<td valign="top" align="char" char=".">32.3%</td>
</tr>
<tr>
<td valign="top" align="left">Privacy is mentioned as a concern in the article</td>
<td valign="top" align="char" char=".">7</td>
<td valign="top" align="char" char=".">6.7%</td>
<td valign="top" align="char" char=".">2</td>
<td valign="top" align="char" char=".">3.2%</td>
</tr>
<tr>
<td valign="top" align="left">New interactions with users can be sampled with ease</td>
<td valign="top" align="char" char=".">60</td>
<td valign="top" align="char" char=".">57.7%</td>
<td valign="top" align="char" char=".">37</td>
<td valign="top" align="char" char=".">59.7%</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Figure <xref rid="x1-27004-11">11</xref> describes how many studies include any of the comparisons in scope in this survey, that is: comparisons between solutions with and without personalization, comparisons between RL approaches and other approaches to personalization and comparisons between different RL algorithms. In the first years, no papers includes such a comparison. The period 2000-2010 contains relatively little studies in general and the absolute and relative numbers of studies with a comparison vary. From 2011 to 2018, the absolute number maintains it upward trend. The relative number follows this trend but flattens after 2016.</p>
<fig id="x1-27004-11">
<label>Fig. 11.</label>
<caption>
<p>Number of papers that include any comparison between solutions over time.</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g014.jpg"/>
</fig>
</sec>
</sec>
<sec id="x1-28000-7">
<label>7.</label>
<title>Discussion</title>
<p>The goal of this study was to give an overview and categorization of RL applications for personalization in different application domains which we addressed using a SLR on settings, solution architectures and evaluation strategies. The main result is the marked increase in studies that use RL for personalization problems over time. Additionally, techniques are increasingly evaluated on real-life data. RL has proven a suitable paradigm for adaptation of systems to individual preferences using data.</p>
<p>Results further indicate that this development is driven by various techniques, which we list in no particular order. Firstly, techniques have been developed to estimate the performance of deploying a particular RL model prior to deployment. This helps in communicating risks and benefits of RL solutions with stakeholders and moves RL further into the realm of feasible technologies for high-impact application domains [<xref ref-type="bibr" rid="ref200">200</xref>]. For single-step decision making problems, contextual bandit algorithms with theoretical bounds on decision-theoretic regret have become available. For multi-step decision making problems, methods that can estimate the performance of some policy based on data generated by another policy have been developed [<xref ref-type="bibr" rid="ref037">37</xref>,<xref ref-type="bibr" rid="ref090">90</xref>,<xref ref-type="bibr" rid="ref204">204</xref>]. Secondly, advances in the field of deep learning have wholly or partly removed the need for feature engineering [<xref ref-type="bibr" rid="ref053">53</xref>]. This may be especially challenging for sequential decision-making problems as different features may be of importance in different states encountered over time. Finally, research on safe exploration in RL has developed means to avoid harmful actions during exploratory phases of learning [<xref ref-type="bibr" rid="ref064">64</xref>]. How any these techniques are best applied depends on setting. The collected data can be used to find suitable related work for any particular setting [<xref ref-type="bibr" rid="ref046">46</xref>].</p>
<p>Since the field of RL for personalization is growing in size, we investigated whether methodological maturity is keeping pace. Results show that the growth in the <italic>number</italic> of studies with a real-life evaluation is not mirrored by growth of the <italic>ratio</italic> of studies with such an evaluation. Similarly, results show no increase in the relative number of studies with a comparison of approaches over time. These may be signs that the maturity of the field fails to keep pace with its growth. This is worrisome, since the advantages of RL over other approaches or between RL algorithms cannot be understood properly without such comparisons. Such comparisons benefit from standardized tasks. Developing standardized personalization datasets and simulation environments is an excellent opportunity for future research [<xref ref-type="bibr" rid="ref087">87</xref>,<xref ref-type="bibr" rid="ref112">112</xref>].</p>
<p>We found that algorithms presented in literature are reused infrequently. Although this phenomenon may be driven by various different underlying dynamics that cannot be untangled using our data, we propose some possible explanations here without particular order. Firstly, it might be the case that separate applications require tailored algorithms to the extend that these can only be used once. This raises the question on the scientific contribution of such a tailored algorithm and does not fit with the reuse of some well-established algorithms. Another explanation is that top-ranked venues prefer contributions that are theoretical or technical in nature, resulting in minor variations to well-known algorithms being presented as novel. Whether this is the case is out of scope for this research and forms an excellent avenue for future work. A final explanation for us to propose, is the myriad axes along which any RL algorithm can be identified, such as whether and where estimation is involved, which estimation technique is used and how domain knowledge is encoded in the algorithm. This may yield a large number of unique algorithms, constructed out of a relatively small set of core ideas in RL. An overview of these core ideas would be useful in understanding how individual algorithms relate to each other.</p>
<p>On top of algorithm reuse, we analyzed which RL algorithms were used most frequently. Generic and well-established (families of) algorithms such as Q-learning are the most popular. A notable entry in the top six most-used techniques is inverse reinforcement learning (IRL). Its frequent usage is surprising, as the only viable application area of IRL under a decade ago was robotics [<xref ref-type="bibr" rid="ref097">97</xref>]. Personalization may be one of the other useful application areas of this branch of RL and many existing personalization challenges may still benefit from an IRL approach. Finally, we investigated how many RL models were included in the proposed solutions and found that the majority of studies resorts to using either one RL model in total or one RL model per user. Inspired by common practice of clustering in the related fields such as e.g. recommender systems, we believe that there exists opportunities in pooling data of similar users and training RL models on the pooled data.</p>
<p>Besides these findings, we contribute a categorization of personalization settings in RL. This framework can be used to find related work based on the setting of a problem at hand. In designing such a framework, one has to balance specificity and usefulness of aspects in the framework. We take the aspect of ‘safety’ as an example: any application of RL will imply safety concerns at some level, but they are more prominent in some application areas. The framework intentionally includes a single ambiguous aspect to describe a broad range ‘safety sensitivity levels’ in order for it to suit its purpose of navigating literature. A possibility for future work is to extend the framework with other, more formal, aspects of problem setting such as those identified in [<xref ref-type="bibr" rid="ref170">170</xref>].</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>The authors would like to thank Frank van Harmelen for useful feedback on the presented classification of personalization settings.</p>
<p>The authors declare that they have no conflict of interest.</p></ack>
<app-group>
<app id="x1-31000-A"><label>Appendix A.</label>
<title>Queries</title>
<fig id="x1-31001-1">
<label>Listing 1.</label>
<caption>
<p>Query for Scopus database</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g015.jpg"/>
</fig> 
<fig id="x1-31002-2">
<label>Listing 2.</label>
<caption>
<p>Query for IEEE Xplore database command search</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g016.jpg"/>
</fig> 
<fig id="x1-31003-3">
<label>Listing 3.</label>
<caption>
<p>Query for ACM DL database</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g017.jpg"/>
</fig> 
<fig id="x1-31004-4">
<label>Listing 4.</label>
<caption>
<p>First query for DBLP database</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g018.jpg"/>
</fig> 
<fig id="x1-31005-5">
<label>Listing 5.</label>
<caption>
<p>Second query for DBLP database</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g019.jpg"/>
</fig> 
<fig id="x1-31006-6">
<label>Listing 6.</label>
<caption>
<p>First query for Google Scholar database</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g020.jpg"/>
</fig> 
<fig id="x1-31007-7">
<label>Listing 7.</label>
<caption>
<p>Second query for Google Scholar database</p>
</caption>
<graphic xlink:href="ds-3-ds200028-g021.jpg"/>
</fig>
</app>
<app id="x1-32000-B"><label>Appendix B.</label>
<title>Tabular view of data</title>
<table-wrap id="x1-32001-7">
<label>Table 7</label>
<caption>
<p>Table containing all included publications. The first column refers to the data items in Table <xref rid="x1-22001-2">2</xref></p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<td valign="top" align="left">#</td>
<td valign="top" align="center">Value</td>
<td valign="top" align="center">Publications</td>
</tr>
</thead>
<tbody>
<tr>
<td valign="top" align="left" rowspan="2">1</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>–<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref024">24</xref>–<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>–<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref056">56</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref106">106</xref>–<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref115">115</xref>–<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>–<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref137">137</xref>–<xref ref-type="bibr" rid="ref141">141</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref155">155</xref>–<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref167">167</xref>,<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>–<xref ref-type="bibr" rid="ref175">175</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref182">182</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref192">192</xref>–<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref205">205</xref>–<xref ref-type="bibr" rid="ref207">207</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref215">215</xref>,<xref ref-type="bibr" rid="ref216">216</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref221">221</xref>–<xref ref-type="bibr" rid="ref225">225</xref>,<xref ref-type="bibr" rid="ref227">227</xref>,<xref ref-type="bibr" rid="ref230">230</xref>–<xref ref-type="bibr" rid="ref232">232</xref>,<xref ref-type="bibr" rid="ref234">234</xref>–<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref033">33</xref>,<xref ref-type="bibr" rid="ref034">34</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref057">57</xref>–<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref168">168</xref>,<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref208">208</xref>–<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref214">214</xref>,<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref226">226</xref>,<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref233">233</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">2</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref024">24</xref>,<xref ref-type="bibr" rid="ref028">28</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>,<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref057">57</xref>,<xref ref-type="bibr" rid="ref059">59</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref126">126</xref>,<xref ref-type="bibr" rid="ref129">129</xref>,<xref ref-type="bibr" rid="ref131">131</xref>,<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref214">214</xref>,<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref226">226</xref>–<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref233">233</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>–<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref025">25</xref>–<xref ref-type="bibr" rid="ref027">27</xref>,<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>–<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref041">41</xref>–<xref ref-type="bibr" rid="ref044">44</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref056">56</xref>,<xref ref-type="bibr" rid="ref058">58</xref>,<xref ref-type="bibr" rid="ref060">60</xref>–<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref106">106</xref>–<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref115">115</xref>–<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref119">119</xref>–<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>–<xref ref-type="bibr" rid="ref125">125</xref>,<xref ref-type="bibr" rid="ref127">127</xref>,<xref ref-type="bibr" rid="ref128">128</xref>,<xref ref-type="bibr" rid="ref130">130</xref>,<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref137">137</xref>–<xref ref-type="bibr" rid="ref141">141</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>–<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref167">167</xref>–<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>–<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref182">182</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref192">192</xref>–<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref205">205</xref>–<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref215">215</xref>,<xref ref-type="bibr" rid="ref216">216</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref221">221</xref>–<xref ref-type="bibr" rid="ref225">225</xref>,<xref ref-type="bibr" rid="ref229">229</xref>–<xref ref-type="bibr" rid="ref232">232</xref>,<xref ref-type="bibr" rid="ref234">234</xref>–<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">3</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref025">25</xref>–<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref033">33</xref>–<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>,<xref ref-type="bibr" rid="ref042">42</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref056">56</xref>–<xref ref-type="bibr" rid="ref061">61</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref072">72</xref>–<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref107">107</xref>–<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref115">115</xref>,<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>–<xref ref-type="bibr" rid="ref125">125</xref>,<xref ref-type="bibr" rid="ref127">127</xref>–<xref ref-type="bibr" rid="ref131">131</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref137">137</xref>–<xref ref-type="bibr" rid="ref139">139</xref>,<xref ref-type="bibr" rid="ref141">141</xref>,<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>,<xref ref-type="bibr" rid="ref157">157</xref>–<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref167">167</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>–<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref183">183</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref192">192</xref>–<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref205">205</xref>,<xref ref-type="bibr" rid="ref207">207</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref214">214</xref>–<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref221">221</xref>–<xref ref-type="bibr" rid="ref231">231</xref>,<xref ref-type="bibr" rid="ref233">233</xref>–<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref024">24</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref041">41</xref>,<xref ref-type="bibr" rid="ref043">43</xref>–<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref116">116</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref126">126</xref>,<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref140">140</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref156">156</xref>,<xref ref-type="bibr" rid="ref168">168</xref>,<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref206">206</xref>,<xref ref-type="bibr" rid="ref208">208</xref>–<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref232">232</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">4</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>–<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref024">24</xref>–<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>–<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>–<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref056">56</xref>–<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref108">108</xref>–<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>–<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref119">119</xref>–<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>–<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref137">137</xref>–<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>–<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref167">167</xref>–<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref173">173</xref>–<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref182">182</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref192">192</xref>–<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref205">205</xref>–<xref ref-type="bibr" rid="ref208">208</xref>,<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref213">213</xref>–<xref ref-type="bibr" rid="ref216">216</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref221">221</xref>–<xref ref-type="bibr" rid="ref239">239</xref>,<xref ref-type="bibr" rid="ref241">241</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref107">107</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref209">209</xref>,<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref240">240</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">5</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref024">24</xref>–<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref033">33</xref>–<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>,<xref ref-type="bibr" rid="ref042">42</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref056">56</xref>,<xref ref-type="bibr" rid="ref057">57</xref>,<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref106">106</xref>–<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>–<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>,<xref ref-type="bibr" rid="ref127">127</xref>–<xref ref-type="bibr" rid="ref129">129</xref>,<xref ref-type="bibr" rid="ref131">131</xref>,<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref137">137</xref>–<xref ref-type="bibr" rid="ref139">139</xref>,<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>,<xref ref-type="bibr" rid="ref156">156</xref>,<xref ref-type="bibr" rid="ref158">158</xref>–<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref167">167</xref>–<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>–<xref ref-type="bibr" rid="ref175">175</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref184">184</xref>,<xref ref-type="bibr" rid="ref192">192</xref>,<xref ref-type="bibr" rid="ref193">193</xref>,<xref ref-type="bibr" rid="ref195">195</xref>–<xref ref-type="bibr" rid="ref197">197</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref205">205</xref>–<xref ref-type="bibr" rid="ref207">207</xref>,<xref ref-type="bibr" rid="ref209">209</xref>,<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref213">213</xref>–<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref221">221</xref>–<xref ref-type="bibr" rid="ref223">223</xref>,<xref ref-type="bibr" rid="ref227">227</xref>,<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref231">231</xref>,<xref ref-type="bibr" rid="ref233">233</xref>–<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref041">41</xref>,<xref ref-type="bibr" rid="ref043">43</xref>–<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref058">58</xref>–<xref ref-type="bibr" rid="ref061">61</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref124">124</xref>–<xref ref-type="bibr" rid="ref126">126</xref>,<xref ref-type="bibr" rid="ref130">130</xref>,<xref ref-type="bibr" rid="ref140">140</xref>,<xref ref-type="bibr" rid="ref141">141</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref157">157</xref>,<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref194">194</xref>,<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref208">208</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref224">224</xref>–<xref ref-type="bibr" rid="ref226">226</xref>,<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref232">232</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">6</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref024">24</xref>,<xref ref-type="bibr" rid="ref026">26</xref>–<xref ref-type="bibr" rid="ref028">28</xref>,<xref ref-type="bibr" rid="ref031">31</xref>–<xref ref-type="bibr" rid="ref033">33</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>–<xref ref-type="bibr" rid="ref042">42</xref>,<xref ref-type="bibr" rid="ref044">44</xref>,<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref056">56</xref>,<xref ref-type="bibr" rid="ref057">57</xref>,<xref ref-type="bibr" rid="ref059">59</xref>,<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref116">116</xref>,<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref119">119</xref>–<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>–<xref ref-type="bibr" rid="ref126">126</xref>,<xref ref-type="bibr" rid="ref128">128</xref>,<xref ref-type="bibr" rid="ref130">130</xref>–<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref138">138</xref>–<xref ref-type="bibr" rid="ref141">141</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref156">156</xref>,<xref ref-type="bibr" rid="ref157">157</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref167">167</xref>,<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref175">175</xref>–<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref182">182</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref193">193</xref>,<xref ref-type="bibr" rid="ref205">205</xref>–<xref ref-type="bibr" rid="ref208">208</xref>,<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref214">214</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref221">221</xref>,<xref ref-type="bibr" rid="ref222">222</xref>,<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref232">232</xref>,<xref ref-type="bibr" rid="ref234">234</xref>–<xref ref-type="bibr" rid="ref238">238</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref025">25</xref>,<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref034">34</xref>–<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref043">43</xref>,<xref ref-type="bibr" rid="ref058">58</xref>,<xref ref-type="bibr" rid="ref060">60</xref>,<xref ref-type="bibr" rid="ref061">61</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref072">72</xref>–<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref107">107</xref>,<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>–<xref ref-type="bibr" rid="ref115">115</xref>,<xref ref-type="bibr" rid="ref127">127</xref>,<xref ref-type="bibr" rid="ref129">129</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref137">137</xref>,<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>,<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref168">168</xref>,<xref ref-type="bibr" rid="ref173">173</xref>,<xref ref-type="bibr" rid="ref174">174</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref192">192</xref>,<xref ref-type="bibr" rid="ref194">194</xref>–<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref209">209</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref215">215</xref>–<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref223">223</xref>–<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref231">231</xref>,<xref ref-type="bibr" rid="ref233">233</xref>,<xref ref-type="bibr" rid="ref239">239</xref>–<xref ref-type="bibr" rid="ref241">241</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">7</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref027">27</xref>–<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>,<xref ref-type="bibr" rid="ref043">43</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref057">57</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref116">116</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>,<xref ref-type="bibr" rid="ref125">125</xref>,<xref ref-type="bibr" rid="ref126">126</xref>,<xref ref-type="bibr" rid="ref131">131</xref>,<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref137">137</xref>,<xref ref-type="bibr" rid="ref139">139</xref>,<xref ref-type="bibr" rid="ref141">141</xref>,<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref155">155</xref>–<xref ref-type="bibr" rid="ref157">157</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref168">168</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref205">205</xref>–<xref ref-type="bibr" rid="ref207">207</xref>,<xref ref-type="bibr" rid="ref209">209</xref>,<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref232">232</xref>,<xref ref-type="bibr" rid="ref234">234</xref>–<xref ref-type="bibr" rid="ref236">236</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref018">18</xref>–<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref024">24</xref>–<xref ref-type="bibr" rid="ref026">26</xref>,<xref ref-type="bibr" rid="ref033">33</xref>,<xref ref-type="bibr" rid="ref034">34</xref>,<xref ref-type="bibr" rid="ref041">41</xref>,<xref ref-type="bibr" rid="ref042">42</xref>,<xref ref-type="bibr" rid="ref044">44</xref>,<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref056">56</xref>,<xref ref-type="bibr" rid="ref058">58</xref>–<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref107">107</xref>,<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref115">115</xref>,<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref124">124</xref>,<xref ref-type="bibr" rid="ref127">127</xref>–<xref ref-type="bibr" rid="ref130">130</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref138">138</xref>,<xref ref-type="bibr" rid="ref140">140</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref167">167</xref>,<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref174">174</xref>–<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref183">183</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref192">192</xref>–<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref208">208</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref214">214</xref>–<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref221">221</xref>–<xref ref-type="bibr" rid="ref227">227</xref>,<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref231">231</xref>,<xref ref-type="bibr" rid="ref233">233</xref>,<xref ref-type="bibr" rid="ref237">237</xref>–<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">8</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref025">25</xref>,<xref ref-type="bibr" rid="ref028">28</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref034">34</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref056">56</xref>,<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref107">107</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref115">115</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref167">167</xref>,<xref ref-type="bibr" rid="ref192">192</xref>,<xref ref-type="bibr" rid="ref195">195</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref215">215</xref>–<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref222">222</xref>,<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref231">231</xref>,<xref ref-type="bibr" rid="ref238">238</xref>,<xref ref-type="bibr" rid="ref240">240</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref024">24</xref>,<xref ref-type="bibr" rid="ref026">26</xref>,<xref ref-type="bibr" rid="ref027">27</xref>,<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref033">33</xref>,<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>–<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref057">57</xref>–<xref ref-type="bibr" rid="ref061">61</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref108">108</xref>–<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref116">116</xref>,<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref123">123</xref>–<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref137">137</xref>–<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>–<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref160">160</xref>–<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref168">168</xref>,<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>–<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref183">183</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref193">193</xref>,<xref ref-type="bibr" rid="ref194">194</xref>,<xref ref-type="bibr" rid="ref196">196</xref>–<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref205">205</xref>–<xref ref-type="bibr" rid="ref209">209</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref214">214</xref>,<xref ref-type="bibr" rid="ref221">221</xref>,<xref ref-type="bibr" rid="ref223">223</xref>–<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref232">232</xref>–<xref ref-type="bibr" rid="ref237">237</xref>,<xref ref-type="bibr" rid="ref239">239</xref>,<xref ref-type="bibr" rid="ref241">241</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="4">10</td>
<td valign="top" align="left">1</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref026">26</xref>–<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref034">34</xref>,<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>,<xref ref-type="bibr" rid="ref041">41</xref>,<xref ref-type="bibr" rid="ref043">43</xref>,<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref056">56</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref107">107</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref125">125</xref>–<xref ref-type="bibr" rid="ref127">127</xref>,<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref137">137</xref>,<xref ref-type="bibr" rid="ref140">140</xref>,<xref ref-type="bibr" rid="ref141">141</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref157">157</xref>–<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>,<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref182">182</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref192">192</xref>–<xref ref-type="bibr" rid="ref197">197</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref206">206</xref>–<xref ref-type="bibr" rid="ref209">209</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref213">213</xref>–<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref223">223</xref>–<xref ref-type="bibr" rid="ref226">226</xref>,<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref232">232</xref>,<xref ref-type="bibr" rid="ref234">234</xref>,<xref ref-type="bibr" rid="ref235">235</xref>,<xref ref-type="bibr" rid="ref238">238</xref>,<xref ref-type="bibr" rid="ref241">241</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">1/group</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref042">42</xref>,<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref115">115</xref>,<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref123">123</xref>,<xref ref-type="bibr" rid="ref124">124</xref>,<xref ref-type="bibr" rid="ref155">155</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref221">221</xref>,<xref ref-type="bibr" rid="ref236">236</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">1/person</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref024">24</xref>,<xref ref-type="bibr" rid="ref025">25</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref033">33</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref044">44</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref057">57</xref>–<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref072">72</xref>–<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref116">116</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref128">128</xref>–<xref ref-type="bibr" rid="ref131">131</xref>,<xref ref-type="bibr" rid="ref138">138</xref>,<xref ref-type="bibr" rid="ref139">139</xref>,<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref156">156</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref167">167</xref>,<xref ref-type="bibr" rid="ref168">168</xref>,<xref ref-type="bibr" rid="ref174">174</xref>,<xref ref-type="bibr" rid="ref175">175</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref205">205</xref>,<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref222">222</xref>,<xref ref-type="bibr" rid="ref227">227</xref>,<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref231">231</xref>,<xref ref-type="bibr" rid="ref233">233</xref>,<xref ref-type="bibr" rid="ref237">237</xref>,<xref ref-type="bibr" rid="ref239">239</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">multiple</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref240">240</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="3">11</td>
<td valign="top" align="left">not used</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref024">24</xref>,<xref ref-type="bibr" rid="ref026">26</xref>,<xref ref-type="bibr" rid="ref028">28</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref033">33</xref>–<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>,<xref ref-type="bibr" rid="ref042">42</xref>–<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref057">57</xref>,<xref ref-type="bibr" rid="ref059">59</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref115">115</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref127">127</xref>,<xref ref-type="bibr" rid="ref128">128</xref>,<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref137">137</xref>,<xref ref-type="bibr" rid="ref138">138</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref168">168</xref>,<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref173">173</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref192">192</xref>,<xref ref-type="bibr" rid="ref193">193</xref>,<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref205">205</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref214">214</xref>,<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref222">222</xref>,<xref ref-type="bibr" rid="ref223">223</xref>,<xref ref-type="bibr" rid="ref226">226</xref>,<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref237">237</xref>,<xref ref-type="bibr" rid="ref239">239</xref>,<xref ref-type="bibr" rid="ref241">241</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">other</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref025">25</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref130">130</xref>,<xref ref-type="bibr" rid="ref131">131</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref156">156</xref>,<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref195">195</xref>,<xref ref-type="bibr" rid="ref197">197</xref>,<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref232">232</xref>,<xref ref-type="bibr" rid="ref240">240</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">state representation</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref027">27</xref>,<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref041">41</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref056">56</xref>,<xref ref-type="bibr" rid="ref058">58</xref>,<xref ref-type="bibr" rid="ref060">60</xref>–<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref106">106</xref>–<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref116">116</xref>,<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref123">123</xref>–<xref ref-type="bibr" rid="ref126">126</xref>,<xref ref-type="bibr" rid="ref129">129</xref>,<xref ref-type="bibr" rid="ref139">139</xref>–<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>,<xref ref-type="bibr" rid="ref157">157</xref>,<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref167">167</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref174">174</xref>–<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref184">184</xref>,<xref ref-type="bibr" rid="ref194">194</xref>,<xref ref-type="bibr" rid="ref196">196</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref206">206</xref>–<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref215">215</xref>,<xref ref-type="bibr" rid="ref216">216</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref221">221</xref>,<xref ref-type="bibr" rid="ref224">224</xref>,<xref ref-type="bibr" rid="ref225">225</xref>,<xref ref-type="bibr" rid="ref227">227</xref>,<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref231">231</xref>,<xref ref-type="bibr" rid="ref233">233</xref>–<xref ref-type="bibr" rid="ref236">236</xref>,<xref ref-type="bibr" rid="ref238">238</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="5">12</td>
<td valign="top" align="left">batch</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref034">34</xref>,<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref040">40</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref056">56</xref>,<xref ref-type="bibr" rid="ref058">58</xref>–<xref ref-type="bibr" rid="ref061">61</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref116">116</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>–<xref ref-type="bibr" rid="ref126">126</xref>,<xref ref-type="bibr" rid="ref129">129</xref>,<xref ref-type="bibr" rid="ref140">140</xref>,<xref ref-type="bibr" rid="ref141">141</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref157">157</xref>,<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref183">183</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref192">192</xref>,<xref ref-type="bibr" rid="ref193">193</xref>,<xref ref-type="bibr" rid="ref196">196</xref>–<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref206">206</xref>,<xref ref-type="bibr" rid="ref215">215</xref>,<xref ref-type="bibr" rid="ref216">216</xref>,<xref ref-type="bibr" rid="ref223">223</xref>,<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref234">234</xref>–<xref ref-type="bibr" rid="ref236">236</xref>,<xref ref-type="bibr" rid="ref238">238</xref>,<xref ref-type="bibr" rid="ref240">240</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref222">222</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">online</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref018">18</xref>–<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref024">24</xref>–<xref ref-type="bibr" rid="ref026">26</xref>,<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref033">33</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref041">41</xref>,<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref057">57</xref>,<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref107">107</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>–<xref ref-type="bibr" rid="ref115">115</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref128">128</xref>,<xref ref-type="bibr" rid="ref130">130</xref>–<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref137">137</xref>,<xref ref-type="bibr" rid="ref138">138</xref>,<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref167">167</xref>–<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>–<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref194">194</xref>,<xref ref-type="bibr" rid="ref195">195</xref>,<xref ref-type="bibr" rid="ref208">208</xref>,<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref224">224</xref>–<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref233">233</xref>,<xref ref-type="bibr" rid="ref237">237</xref>,<xref ref-type="bibr" rid="ref239">239</xref>,<xref ref-type="bibr" rid="ref241">241</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">other</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref044">44</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref139">139</xref>,<xref ref-type="bibr" rid="ref155">155</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref231">231</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">unknown</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref027">27</xref>,<xref ref-type="bibr" rid="ref028">28</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref042">42</xref>,<xref ref-type="bibr" rid="ref043">43</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref127">127</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref156">156</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref205">205</xref>,<xref ref-type="bibr" rid="ref207">207</xref>,<xref ref-type="bibr" rid="ref209">209</xref>,<xref ref-type="bibr" rid="ref214">214</xref>,<xref ref-type="bibr" rid="ref221">221</xref>,<xref ref-type="bibr" rid="ref232">232</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">13</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref026">26</xref>–<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref033">33</xref>–<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref040">40</xref>,<xref ref-type="bibr" rid="ref043">43</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref107">107</xref>–<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>,<xref ref-type="bibr" rid="ref125">125</xref>–<xref ref-type="bibr" rid="ref129">129</xref>,<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref137">137</xref>,<xref ref-type="bibr" rid="ref140">140</xref>–<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>,<xref ref-type="bibr" rid="ref157">157</xref>,<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref160">160</xref>–<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>–<xref ref-type="bibr" rid="ref175">175</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref196">196</xref>–<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref207">207</xref>,<xref ref-type="bibr" rid="ref209">209</xref>,<xref ref-type="bibr" rid="ref213">213</xref>–<xref ref-type="bibr" rid="ref215">215</xref>,<xref ref-type="bibr" rid="ref221">221</xref>–<xref ref-type="bibr" rid="ref226">226</xref>,<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref230">230</xref>–<xref ref-type="bibr" rid="ref233">233</xref>,<xref ref-type="bibr" rid="ref235">235</xref>,<xref ref-type="bibr" rid="ref238">238</xref>,<xref ref-type="bibr" rid="ref239">239</xref>,<xref ref-type="bibr" rid="ref241">241</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref024">24</xref>,<xref ref-type="bibr" rid="ref025">25</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref041">41</xref>,<xref ref-type="bibr" rid="ref042">42</xref>,<xref ref-type="bibr" rid="ref044">44</xref>,<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref056">56</xref>–<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref115">115</xref>,<xref ref-type="bibr" rid="ref116">116</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref124">124</xref>,<xref ref-type="bibr" rid="ref130">130</xref>,<xref ref-type="bibr" rid="ref131">131</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref138">138</xref>,<xref ref-type="bibr" rid="ref139">139</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref156">156</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref167">167</xref>–<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref183">183</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref192">192</xref>–<xref ref-type="bibr" rid="ref195">195</xref>,<xref ref-type="bibr" rid="ref205">205</xref>,<xref ref-type="bibr" rid="ref206">206</xref>,<xref ref-type="bibr" rid="ref208">208</xref>,<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref216">216</xref>–<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref227">227</xref>,<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref234">234</xref>,<xref ref-type="bibr" rid="ref236">236</xref>,<xref ref-type="bibr" rid="ref237">237</xref>,<xref ref-type="bibr" rid="ref240">240</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">14</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref024">24</xref>–<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>–<xref ref-type="bibr" rid="ref034">34</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref041">41</xref>,<xref ref-type="bibr" rid="ref042">42</xref>,<xref ref-type="bibr" rid="ref044">44</xref>,<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref056">56</xref>–<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref072">72</xref>–<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref115">115</xref>–<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref124">124</xref>,<xref ref-type="bibr" rid="ref130">130</xref>–<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref138">138</xref>,<xref ref-type="bibr" rid="ref139">139</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref156">156</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref167">167</xref>–<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>,<xref ref-type="bibr" rid="ref174">174</xref>,<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref192">192</xref>,<xref ref-type="bibr" rid="ref205">205</xref>,<xref ref-type="bibr" rid="ref207">207</xref>–<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref214">214</xref>,<xref ref-type="bibr" rid="ref216">216</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref222">222</xref>,<xref ref-type="bibr" rid="ref226">226</xref>–<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref234">234</xref>,<xref ref-type="bibr" rid="ref236">236</xref>–<xref ref-type="bibr" rid="ref239">239</xref>,<xref ref-type="bibr" rid="ref241">241</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref040">40</xref>,<xref ref-type="bibr" rid="ref043">43</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref107">107</xref>–<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>,<xref ref-type="bibr" rid="ref125">125</xref>–<xref ref-type="bibr" rid="ref129">129</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref137">137</xref>,<xref ref-type="bibr" rid="ref140">140</xref>–<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref155">155</xref>,<xref ref-type="bibr" rid="ref157">157</xref>,<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref175">175</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref183">183</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref193">193</xref>–<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref206">206</xref>,<xref ref-type="bibr" rid="ref215">215</xref>,<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref221">221</xref>,<xref ref-type="bibr" rid="ref223">223</xref>–<xref ref-type="bibr" rid="ref225">225</xref>,<xref ref-type="bibr" rid="ref230">230</xref>–<xref ref-type="bibr" rid="ref233">233</xref>,<xref ref-type="bibr" rid="ref235">235</xref>,<xref ref-type="bibr" rid="ref240">240</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">15</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref024">24</xref>,<xref ref-type="bibr" rid="ref025">25</xref>,<xref ref-type="bibr" rid="ref027">27</xref>,<xref ref-type="bibr" rid="ref028">28</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref034">34</xref>,<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>–<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref056">56</xref>,<xref ref-type="bibr" rid="ref057">57</xref>,<xref ref-type="bibr" rid="ref059">59</xref>,<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref107">107</xref>–<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref113">113</xref>–<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>–<xref ref-type="bibr" rid="ref131">131</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref137">137</xref>–<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>–<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref167">167</xref>–<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref175">175</xref>–<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref183">183</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref192">192</xref>–<xref ref-type="bibr" rid="ref195">195</xref>,<xref ref-type="bibr" rid="ref197">197</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref205">205</xref>–<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref214">214</xref>–<xref ref-type="bibr" rid="ref216">216</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref221">221</xref>–<xref ref-type="bibr" rid="ref225">225</xref>,<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref232">232</xref>–<xref ref-type="bibr" rid="ref236">236</xref>,<xref ref-type="bibr" rid="ref238">238</xref>,<xref ref-type="bibr" rid="ref240">240</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref026">26</xref>,<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref033">33</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref058">58</xref>,<xref ref-type="bibr" rid="ref060">60</xref>,<xref ref-type="bibr" rid="ref061">61</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>,<xref ref-type="bibr" rid="ref174">174</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref196">196</xref>,<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref226">226</xref>–<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref231">231</xref>,<xref ref-type="bibr" rid="ref237">237</xref>,<xref ref-type="bibr" rid="ref239">239</xref>,<xref ref-type="bibr" rid="ref241">241</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">16</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref026">26</xref>–<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref033">33</xref>–<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref040">40</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref107">107</xref>,<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>,<xref ref-type="bibr" rid="ref125">125</xref>–<xref ref-type="bibr" rid="ref129">129</xref>,<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref137">137</xref>,<xref ref-type="bibr" rid="ref140">140</xref>–<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>,<xref ref-type="bibr" rid="ref157">157</xref>,<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref160">160</xref>–<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>–<xref ref-type="bibr" rid="ref175">175</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref196">196</xref>–<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref207">207</xref>,<xref ref-type="bibr" rid="ref209">209</xref>,<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref213">213</xref>–<xref ref-type="bibr" rid="ref215">215</xref>,<xref ref-type="bibr" rid="ref221">221</xref>–<xref ref-type="bibr" rid="ref226">226</xref>,<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref231">231</xref>–<xref ref-type="bibr" rid="ref233">233</xref>,<xref ref-type="bibr" rid="ref235">235</xref>,<xref ref-type="bibr" rid="ref238">238</xref>,<xref ref-type="bibr" rid="ref239">239</xref>,<xref ref-type="bibr" rid="ref241">241</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref024">24</xref>,<xref ref-type="bibr" rid="ref025">25</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref041">41</xref>–<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref056">56</xref>–<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref115">115</xref>,<xref ref-type="bibr" rid="ref116">116</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref124">124</xref>,<xref ref-type="bibr" rid="ref130">130</xref>,<xref ref-type="bibr" rid="ref131">131</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref138">138</xref>,<xref ref-type="bibr" rid="ref139">139</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref156">156</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref167">167</xref>–<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref183">183</xref>–<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref192">192</xref>–<xref ref-type="bibr" rid="ref195">195</xref>,<xref ref-type="bibr" rid="ref205">205</xref>,<xref ref-type="bibr" rid="ref206">206</xref>,<xref ref-type="bibr" rid="ref208">208</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref216">216</xref>–<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref227">227</xref>,<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref234">234</xref>,<xref ref-type="bibr" rid="ref236">236</xref>,<xref ref-type="bibr" rid="ref237">237</xref>,<xref ref-type="bibr" rid="ref240">240</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">17</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref024">24</xref>–<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>–<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref041">41</xref>–<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref056">56</xref>–<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref072">72</xref>–<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref115">115</xref>–<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref124">124</xref>,<xref ref-type="bibr" rid="ref130">130</xref>–<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref137">137</xref>–<xref ref-type="bibr" rid="ref139">139</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref156">156</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref167">167</xref>–<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>,<xref ref-type="bibr" rid="ref174">174</xref>,<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref184">184</xref>,<xref ref-type="bibr" rid="ref192">192</xref>,<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref205">205</xref>,<xref ref-type="bibr" rid="ref207">207</xref>–<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref214">214</xref>,<xref ref-type="bibr" rid="ref216">216</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref222">222</xref>,<xref ref-type="bibr" rid="ref226">226</xref>–<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref234">234</xref>,<xref ref-type="bibr" rid="ref237">237</xref>–<xref ref-type="bibr" rid="ref239">239</xref>,<xref ref-type="bibr" rid="ref241">241</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref040">40</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref107">107</xref>,<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>,<xref ref-type="bibr" rid="ref125">125</xref>–<xref ref-type="bibr" rid="ref129">129</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref140">140</xref>–<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref155">155</xref>,<xref ref-type="bibr" rid="ref157">157</xref>,<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref175">175</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref193">193</xref>–<xref ref-type="bibr" rid="ref197">197</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref206">206</xref>,<xref ref-type="bibr" rid="ref215">215</xref>,<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref221">221</xref>,<xref ref-type="bibr" rid="ref223">223</xref>–<xref ref-type="bibr" rid="ref225">225</xref>,<xref ref-type="bibr" rid="ref231">231</xref>–<xref ref-type="bibr" rid="ref233">233</xref>,<xref ref-type="bibr" rid="ref235">235</xref>,<xref ref-type="bibr" rid="ref236">236</xref>,<xref ref-type="bibr" rid="ref240">240</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">18</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref024">24</xref>,<xref ref-type="bibr" rid="ref025">25</xref>,<xref ref-type="bibr" rid="ref027">27</xref>,<xref ref-type="bibr" rid="ref028">28</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>–<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref056">56</xref>,<xref ref-type="bibr" rid="ref057">57</xref>,<xref ref-type="bibr" rid="ref059">59</xref>,<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref107">107</xref>,<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref113">113</xref>–<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>–<xref ref-type="bibr" rid="ref131">131</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref138">138</xref>–<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>–<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref167">167</xref>,<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref175">175</xref>–<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref184">184</xref>,<xref ref-type="bibr" rid="ref192">192</xref>–<xref ref-type="bibr" rid="ref195">195</xref>,<xref ref-type="bibr" rid="ref197">197</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref205">205</xref>–<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref214">214</xref>–<xref ref-type="bibr" rid="ref216">216</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref221">221</xref>–<xref ref-type="bibr" rid="ref223">223</xref>,<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref232">232</xref>–<xref ref-type="bibr" rid="ref236">236</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref026">26</xref>,<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref033">33</xref>–<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref058">58</xref>,<xref ref-type="bibr" rid="ref060">60</xref>,<xref ref-type="bibr" rid="ref061">61</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref137">137</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref168">168</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>,<xref ref-type="bibr" rid="ref174">174</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref196">196</xref>,<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref224">224</xref>–<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref231">231</xref>,<xref ref-type="bibr" rid="ref237">237</xref>–<xref ref-type="bibr" rid="ref241">241</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">19</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref024">24</xref>–<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref040">40</xref>–<xref ref-type="bibr" rid="ref043">43</xref>,<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref056">56</xref>–<xref ref-type="bibr" rid="ref058">58</xref>,<xref ref-type="bibr" rid="ref060">60</xref>–<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>–<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref119">119</xref>–<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>,<xref ref-type="bibr" rid="ref127">127</xref>–<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref137">137</xref>,<xref ref-type="bibr" rid="ref138">138</xref>,<xref ref-type="bibr" rid="ref141">141</xref>,<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>,<xref ref-type="bibr" rid="ref156">156</xref>,<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref167">167</xref>–<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>,<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref184">184</xref>,<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref192">192</xref>–<xref ref-type="bibr" rid="ref194">194</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref205">205</xref>,<xref ref-type="bibr" rid="ref207">207</xref>–<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref214">214</xref>,<xref ref-type="bibr" rid="ref216">216</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref221">221</xref>–<xref ref-type="bibr" rid="ref223">223</xref>,<xref ref-type="bibr" rid="ref226">226</xref>,<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref229">229</xref>,<xref ref-type="bibr" rid="ref234">234</xref>,<xref ref-type="bibr" rid="ref235">235</xref>,<xref ref-type="bibr" rid="ref237">237</xref>–<xref ref-type="bibr" rid="ref240">240</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref032">32</xref>–<xref ref-type="bibr" rid="ref034">34</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref044">44</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref059">59</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref107">107</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref124">124</xref>–<xref ref-type="bibr" rid="ref126">126</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref139">139</xref>,<xref ref-type="bibr" rid="ref140">140</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref157">157</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref174">174</xref>,<xref ref-type="bibr" rid="ref175">175</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref195">195</xref>–<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref206">206</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref215">215</xref>,<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref224">224</xref>,<xref ref-type="bibr" rid="ref225">225</xref>,<xref ref-type="bibr" rid="ref227">227</xref>,<xref ref-type="bibr" rid="ref230">230</xref>–<xref ref-type="bibr" rid="ref233">233</xref>,<xref ref-type="bibr" rid="ref236">236</xref>,<xref ref-type="bibr" rid="ref241">241</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="2">20</td>
<td valign="top" align="left">n</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref024">24</xref>–<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>–<xref ref-type="bibr" rid="ref034">34</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref040">40</xref>–<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref056">56</xref>–<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref072">72</xref>–<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref107">107</xref>,<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref116">116</xref>,<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref119">119</xref>–<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>,<xref ref-type="bibr" rid="ref125">125</xref>–<xref ref-type="bibr" rid="ref128">128</xref>,<xref ref-type="bibr" rid="ref131">131</xref>,<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref137">137</xref>–<xref ref-type="bibr" rid="ref140">140</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref155">155</xref>,<xref ref-type="bibr" rid="ref156">156</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref167">167</xref>–<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref194">194</xref>,<xref ref-type="bibr" rid="ref195">195</xref>,<xref ref-type="bibr" rid="ref197">197</xref>,<xref ref-type="bibr" rid="ref198">198</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref205">205</xref>,<xref ref-type="bibr" rid="ref207">207</xref>–<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref213">213</xref>–<xref ref-type="bibr" rid="ref216">216</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref221">221</xref>–<xref ref-type="bibr" rid="ref223">223</xref>,<xref ref-type="bibr" rid="ref226">226</xref>,<xref ref-type="bibr" rid="ref228">228</xref>–<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref234">234</xref>,<xref ref-type="bibr" rid="ref236">236</xref>,<xref ref-type="bibr" rid="ref237">237</xref>,<xref ref-type="bibr" rid="ref239">239</xref>–<xref ref-type="bibr" rid="ref241">241</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">y</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref115">115</xref>,<xref ref-type="bibr" rid="ref124">124</xref>,<xref ref-type="bibr" rid="ref129">129</xref>,<xref ref-type="bibr" rid="ref130">130</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref141">141</xref>,<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref157">157</xref>,<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref171">171</xref>,<xref ref-type="bibr" rid="ref173">173</xref>–<xref ref-type="bibr" rid="ref175">175</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref184">184</xref>,<xref ref-type="bibr" rid="ref192">192</xref>,<xref ref-type="bibr" rid="ref193">193</xref>,<xref ref-type="bibr" rid="ref196">196</xref>,<xref ref-type="bibr" rid="ref206">206</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref224">224</xref>,<xref ref-type="bibr" rid="ref225">225</xref>,<xref ref-type="bibr" rid="ref227">227</xref>,<xref ref-type="bibr" rid="ref231">231</xref>–<xref ref-type="bibr" rid="ref233">233</xref>,<xref ref-type="bibr" rid="ref235">235</xref>,<xref ref-type="bibr" rid="ref238">238</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left" rowspan="10">  Domain</td>
<td valign="top" align="left">Commerce</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref001">1</xref>,<xref ref-type="bibr" rid="ref019">19</xref>,<xref ref-type="bibr" rid="ref027">27</xref>,<xref ref-type="bibr" rid="ref049">49</xref>,<xref ref-type="bibr" rid="ref056">56</xref>,<xref ref-type="bibr" rid="ref082">82</xref>,<xref ref-type="bibr" rid="ref086">86</xref>,<xref ref-type="bibr" rid="ref108">108</xref>,<xref ref-type="bibr" rid="ref114">114</xref>,<xref ref-type="bibr" rid="ref117">117</xref>,<xref ref-type="bibr" rid="ref119">119</xref>,<xref ref-type="bibr" rid="ref120">120</xref>,<xref ref-type="bibr" rid="ref129">129</xref>,<xref ref-type="bibr" rid="ref134">134</xref>,<xref ref-type="bibr" rid="ref144">144</xref>,<xref ref-type="bibr" rid="ref160">160</xref>,<xref ref-type="bibr" rid="ref193">193</xref>,<xref ref-type="bibr" rid="ref196">196</xref>,<xref ref-type="bibr" rid="ref197">197</xref>,<xref ref-type="bibr" rid="ref200">200</xref>,<xref ref-type="bibr" rid="ref201">201</xref>,<xref ref-type="bibr" rid="ref218">218</xref>,<xref ref-type="bibr" rid="ref226">226</xref>,<xref ref-type="bibr" rid="ref227">227</xref>,<xref ref-type="bibr" rid="ref230">230</xref>,<xref ref-type="bibr" rid="ref233">233</xref>,<xref ref-type="bibr" rid="ref235">235</xref>,<xref ref-type="bibr" rid="ref238">238</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">Communication</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref043">43</xref>,<xref ref-type="bibr" rid="ref099">99</xref>,<xref ref-type="bibr" rid="ref104">104</xref>,<xref ref-type="bibr" rid="ref171">171</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">Domain Independent</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref020">20</xref>,<xref ref-type="bibr" rid="ref025">25</xref>,<xref ref-type="bibr" rid="ref036">36</xref>,<xref ref-type="bibr" rid="ref126">126</xref>,<xref ref-type="bibr" rid="ref138">138</xref>,<xref ref-type="bibr" rid="ref139">139</xref>,<xref ref-type="bibr" rid="ref148">148</xref>,<xref ref-type="bibr" rid="ref194">194</xref>,<xref ref-type="bibr" rid="ref224">224</xref>,<xref ref-type="bibr" rid="ref225">225</xref>,<xref ref-type="bibr" rid="ref229">229</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">Education</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref029">29</xref>,<xref ref-type="bibr" rid="ref031">31</xref>,<xref ref-type="bibr" rid="ref035">35</xref>,<xref ref-type="bibr" rid="ref052">52</xref>,<xref ref-type="bibr" rid="ref063">63</xref>,<xref ref-type="bibr" rid="ref074">74</xref>,<xref ref-type="bibr" rid="ref080">80</xref>,<xref ref-type="bibr" rid="ref083">83</xref>,<xref ref-type="bibr" rid="ref093">93</xref>,<xref ref-type="bibr" rid="ref094">94</xref>,<xref ref-type="bibr" rid="ref103">103</xref>,<xref ref-type="bibr" rid="ref115">115</xref>,<xref ref-type="bibr" rid="ref121">121</xref>,<xref ref-type="bibr" rid="ref123">123</xref>,<xref ref-type="bibr" rid="ref124">124</xref>,<xref ref-type="bibr" rid="ref147">147</xref>,<xref ref-type="bibr" rid="ref161">161</xref>,<xref ref-type="bibr" rid="ref162">162</xref>,<xref ref-type="bibr" rid="ref176">176</xref>,<xref ref-type="bibr" rid="ref177">177</xref>,<xref ref-type="bibr" rid="ref184">184</xref>,<xref ref-type="bibr" rid="ref185">185</xref>,<xref ref-type="bibr" rid="ref215">215</xref>,<xref ref-type="bibr" rid="ref216">216</xref>,<xref ref-type="bibr" rid="ref223">223</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">Energy</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref091">91</xref>,<xref ref-type="bibr" rid="ref130">130</xref>,<xref ref-type="bibr" rid="ref131">131</xref>,<xref ref-type="bibr" rid="ref150">150</xref>,<xref ref-type="bibr" rid="ref211">211</xref>,<xref ref-type="bibr" rid="ref237">237</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">Entertainment</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref004">4</xref>,<xref ref-type="bibr" rid="ref010">10</xref>,<xref ref-type="bibr" rid="ref018">18</xref>,<xref ref-type="bibr" rid="ref034">34</xref>,<xref ref-type="bibr" rid="ref038">38</xref>,<xref ref-type="bibr" rid="ref058">58</xref>–<xref ref-type="bibr" rid="ref061">61</xref>,<xref ref-type="bibr" rid="ref070">70</xref>,<xref ref-type="bibr" rid="ref072">72</xref>,<xref ref-type="bibr" rid="ref088">88</xref>,<xref ref-type="bibr" rid="ref107">107</xref>,<xref ref-type="bibr" rid="ref109">109</xref>,<xref ref-type="bibr" rid="ref127">127</xref>,<xref ref-type="bibr" rid="ref128">128</xref>,<xref ref-type="bibr" rid="ref137">137</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref158">158</xref>,<xref ref-type="bibr" rid="ref159">159</xref>,<xref ref-type="bibr" rid="ref167">167</xref>,<xref ref-type="bibr" rid="ref175">175</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref183">183</xref>,<xref ref-type="bibr" rid="ref192">192</xref>,<xref ref-type="bibr" rid="ref195">195</xref>,<xref ref-type="bibr" rid="ref217">217</xref>,<xref ref-type="bibr" rid="ref228">228</xref>,<xref ref-type="bibr" rid="ref231">231</xref>,<xref ref-type="bibr" rid="ref240">240</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">Health</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref003">3</xref>,<xref ref-type="bibr" rid="ref006">6</xref>,<xref ref-type="bibr" rid="ref008">8</xref>,<xref ref-type="bibr" rid="ref011">11</xref>,<xref ref-type="bibr" rid="ref028">28</xref>,<xref ref-type="bibr" rid="ref040">40</xref>–<xref ref-type="bibr" rid="ref042">42</xref>,<xref ref-type="bibr" rid="ref044">44</xref>,<xref ref-type="bibr" rid="ref045">45</xref>,<xref ref-type="bibr" rid="ref048">48</xref>,<xref ref-type="bibr" rid="ref051">51</xref>,<xref ref-type="bibr" rid="ref066">66</xref>,<xref ref-type="bibr" rid="ref068">68</xref>,<xref ref-type="bibr" rid="ref073">73</xref>,<xref ref-type="bibr" rid="ref101">101</xref>,<xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref113">113</xref>,<xref ref-type="bibr" rid="ref116">116</xref>,<xref ref-type="bibr" rid="ref125">125</xref>,<xref ref-type="bibr" rid="ref132">132</xref>,<xref ref-type="bibr" rid="ref140">140</xref>–<xref ref-type="bibr" rid="ref142">142</xref>,<xref ref-type="bibr" rid="ref155">155</xref>–<xref ref-type="bibr" rid="ref157">157</xref>,<xref ref-type="bibr" rid="ref168">168</xref>,<xref ref-type="bibr" rid="ref169">169</xref>,<xref ref-type="bibr" rid="ref173">173</xref>,<xref ref-type="bibr" rid="ref180">180</xref>,<xref ref-type="bibr" rid="ref205">205</xref>–<xref ref-type="bibr" rid="ref209">209</xref>,<xref ref-type="bibr" rid="ref213">213</xref>,<xref ref-type="bibr" rid="ref222">222</xref>,<xref ref-type="bibr" rid="ref232">232</xref>,<xref ref-type="bibr" rid="ref234">234</xref>,<xref ref-type="bibr" rid="ref236">236</xref>,<xref ref-type="bibr" rid="ref239">239</xref>,<xref ref-type="bibr" rid="ref241">241</xref>,<xref ref-type="bibr" rid="ref242">242</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">Other</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref024">24</xref>,<xref ref-type="bibr" rid="ref026">26</xref>,<xref ref-type="bibr" rid="ref174">174</xref>,<xref ref-type="bibr" rid="ref210">210</xref>,<xref ref-type="bibr" rid="ref214">214</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">Smart Home</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref033">33</xref>,<xref ref-type="bibr" rid="ref057">57</xref>,<xref ref-type="bibr" rid="ref110">110</xref>,<xref ref-type="bibr" rid="ref198">198</xref>]</td>
</tr>
<tr>
<td valign="top" align="left">Transport</td>
<td valign="top" align="left">[<xref ref-type="bibr" rid="ref013">13</xref>,<xref ref-type="bibr" rid="ref016">16</xref>,<xref ref-type="bibr" rid="ref032">32</xref>,<xref ref-type="bibr" rid="ref062">62</xref>,<xref ref-type="bibr" rid="ref085">85</xref>,<xref ref-type="bibr" rid="ref145">145</xref>,<xref ref-type="bibr" rid="ref151">151</xref>,<xref ref-type="bibr" rid="ref182">182</xref>,<xref ref-type="bibr" rid="ref221">221</xref>]</td>
</tr>
</tbody>
</table>
</table-wrap>
</app></app-group>
<ref-list>
<title>References</title>
<ref id="ref001">
<label>[1]</label><mixed-citation publication-type="other"><string-name><given-names>N.</given-names> <surname>Abe</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Verma</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Apte</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Schroko</surname></string-name>, <chapter-title>Cross channel optimized marketing by reinforcement learning</chapter-title>, in: <source>Proceedings of the 2004 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining – KDD ’04</source>, <year>2004</year>. doi:<pub-id pub-id-type="doi">10.1145/1014052.1016912</pub-id>.</mixed-citation>
</ref>
<ref id="ref002">
<label>[2]</label><mixed-citation publication-type="chapter"><string-name><given-names>G.</given-names> <surname>Abowd</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Dey</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Brown</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Davies</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Smith</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Steggles</surname></string-name>, <chapter-title>Towards a better understanding of context and context-awareness</chapter-title>, in: <source>Handheld and Ubiquitous Computing</source>, <publisher-name>Springer</publisher-name>, <year>1999</year>, p. <fpage>319</fpage>. doi:<pub-id pub-id-type="doi">10.1007/3-540-48157-5_29</pub-id>.</mixed-citation>
</ref>
<ref id="ref003">
<label>[3]</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Ahrndt</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Lützenberger</surname></string-name> and <string-name><given-names>S.M.</given-names> <surname>Prochnow</surname></string-name>, <chapter-title>Using personality models as prior knowledge to accelerate learning about stress-coping preferences: (demonstration)</chapter-title>, in: <source>AAMAS</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.5555/2936924.2937221</pub-id>.</mixed-citation>
</ref>
<ref id="ref004">
<label>[4]</label><mixed-citation publication-type="other"><string-name><given-names>G.</given-names> <surname>Andrade</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Ramalho</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Santana</surname></string-name> and <string-name><given-names>V.</given-names> <surname>Corruble</surname></string-name>, <chapter-title>Automatic computer game balancing</chapter-title>, in: <source>Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems – AAMAS ’05</source>, <year>2005</year>. doi:<pub-id pub-id-type="doi">10.1145/1082473.1082648</pub-id>.</mixed-citation>
</ref>
<ref id="ref005">
<label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>M.G.</given-names> <surname>Aspinall</surname></string-name> and <string-name><given-names>R.G.</given-names> <surname>Hamermesh</surname></string-name>, <article-title>Realizing the promise of personalized medicine</article-title>, <source>Harvard Business Review</source> <volume>85</volume>(<issue>10</issue>) (<year>2007</year>), <fpage>108</fpage>. <uri>https://hbr.org/2007/10/realizing-the-promise-of-personalized-medicine</uri>.</mixed-citation>
</ref>
<ref id="ref006">
<label>[6]</label><mixed-citation publication-type="other"><string-name><given-names>A.</given-names> <surname>Atrash</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Pineau</surname></string-name>, <chapter-title>A Bayesian reinforcement learning approach for customizing human–robot interfaces</chapter-title>, in: <source>Proceedings of the 13th International Conference on Intelligent User Interfaces – IUI ’09</source>, <year>2008</year>. doi:<pub-id pub-id-type="doi">10.1145/1502650.1502700</pub-id>.</mixed-citation>
</ref>
<ref id="ref007">
<label>[7]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Auer</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Cesa-Bianchi</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Fischer</surname></string-name>, <article-title>Finite-time analysis of the multiarmed bandit problem</article-title>, <source>Machine Learning</source> <volume>47</volume>(<issue>2–3</issue>) (<year>2002</year>), <fpage>235</fpage>–<lpage>256</lpage>. doi:<pub-id pub-id-type="doi">10.1023/A:1013689704352</pub-id>.</mixed-citation>
</ref>
<ref id="ref008">
<label>[8]</label><mixed-citation publication-type="chapter"><string-name><given-names>S.</given-names> <surname>Ávila-Sansores</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Orihuela-Espina</surname></string-name> and <string-name><given-names>L.</given-names> <surname>Enrique-Sucar</surname></string-name>, <chapter-title>Patient tailored virtual rehabilitation</chapter-title>, in: <source>Converging Clinical and Engineering Research on Neurorehabilitation</source>, <series>Biosystems &amp; Biorobotics</series>, Vol. <volume>1</volume>, <year>2013</year>, pp. <fpage>879</fpage>–<lpage>883</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-642-34546-3_143</pub-id>.</mixed-citation>
</ref>
<ref id="ref009">
<label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>N.F.</given-names> <surname>Awad</surname></string-name> and <string-name><given-names>M.S.</given-names> <surname>Krishnan</surname></string-name>, <article-title>The personalization privacy paradox: An empirical evaluation of information transparency and the willingness to be profiled online for personalization</article-title>, <source>MIS Quarterly</source> <volume>30</volume>(<issue>1</issue>) (<year>2006</year>), <fpage>13</fpage>–<lpage>28</lpage>. doi:<pub-id pub-id-type="doi">10.2307/25148715</pub-id>.</mixed-citation>
</ref>
<ref id="ref010">
<label>[10]</label><mixed-citation publication-type="other"><string-name><given-names>N.</given-names> <surname>Bagdure</surname></string-name> and <string-name><given-names>B.</given-names> <surname>Ambudkar</surname></string-name>, <chapter-title>Reducing delay during vertical handover</chapter-title>, in: <source>2015 International Conference on Computing Communication Control and Automation</source>, <year>2015</year>. doi:<pub-id pub-id-type="doi">10.1109/ICCUBEA.2015.44</pub-id>.</mixed-citation>
</ref>
<ref id="ref011">
<label>[11]</label><mixed-citation publication-type="chapter"><string-name><given-names>A.</given-names> <surname>Baniya</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Herrmann</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Qiao</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Lu</surname></string-name>, <chapter-title>Adaptive interventions treatment modelling and regimen optimization using sequential multiple assignment randomized trials (SMART) and Q-learning</chapter-title>, in: <source>IIE Annual Conference. Proceedings</source>, <publisher-name>Institute of Industrial and Systems Engineers (IISE)</publisher-name>, <year>2017</year>, pp. <fpage>1187</fpage>–<lpage>1192</lpage>. <uri>https://pdfs.semanticscholar.org/858e/ffd10b711ad6c86eff9c32cdc0bc320a6e1a.pdf</uri>.</mixed-citation>
</ref>
<ref id="ref012">
<label>[12]</label><mixed-citation publication-type="other"><string-name><given-names>A.G.</given-names> <surname>Barto</surname></string-name>, <string-name><given-names>P.S.</given-names> <surname>Thomas</surname></string-name> and <string-name><given-names>R.S.</given-names> <surname>Sutton</surname></string-name>, Some recent applications of reinforcement learning, 2017. <uri>https://people.cs.umass.edu/~pthomas/papers/Barto2017.pdf</uri>.</mixed-citation>
</ref>
<ref id="ref013">
<label>[13]</label><mixed-citation publication-type="other"><string-name><given-names>A.L.C.</given-names> <surname>Bazzan</surname></string-name>, <chapter-title>Synergies between evolutionary computation and multiagent reinforcement learning</chapter-title>, in: <source>Proceedings of the Genetic and Evolutionary Computation Conference Companion – GECCO ’17</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1145/3067695.3075970</pub-id>.</mixed-citation>
</ref>
<ref id="ref014">
<label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>M.G.</given-names> <surname>Bellemare</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Naddaf</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Veness</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Bowling</surname></string-name>, <article-title>The arcade learning environment: An evaluation platform for general agents</article-title>, <source>Journal of Artificial Intelligence Research</source> <volume>47</volume> (<year>2013</year>), <fpage>253</fpage>–<lpage>279</lpage>. doi:<pub-id pub-id-type="doi">10.1613/jair.3912</pub-id>.</mixed-citation>
</ref>
<ref id="ref015">
<label>[15]</label><mixed-citation publication-type="other"><string-name><given-names>R.E.</given-names> <surname>Bellman</surname></string-name>, <source>Adaptive Control Processes: A Guided Tour</source>, Vol. <volume>2045</volume>, <publisher-name>Princeton University Press</publisher-name>, <year>2015</year>. doi:<pub-id pub-id-type="doi">10.1002/nav.3800080314</pub-id>.</mixed-citation>
</ref>
<ref id="ref016">
<label>[16]</label><mixed-citation publication-type="other"><string-name><given-names>H.</given-names> <surname>Bi</surname></string-name>, <string-name><given-names>O.J.</given-names> <surname>Akinwande</surname></string-name> and <string-name><given-names>E.</given-names> <surname>Gelenbe</surname></string-name>, <chapter-title>Emergency navigation in confined spaces using dynamic grouping</chapter-title>, in: <source>2015 9th International Conference on Next Generation Mobile Applications, Services and Technologies</source>, <year>2015</year>. doi:<pub-id pub-id-type="doi">10.1109/NGMAST.2015.12</pub-id>.</mixed-citation>
</ref>
<ref id="ref017">
<label>[17]</label><mixed-citation publication-type="chapter"><string-name><given-names>G.</given-names> <surname>Biegel</surname></string-name> and <string-name><given-names>V.</given-names> <surname>Cahill</surname></string-name>, <chapter-title>A framework for developing mobile, context-aware applications</chapter-title>, in: <source>Proceedings of the Second IEEE Annual Conference on Pervasive Computing and Communications, PerCom 2004</source>, <publisher-name>IEEE</publisher-name>, <year>2004</year>, pp. <fpage>361</fpage>–<lpage>365</lpage>. doi:<pub-id pub-id-type="doi">10.1109/PERCOM.2004.1276875</pub-id>.</mixed-citation>
</ref>
<ref id="ref018">
<label>[18]</label><mixed-citation publication-type="other"><string-name><given-names>A.</given-names> <surname>Bodas</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Upadhyay</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Nadiger</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Abdelhak</surname></string-name>, <chapter-title>Reinforcement learning for game personalization on edge devices</chapter-title>, in: <source>2018 International Conference on Information and Computer Technologies (ICICT)</source>, <year>2018</year>. doi:<pub-id pub-id-type="doi">10.1109/INFOCT.2018.8356853</pub-id>.</mixed-citation>
</ref>
<ref id="ref019">
<label>[19]</label><mixed-citation publication-type="chapter"><string-name><given-names>D.</given-names> <surname>Bouneffouf</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Bouzeghoub</surname></string-name> and <string-name><given-names>A.L.</given-names> <surname>Gançarski</surname></string-name>, <chapter-title>Hybrid-<italic>ϵ</italic>-greedy for mobile context-aware recommender system</chapter-title>, in: <source>Pacific-Asia Conference on Knowledge Discovery and Data Mining</source>, <series>Lecture Notes in Computer Science</series>, Vol. <volume>7301</volume>, <year>2012</year>, pp. <fpage>468</fpage>–<lpage>479</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-642-30217-6_39</pub-id>.</mixed-citation>
</ref>
<ref id="ref020">
<label>[20]</label><mixed-citation publication-type="other"><string-name><given-names>J.</given-names> <surname>Bragg</surname></string-name>, <string-name><surname>Mausam</surname></string-name> and <string-name><given-names>D.S.</given-names> <surname>Weld</surname></string-name>, <chapter-title>Optimal testing for crowd workers</chapter-title>, in: <source>AAMAS</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.5555/2936924.2937066</pub-id>.</mixed-citation>
</ref>
<ref id="ref021">
<label>[21]</label><mixed-citation publication-type="other"><string-name><given-names>G.</given-names> <surname>Brockman</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Cheung</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Pettersson</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Schneider</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Schulman</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Tang</surname></string-name> and <string-name><given-names>W.</given-names> <surname>Zaremba</surname></string-name>, Openai gym, Preprint, <pub-id pub-id-type="arxiv">arXiv:1606.01540</pub-id>, 2016.</mixed-citation>
</ref>
<ref id="ref022">
<label>[22]</label><mixed-citation publication-type="other"><string-name><given-names>P.</given-names> <surname>Brusilovski</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Kobsa</surname></string-name> and <string-name><given-names>W.</given-names> <surname>Nejdl</surname></string-name>, <source>The Adaptive Web: Methods and Strategies of Web Personalization</source>, Vol. <volume>4321</volume>, <publisher-name>Springer</publisher-name>, <year>2007</year>. doi:<pub-id pub-id-type="doi">10.1007/978-3-540-72079-9</pub-id>.</mixed-citation>
</ref>
<ref id="ref023">
<label>[23]</label><mixed-citation publication-type="chapter"><string-name><given-names>D.</given-names> <surname>Budgen</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Brereton</surname></string-name>, <chapter-title>Performing systematic literature reviews in software engineering</chapter-title>, in: <source>Proceedings of the 28th International Conference on Software Engineering</source>, <publisher-name>ACM</publisher-name>, <year>2006</year>, pp. <fpage>1051</fpage>–<lpage>1052</lpage>. doi:<pub-id pub-id-type="doi">10.1145/1134285.1134500</pub-id>.</mixed-citation>
</ref>
<ref id="ref024">
<label>[24]</label><mixed-citation publication-type="other"><string-name><given-names>A.B.</given-names> <surname>Buduru</surname></string-name> and <string-name><given-names>S.S.</given-names> <surname>Yau</surname></string-name>, <chapter-title>An effective approach to continuous user authentication for touch screen smart devices</chapter-title>, in: <source>2015 IEEE International Conference on Software Quality, Reliability and Security</source>, <year>2015</year>. doi:<pub-id pub-id-type="doi">10.1109/QRS.2015.40</pub-id>.</mixed-citation>
</ref>
<ref id="ref025">
<label>[25]</label><mixed-citation publication-type="chapter"><string-name><given-names>I.</given-names> <surname>Casanueva</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Hain</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Christensen</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Marxer</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Green</surname></string-name>, <chapter-title>Knowledge transfer between speakers for personalised dialogue management</chapter-title>, in: <source>Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</source>, <year>2015</year>, pp. <fpage>12</fpage>–<lpage>21</lpage>. doi:<pub-id pub-id-type="doi">10.18653/v1/W15-4603</pub-id>.</mixed-citation>
</ref>
<ref id="ref026">
<label>[26]</label><mixed-citation publication-type="other"><string-name><given-names>A.</given-names> <surname>Castro-Gonzalez</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Amirabdollahian</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Polani</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Malfaz</surname></string-name> and <string-name><given-names>M.A.</given-names> <surname>Salichs</surname></string-name>, <chapter-title>Robot self-preservation and adaptation to user preferences in game play, a preliminary study</chapter-title>, in: <source>2011 IEEE International Conference on Robotics and Biomimetics</source>, <year>2011</year>. doi:<pub-id pub-id-type="doi">10.1109/ROBIO.2011.6181679</pub-id>.</mixed-citation>
</ref>
<ref id="ref027">
<label>[27]</label><mixed-citation publication-type="other"><string-name><given-names>L.</given-names> <surname>Cella</surname></string-name>, <chapter-title>Modelling user behaviors with evolving users and catalogs of evolving items</chapter-title>, in: <source>Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization – UMAP ’17</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1145/3099023.3102251</pub-id>.</mixed-citation>
</ref>
<ref id="ref028">
<label>[28]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Chakraborty</surname></string-name> and <string-name><given-names>S.A.</given-names> <surname>Murphy</surname></string-name>, <article-title>Dynamic treatment regimes</article-title>, <source>Annual Review of Statistics and Its Application</source> <volume>1</volume>(<issue>1</issue>) (<year>2014</year>), <fpage>447</fpage>–<lpage>464</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev-statistics-022513-115553</pub-id>.</mixed-citation>
</ref>
<ref id="ref029">
<label>[29]</label><mixed-citation publication-type="other"><string-name><given-names>J.</given-names> <surname>Chan</surname></string-name> and <string-name><given-names>G.</given-names> <surname>Nejat</surname></string-name>, <chapter-title>A learning-based control architecture for an assistive robot providing social engagement during cognitively stimulating activities</chapter-title>, in: <source>2011 IEEE International Conference on Robotics and Automation</source>, <year>2011</year>. doi:<pub-id pub-id-type="doi">10.1109/ICRA.2011.5980426</pub-id>.</mixed-citation>
</ref>
<ref id="ref030">
<label>[30]</label><mixed-citation publication-type="journal"><string-name><given-names>R.K.</given-names> <surname>Chellappa</surname></string-name> and <string-name><given-names>R.G.</given-names> <surname>Sin</surname></string-name>, <article-title>Personalization versus privacy: An empirical examination of the online consumer’s dilemma</article-title>, <source>Information Technology and Management</source> <volume>6</volume>(<issue>2–3</issue>) (<year>2005</year>), <fpage>181</fpage>–<lpage>202</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s10799-005-5879-y</pub-id>.</mixed-citation>
</ref>
<ref id="ref031">
<label>[31]</label><mixed-citation publication-type="other"><string-name><given-names>J.</given-names> <surname>Chen</surname></string-name> and <string-name><given-names>Z.</given-names> <surname>Yang</surname></string-name>, <chapter-title>A learning multi-agent system for personalized information filtering</chapter-title>, in: <source>Proceedings of the 2003 Joint Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia</source>, <year>2003</year>. doi:<pub-id pub-id-type="doi">10.1109/ICICS.2003.1292790</pub-id>.</mixed-citation>
</ref>
<ref id="ref032">
<label>[32]</label><mixed-citation publication-type="other"><string-name><given-names>X.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Zhai</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Lu</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Gong</surname></string-name> and <string-name><given-names>G.</given-names> <surname>Wang</surname></string-name>, <chapter-title>A learning model for personalized adaptive cruise control</chapter-title>, in: <source>2017 IEEE Intelligent Vehicles Symposium (IV)</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1109/IVS.2017.7995748</pub-id>.</mixed-citation>
</ref>
<ref id="ref033">
<label>[33]</label><mixed-citation publication-type="journal"><string-name><given-names>Z.</given-names> <surname>Cheng</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Jiang</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Xia</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Ding</surname></string-name>, <article-title>Satisfaction based Q-learning for integrated lighting and blind control</article-title>, <source>Energy and Buildings</source> <volume>127</volume> (<year>2016</year>), <fpage>43</fpage>–<lpage>55</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.enbuild.2016.05.067</pub-id>.</mixed-citation>
</ref>
<ref id="ref034">
<label>[34]</label><mixed-citation publication-type="other"><string-name><given-names>C.-Y.</given-names> <surname>Chi</surname></string-name>, <string-name><given-names>R.T.-H.</given-names> <surname>Tsai</surname></string-name>, <string-name><given-names>J.-Y.</given-names> <surname>Lai</surname></string-name> and <string-name><given-names>J.Y.</given-names> <surname>Hsu</surname></string-name>, <chapter-title>A reinforcement learning approach to emotion-based automatic playlist generation</chapter-title>, in: <source>2010 International Conference on Technologies and Applications of Artificial Intelligence</source>, <year>2010</year>. doi:<pub-id pub-id-type="doi">10.1109/TAAI.2010.21</pub-id>.</mixed-citation>
</ref>
<ref id="ref035">
<label>[35]</label><mixed-citation publication-type="chapter"><string-name><given-names>M.</given-names> <surname>Chi</surname></string-name>, <string-name><given-names>K.</given-names> <surname>VanLehn</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Litman</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Jordan</surname></string-name>, <chapter-title>Inducing effective pedagogical strategies using learning context features</chapter-title>, in: <source>International Conference on User Modeling, Adaptation, and Personalization</source>, <series>Lecture Notes in Computer Science</series>, Vol. <volume>6075</volume>, <year>2010</year>, pp. <fpage>147</fpage>–<lpage>158</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-642-13470-8_15</pub-id>.</mixed-citation>
</ref>
<ref id="ref036">
<label>[36]</label><mixed-citation publication-type="other"><string-name><given-names>Y.-S.</given-names> <surname>Chiang</surname></string-name>, <string-name><given-names>T.-S.</given-names> <surname>Chu</surname></string-name>, <string-name><given-names>C.D.</given-names> <surname>Lim</surname></string-name>, <string-name><given-names>T.-Y.</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>S.-H.</given-names> <surname>Tseng</surname></string-name> and <string-name><given-names>L.-C.</given-names> <surname>Fu</surname></string-name>, <chapter-title>Personalizing robot behavior for interruption in social human–robot interaction</chapter-title>, in: <source>2014 IEEE International Workshop on Advanced Robotics and Its Social Impacts</source>, <year>2014</year>. doi:<pub-id pub-id-type="doi">10.1109/ARSO.2014.7020978</pub-id>.</mixed-citation>
</ref>
<ref id="ref037">
<label>[37]</label><mixed-citation publication-type="chapter"><string-name><given-names>W.</given-names> <surname>Chu</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Reyzin</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Schapire</surname></string-name>, <chapter-title>Contextual bandits with linear payoff functions</chapter-title>, in: <source>Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</source>, <year>2011</year>, pp. <fpage>208</fpage>–<lpage>214</lpage>. <uri>http://proceedings.mlr.press/v15/chu11a</uri>.</mixed-citation>
</ref>
<ref id="ref038">
<label>[38]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Claeys</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Latre</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Famaey</surname></string-name> and <string-name><given-names>F.</given-names> <surname>De Turck</surname></string-name>, <article-title>Design and evaluation of a self-learning HTTP adaptive video streaming client</article-title>, <source>IEEE Communications Letters</source> <volume>18</volume>(<issue>4</issue>) (<year>2014</year>), <fpage>716</fpage>–<lpage>719</lpage>. doi:<pub-id pub-id-type="doi">10.1109/LCOMM.2014.020414.132649</pub-id>.</mixed-citation>
</ref>
<ref id="ref039">
<label>[39]</label><mixed-citation publication-type="journal"><string-name><given-names>G.</given-names> <surname>Da Silveira</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Borenstein</surname></string-name> and <string-name><given-names>F.S.</given-names> <surname>Fogliatto</surname></string-name>, <article-title>Mass customization: Literature review and research directions</article-title>, <source>International Journal of Production Economics</source> <volume>72</volume>(<issue>1</issue>) (<year>2001</year>), <fpage>1</fpage>–<lpage>13</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0925-5273(00)00079-7</pub-id>.</mixed-citation>
</ref>
<ref id="ref040">
<label>[40]</label><mixed-citation publication-type="other"><string-name><given-names>M.</given-names> <surname>Daltayanni</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Wang</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Akella</surname></string-name>, <chapter-title>A fast interactive search system for healthcare services</chapter-title>, in: <source>2012 Annual SRII Global Conference</source>, <year>2012</year>. doi:<pub-id pub-id-type="doi">10.1109/SRII.2012.65</pub-id>.</mixed-citation>
</ref>
<ref id="ref041">
<label>[41]</label><mixed-citation publication-type="other"><string-name><given-names>E.</given-names> <surname>Daskalaki</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Diem</surname></string-name> and <string-name><given-names>S.G.</given-names> <surname>Mougiakakou</surname></string-name>, <chapter-title>Personalized tuning of a reinforcement learning control algorithm for glucose regulation</chapter-title>, in: <source>2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source>, <year>2013</year>. doi:<pub-id pub-id-type="doi">10.1109/EMBC.2013.6610293</pub-id>.</mixed-citation>
</ref>
<ref id="ref042">
<label>[42]</label><mixed-citation publication-type="journal"><string-name><given-names>E.</given-names> <surname>Daskalaki</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Diem</surname></string-name> and <string-name><given-names>S.G.</given-names> <surname>Mougiakakou</surname></string-name>, <article-title>An actor–critic based controller for glucose regulation in type 1 diabetes</article-title>, <source>Computer Methods and Programs in Biomedicine</source> <volume>109</volume>(<issue>2</issue>) (<year>2013</year>), <fpage>116</fpage>–<lpage>125</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cmpb.2012.03.002</pub-id>.</mixed-citation>
</ref>
<ref id="ref043">
<label>[43]</label><mixed-citation publication-type="other"><string-name><given-names>E.</given-names> <surname>Daskalaki</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Diem</surname></string-name> and <string-name><given-names>S.G.</given-names> <surname>Mougiakakou</surname></string-name>, <article-title>Model-free machine learning in biomedicine: Feasibility study in type 1 diabetes</article-title>, <source>PLoS ONE</source> <volume>11</volume>(<issue>7</issue>) (<year>2016</year>), <elocation-id>e0158722</elocation-id>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0158722</pub-id>.</mixed-citation>
</ref>
<ref id="ref044">
<label>[44]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>De Paula</surname></string-name>, <string-name><given-names>G.G.</given-names> <surname>Acosta</surname></string-name> and <string-name><given-names>E.C.</given-names> <surname>Martínez</surname></string-name>, <article-title>On-line policy learning and adaptation for real-time personalization of an artificial pancreas</article-title>, <source>Expert Systems with Applications</source> <volume>42</volume>(<issue>4</issue>) (<year>2015</year>), <fpage>2234</fpage>–<lpage>2255</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.eswa.2014.10.038</pub-id>.</mixed-citation>
</ref>
<ref id="ref045">
<label>[45]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>De Paula</surname></string-name>, <string-name><given-names>L.O.</given-names> <surname>Ávila</surname></string-name> and <string-name><given-names>E.C.</given-names> <surname>Martínez</surname></string-name>, <article-title>Controlling blood glucose variability under uncertainty using reinforcement learning and Gaussian processes</article-title>, <source>Applied Soft Computing</source> <volume>35</volume> (<year>2015</year>), <fpage>310</fpage>–<lpage>332</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.asoc.2015.06.041</pub-id>.</mixed-citation>
</ref>
<ref id="ref046">
<label>[46]</label><mixed-citation publication-type="other"><string-name><given-names>F.</given-names> <surname>den Hengst</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Grua</surname></string-name>, <string-name><given-names>A.</given-names> <surname>el Hassouni</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Hoogendoorn</surname></string-name>, Release of the systematic literature review into reinforcement learning for personalization, Zenodo, 2020. doi:<pub-id pub-id-type="doi">10.5281/zenodo.3627118</pub-id>.</mixed-citation>
</ref>
<ref id="ref047">
<label>[47]</label><mixed-citation publication-type="chapter"><string-name><given-names>F.</given-names> <surname>den Hengst</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hoogendoorn</surname></string-name>, <string-name><given-names>F.</given-names> <surname>van Harmelen</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Bosman</surname></string-name>, <chapter-title>Reinforcement learning for personalized dialogue management</chapter-title>, in: <source>IEEE/WIC/ACM International Conference on Web Intelligence</source>, <year>2019</year>, pp. <fpage>59</fpage>–<lpage>67</lpage>. doi:<pub-id pub-id-type="doi">10.1145/3350546.3352501</pub-id>.</mixed-citation>
</ref>
<ref id="ref048">
<label>[48]</label><mixed-citation publication-type="other"><string-name><given-names>K.</given-names> <surname>Deng</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Pineau</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Murphy</surname></string-name>, <chapter-title>Active learning for personalizing treatment</chapter-title>, in: <source>2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)</source>, <year>2011</year>. doi:<pub-id pub-id-type="doi">10.1109/ADPRL.2011.5967348</pub-id>.</mixed-citation>
</ref>
<ref id="ref049">
<label>[49]</label><mixed-citation publication-type="other"><string-name><given-names>A.A.</given-names> <surname>Deshmukh</surname></string-name>, <string-name><given-names>Ü.</given-names> <surname>Dogan</surname></string-name> and <string-name><given-names>C.</given-names> <surname>Scott</surname></string-name>, <chapter-title>Multi-task learning for contextual bandits</chapter-title>, in: <source>NIPS</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.5555/3295222.3295238</pub-id>.</mixed-citation>
</ref>
<ref id="ref050">
<label>[50]</label><mixed-citation publication-type="chapter"><string-name><given-names>Y.</given-names> <surname>Duan</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Houthooft</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Schulman</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Abbeel</surname></string-name>, <chapter-title>Benchmarking deep reinforcement learning for continuous control</chapter-title>, in: <source>International Conference on Machine Learning</source>, <year>2016</year>, pp. <fpage>1329</fpage>–<lpage>1338</lpage>. <uri>http://proceedings.mlr.press/v48/duan16.html</uri>.</mixed-citation>
</ref>
<ref id="ref051">
<label>[51]</label><mixed-citation publication-type="other"><string-name><given-names>A.</given-names> <surname>Durand</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Pineau</surname></string-name>, <chapter-title>Adaptive treatment allocation using sub-sampled Gaussian processes</chapter-title>, in: <source>2015 AAAI Fall Symposium Series</source>, <year>2015</year>. <uri>https://www.aaai.org/ocs/index.php/FSS/FSS15/paper/view/11671</uri>.</mixed-citation>
</ref>
<ref id="ref052">
<label>[52]</label><mixed-citation publication-type="other"><string-name><given-names>M.</given-names> <surname>El Fouki</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Aknin</surname></string-name> and <string-name><given-names>K.E.</given-names> <surname>El Kadiri</surname></string-name>, <chapter-title>Intelligent adapted e-learning system based on deep reinforcement learning</chapter-title>, in: <source>Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems – ICCWCS’17</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1145/3167486.3167574</pub-id>.</mixed-citation>
</ref>
<ref id="ref053">
<label>[53]</label><mixed-citation publication-type="chapter"><string-name><given-names>A.</given-names> <surname>El Hassouni</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hoogendoorn</surname></string-name>, <string-name><given-names>A.E.</given-names> <surname>Eiben</surname></string-name>, <string-name><given-names>M.</given-names> <surname>van Otterlo</surname></string-name> and <string-name><given-names>V.</given-names> <surname>Muhonen</surname></string-name>, <chapter-title>End-to-end personalization of digital health interventions using raw sensor data with deep reinforcement learning: A comparative study in digital health interventions for behavior change</chapter-title>, in: <source>2019 IEEE/WIC/ACM International Conference on Web Intelligence (WI)</source>, <publisher-name>IEEE</publisher-name>, <year>2019</year>, pp. <fpage>258</fpage>–<lpage>264</lpage>. doi:<pub-id pub-id-type="doi">10.1145/3350546.3352527</pub-id>.</mixed-citation>
</ref>
<ref id="ref054">
<label>[54]</label><mixed-citation publication-type="chapter"><string-name><given-names>A.</given-names> <surname>el Hassouni</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hoogendoorn</surname></string-name>, <string-name><given-names>M.</given-names> <surname>van Otterlo</surname></string-name> and <string-name><given-names>E.</given-names> <surname>Barbaro</surname></string-name>, <chapter-title>Personalization of health interventions using cluster-based reinforcement learning</chapter-title>, in: <source>International Conference on Principles and Practice of Multi-Agent Systems</source>, <publisher-name>Springer</publisher-name>, <year>2018</year>, pp. <fpage>467</fpage>–<lpage>475</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-030-03098-8_31</pub-id>.</mixed-citation>
</ref>
<ref id="ref055">
<label>[55]</label><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Fan</surname></string-name> and <string-name><given-names>M.S.</given-names> <surname>Poole</surname></string-name>, <article-title>What is personalization? Perspectives on the design and implementation of personalization in information systems</article-title>, <source>Journal of Organizational Computing and Electronic Commerce</source> <volume>16</volume>(<issue>3–4</issue>) (<year>2006</year>), <fpage>179</fpage>–<lpage>202</lpage>. doi:<pub-id pub-id-type="doi">10.1207/s15327744joce1603&amp;4_2</pub-id>.</mixed-citation>
</ref>
<ref id="ref056">
<label>[56]</label><mixed-citation publication-type="other"><string-name><given-names>J.</given-names> <surname>Feng</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Ou</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Wang</surname></string-name> and <string-name><given-names>X.</given-names> <surname>Zhu</surname></string-name>, <chapter-title>Learning to collaborate</chapter-title>, in: <source>Proceedings of the 2018 World Wide Web Conference on World Wide Web – WWW ’18</source>, <year>2018</year>. doi:<pub-id pub-id-type="doi">10.1080/10919392.2006.9681199</pub-id>.</mixed-citation>
</ref>
<ref id="ref057">
<label>[57]</label><mixed-citation publication-type="other"><string-name><given-names>B.</given-names> <surname>Fernandez-Gauna</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Grana</surname></string-name>, <chapter-title>Recipe tuning by reinforcement learning in the SandS ecosystem</chapter-title>, in: <source>2014 6th International Conference on Computational Aspects of Social Networks</source>, <year>2014</year>. doi:<pub-id pub-id-type="doi">10.1109/CASoN.2014.6920422</pub-id>.</mixed-citation>
</ref>
<ref id="ref058">
<label>[58]</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Ferretti</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Mirri</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Prandi</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Salomoni</surname></string-name>, <chapter-title>Exploiting reinforcement learning to profile users and personalize web pages</chapter-title>, in: <source>2014 IEEE 38th International Computer Software and Applications Conference Workshops</source>, <year>2014</year>. doi:<pub-id pub-id-type="doi">10.1109/COMPSACW.2014.45</pub-id>.</mixed-citation>
</ref>
<ref id="ref059">
<label>[59]</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Ferretti</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Mirri</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Prandi</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Salomoni</surname></string-name>, <chapter-title>User centered and context dependent personalization through experiential transcoding</chapter-title>, in: <source>2014 IEEE 11th Consumer Communications and Networking Conference (CCNC)</source>, <year>2014</year>. doi:<pub-id pub-id-type="doi">10.1109/CCNC.2014.6940520</pub-id>.</mixed-citation>
</ref>
<ref id="ref060">
<label>[60]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Ferretti</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Mirri</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Prandi</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Salomoni</surname></string-name>, <article-title>Automatic web content personalization through reinforcement learning</article-title>, <source>Journal of Systems and Software</source> <volume>121</volume> (<year>2016</year>), <fpage>157</fpage>–<lpage>169</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jss.2016.02.008</pub-id>.</mixed-citation>
</ref>
<ref id="ref061">
<label>[61]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Ferretti</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Mirri</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Prandi</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Salomoni</surname></string-name>, <article-title>On personalizing web content through reinforcement learning</article-title>, <source>Universal Access in the Information Society</source> <volume>16</volume>(<issue>2</issue>) (<year>2017</year>), <fpage>395</fpage>–<lpage>410</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s10209-016-0463-2</pub-id>.</mixed-citation>
</ref>
<ref id="ref062">
<label>[62]</label><mixed-citation publication-type="other"><string-name><given-names>L.</given-names> <surname>Fournier</surname></string-name>, <chapter-title>Learning capabilities for improving automatic transmission control</chapter-title>, in: <source>Proceedings of the Intelligent Vehicles ’94 Symposium</source>, <year>1994</year>. doi:<pub-id pub-id-type="doi">10.1109/IVS.1994.639561</pub-id>.</mixed-citation>
</ref>
<ref id="ref063">
<label>[63]</label><mixed-citation publication-type="other"><string-name><given-names>A.Y.</given-names> <surname>Gao</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Barendregt</surname></string-name> and <string-name><given-names>G.</given-names> <surname>Castellano</surname></string-name>, <chapter-title>Personalised human–robot co-adaptation in instructional settings using reinforcement learning</chapter-title>, in: <source>IVA Workshop on Persuasive Embodied Agents for Behavior Change: PEACH 2017</source>, <conf-date>August 27</conf-date>, <conf-loc>Stockholm, Sweden</conf-loc>, <year>2017</year>. <uri>http://www.diva-portal.org/smash/get/diva2:1162389/FULLTEXT01.pdf</uri>.</mixed-citation>
</ref>
<ref id="ref064">
<label>[64]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>García</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Fernández</surname></string-name>, <article-title>A comprehensive survey on safe reinforcement learning</article-title>, <source>Journal of Machine Learning Research</source> <volume>16</volume>(<issue>1</issue>) (<year>2015</year>), <fpage>1437</fpage>–<lpage>1480</lpage>. <uri>http://jmlr.org/papers/v16/garcia15a.html</uri>.</mixed-citation>
</ref>
<ref id="ref065">
<label>[65]</label><mixed-citation publication-type="chapter"><string-name><given-names>A.</given-names> <surname>Garivier</surname></string-name> and <string-name><given-names>E.</given-names> <surname>Moulines</surname></string-name>, <chapter-title>On upper-confidence bound policies for switching bandit problems</chapter-title>, in: <source>International Conference on Algorithmic Learning Theory</source>, <publisher-name>Springer</publisher-name>, <year>2011</year>, pp. <fpage>174</fpage>–<lpage>188</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-642-24412-4_16</pub-id>.</mixed-citation>
</ref>
<ref id="ref066">
<label>[66]</label><mixed-citation publication-type="other"><string-name><given-names>A.E.</given-names> <surname>Gaweda</surname></string-name>, <chapter-title>Improving management of anemia in end stage renal disease using reinforcement learning</chapter-title>, in: <source>2009 International Joint Conference on Neural Networks</source>, <year>2009</year>. doi:<pub-id pub-id-type="doi">10.1109/IJCNN.2009.5179004</pub-id>.</mixed-citation>
</ref>
<ref id="ref067">
<label>[67]</label><mixed-citation publication-type="journal"><string-name><given-names>A.E.</given-names> <surname>Gaweda</surname></string-name>, <string-name><given-names>M.K.</given-names> <surname>Muezzinoglu</surname></string-name>, <string-name><given-names>G.R.</given-names> <surname>Aronoff</surname></string-name>, <string-name><given-names>A.A.</given-names> <surname>Jacobs</surname></string-name>, <string-name><given-names>J.M.</given-names> <surname>Zurada</surname></string-name> and <string-name><given-names>M.E.</given-names> <surname>Brier</surname></string-name>, <article-title>Individualization of pharmacological anemia management using reinforcement learning</article-title>, <source>Neural Networks</source> <volume>18</volume>(<issue>5</issue>) (<year>2005</year>), <fpage>826</fpage>–<lpage>834</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neunet.2005.06.020</pub-id>.</mixed-citation>
</ref>
<ref id="ref068">
<label>[68]</label><mixed-citation publication-type="other"><string-name><given-names>A.E.</given-names> <surname>Gaweda</surname></string-name>, <string-name><given-names>M.K.</given-names> <surname>Muezzinoglu</surname></string-name>, <string-name><given-names>G.R.</given-names> <surname>Aronoff</surname></string-name>, <string-name><given-names>A.A.</given-names> <surname>Jacobs</surname></string-name>, <string-name><given-names>J.M.</given-names> <surname>Zurada</surname></string-name> and <string-name><given-names>M.E.</given-names> <surname>Brier</surname></string-name>, <chapter-title>Incorporating prior knowledge into Q-learning for drug delivery individualization</chapter-title>, in: <source>Fourth International Conference on Machine Learning and Applications (ICMLA’05)</source>, <year>2005</year>. doi:<pub-id pub-id-type="doi">10.1109/ICMLA.2005.40</pub-id>.</mixed-citation>
</ref>
<ref id="ref069">
<label>[69]</label><mixed-citation publication-type="chapter"><string-name><given-names>C.</given-names> <surname>Gentile</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Li</surname></string-name> and <string-name><given-names>G.</given-names> <surname>Zappella</surname></string-name>, <chapter-title>Online clustering of bandits</chapter-title>, in: <source>International Conference on Machine Learning</source>, <year>2014</year>, pp. <fpage>757</fpage>–<lpage>765</lpage>. <uri>http://proceedings.mlr.press/v32/gentile14.html</uri>.</mixed-citation>
</ref>
<ref id="ref070">
<label>[70]</label><mixed-citation publication-type="journal"><string-name><given-names>B.S.</given-names> <surname>Ghahfarokhi</surname></string-name> and <string-name><given-names>N.</given-names> <surname>Movahhedinia</surname></string-name>, <article-title>A personalized QoE-aware handover decision based on distributed reinforcement learning</article-title>, <source>Wireless Networks</source> <volume>19</volume>(<issue>8</issue>) (<year>2013</year>), <fpage>1807</fpage>–<lpage>1828</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s11276-013-0572-2</pub-id>.</mixed-citation>
</ref>
<ref id="ref071">
<label>[71]</label><mixed-citation publication-type="journal"><string-name><given-names>G.S.</given-names> <surname>Ginsburg</surname></string-name> and <string-name><given-names>J.J.</given-names> <surname>McCarthy</surname></string-name>, <article-title>Personalized medicine: Revolutionizing drug discovery and patient care</article-title>, <source>Trends in Biotechnology</source> <volume>19</volume>(<issue>12</issue>) (<year>2001</year>), <fpage>491</fpage>–<lpage>496</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0167-7799(01)01814-5</pub-id>.</mixed-citation>
</ref>
<ref id="ref072">
<label>[72]</label><mixed-citation publication-type="other"><string-name><given-names>D.</given-names> <surname>Glowacka</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Ruotsalo</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Konuyshkova</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Athukorala</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Kaski</surname></string-name> and <string-name><given-names>G.</given-names> <surname>Jacucci</surname></string-name>, <chapter-title>Directing exploratory search: reinforcement learning from user interactions with keywords</chapter-title>, in: <source>Proceedings of the 2013 International Conference on Intelligent User Interfaces – IUI ’13</source>, <year>2013</year>. doi:<pub-id pub-id-type="doi">10.1145/2449396.2449413</pub-id>.</mixed-citation>
</ref>
<ref id="ref073">
<label>[73]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Goldberg</surname></string-name> and <string-name><given-names>M.R.</given-names> <surname>Kosorok</surname></string-name>, <article-title>Q-learning with censored data</article-title>, <source>The Annals of Statistics</source> <volume>40</volume>(<issue>1</issue>) (<year>2012</year>), <fpage>529</fpage>–<lpage>560</lpage>. doi:<pub-id pub-id-type="doi">10.1214/12-AOS968</pub-id>.</mixed-citation>
</ref>
<ref id="ref074">
<label>[74]</label><mixed-citation publication-type="other"><string-name><given-names>G.</given-names> <surname>Gordon</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Spaulding</surname></string-name>, <string-name><given-names>J.K.</given-names> <surname>Westlund</surname></string-name>, <string-name><given-names>J.J.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Plummer</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Martinez</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Das</surname></string-name> and <string-name><given-names>C.</given-names> <surname>Breazeal</surname></string-name>, <chapter-title>Affective personalization of a social robot tutor for children’s second language skills</chapter-title>, in: <source>Thirtieth AAAI Conference on Artificial Intelligence</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.5555/3016387.3016461</pub-id>.</mixed-citation>
</ref>
<ref id="ref075">
<label>[75]</label><mixed-citation publication-type="chapter"><string-name><given-names>E.M.</given-names> <surname>Grua</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Hoogendoorn</surname></string-name>, <chapter-title>Exploring clustering techniques for effective reinforcement learning based personalization for health and wellbeing</chapter-title>, in: <source>2018 IEEE Symposium Series on Computational Intelligence (SSCI)</source>, <publisher-name>IEEE</publisher-name>, <year>2018</year>, pp. <fpage>813</fpage>–<lpage>820</lpage>. doi:<pub-id pub-id-type="doi">10.1109/SSCI.2018.8628621</pub-id>.</mixed-citation>
</ref>
<ref id="ref076">
<label>[76]</label><mixed-citation publication-type="chapter"><string-name><given-names>X.</given-names> <surname>Guo</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Yan</surname></string-name> and <string-name><given-names>N.</given-names> <surname>Wang</surname></string-name>, <chapter-title>Privacy-personalization paradox in adoption of mobile health service: The mediating role of trust</chapter-title>, in: <source>PACIS 2012 Proceedings</source>, <year>2012</year>, p. <fpage>27</fpage>. <uri>https://aisel.aisnet.org/pacis2012/27</uri>.</mixed-citation>
</ref>
<ref id="ref077">
<label>[77]</label><mixed-citation publication-type="journal"><string-name><given-names>M.A.</given-names> <surname>Hamburg</surname></string-name> and <string-name><given-names>F.S.</given-names> <surname>Collins</surname></string-name>, <article-title>The path to personalized medicine</article-title>, <source>N. Engl. J. Med.</source> <volume>2010</volume>(<issue>363</issue>) (<year>2010</year>), <fpage>301</fpage>–<lpage>304</lpage>. doi:<pub-id pub-id-type="doi">10.1056/NEJMp1006304</pub-id>.</mixed-citation>
</ref>
<ref id="ref078">
<label>[78]</label><mixed-citation publication-type="chapter"><string-name><given-names>A.</given-names> <surname>Hans</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Schneegaß</surname></string-name>, <string-name><given-names>A.M.</given-names> <surname>Schäfer</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Udluft</surname></string-name>, <chapter-title>Safe exploration for reinforcement learning.</chapter-title>, in: <source>ESANN</source>, <year>2008</year>, pp. <fpage>143</fpage>–<lpage>148</lpage>. <uri>http://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2008-36.pdf</uri>.</mixed-citation>
</ref>
<ref id="ref079">
<label>[79]</label><mixed-citation publication-type="chapter"><string-name><given-names>F.M.</given-names> <surname>Harper</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Chen</surname></string-name> and <string-name><given-names>J.A.</given-names> <surname>Konstan</surname></string-name>, <chapter-title>An economic model of user rating in an online recommender system</chapter-title>, in: <source>International Conference on User Modeling</source>, <series>Lecture Notes in Computer Science</series>, Vol. <volume>3538</volume>, <year>2005</year>, pp. <fpage>307</fpage>–<lpage>316</lpage>. doi:<pub-id pub-id-type="doi">10.1007/11527886_40</pub-id>.</mixed-citation>
</ref>
<ref id="ref080">
<label>[80]</label><mixed-citation publication-type="other"><string-name><given-names>J.</given-names> <surname>Hemminghaus</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Kopp</surname></string-name>, <chapter-title>Adaptive behavior generation for child–robot interaction</chapter-title>, in: <source>Companion of the 2018 ACM/IEEE International Conference on Human–Robot Interaction – HRI ’18</source>, <year>2018</year>. doi:<pub-id pub-id-type="doi">10.1145/3173386.3176916</pub-id>.</mixed-citation>
</ref>
<ref id="ref081">
<label>[81]</label><mixed-citation publication-type="chapter"><string-name><given-names>T.</given-names> <surname>Hester</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Stone</surname></string-name>, <chapter-title>Learning and using models</chapter-title>, in: <source>Reinforcement Learning</source>, <string-name><given-names>M.</given-names> <surname>Wiering</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Van Otterlo</surname></string-name>, eds, <series>Adaptation, Learning, and Optimization</series>, Vol. <volume>12</volume>, <publisher-name>Springer</publisher-name>, <year>2012</year>, p. <fpage>120</fpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-642-27645-3_4</pub-id>.</mixed-citation>
</ref>
<ref id="ref082">
<label>[82]</label><mixed-citation publication-type="other"><string-name><given-names>D.N.</given-names> <surname>Hill</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Nassif</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Iyer</surname></string-name> and <string-name><given-names>S.V.N.</given-names> <surname>Vishwanathan</surname></string-name>, <chapter-title>An efficient bandit algorithm for realtime multivariate optimization</chapter-title>, in: <source>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining – KDD ’17</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1145/3097983.3098184</pub-id>.</mixed-citation>
</ref>
<ref id="ref083">
<label>[83]</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Hiraoka</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Neubig</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Sakti</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Toda</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Nakamura</surname></string-name>, <article-title>Learning cooperative persuasive dialogue policies using framing</article-title>, <source>Speech Communication</source> <volume>84</volume> (<year>2016</year>), <fpage>83</fpage>–<lpage>96</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.specom.2016.09.002</pub-id>.</mixed-citation>
</ref>
<ref id="ref084">
<label>[84]</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Hood</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Flores</surname></string-name>, <article-title>A personal view on systems medicine and the emergence of proactive P4 medicine: Predictive, preventive, personalized and participatory</article-title>, <source>New Biotechnology</source> <volume>29</volume>(<issue>6</issue>) (<year>2012</year>), <fpage>613</fpage>–<lpage>624</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.nbt.2012.03.004</pub-id>.</mixed-citation>
</ref>
<ref id="ref085">
<label>[85]</label><mixed-citation publication-type="other"><string-name><given-names>Z.</given-names> <surname>Huajun</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Jin</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Rui</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Tan</surname></string-name>, <chapter-title>Multi-objective reinforcement learning algorithm and its application in drive system</chapter-title>, in: <source>2008 34th Annual Conference of IEEE Industrial Electronics</source>, <year>2008</year>. doi:<pub-id pub-id-type="doi">10.1109/IECON.2008.4757965</pub-id>.</mixed-citation>
</ref>
<ref id="ref086">
<label>[86]</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Huang</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Lin</surname></string-name>, <chapter-title>Designing intelligent sales-agent for online selling</chapter-title>, in: <source>Proceedings of the 7th International Conference on Electronic Commerce – ICEC ’05</source>, <year>2005</year>. doi:<pub-id pub-id-type="doi">10.1145/1089551.1089605</pub-id>.</mixed-citation>
</ref>
<ref id="ref087">
<label>[87]</label><mixed-citation publication-type="other"><string-name><given-names>E.</given-names> <surname>Ie</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Hsu</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mladenov</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Jain</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Narvekar</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Wu</surname></string-name> and <string-name><given-names>C.</given-names> <surname>Boutilier</surname></string-name>, RecSim: A configurable simulation platform for recommender systems, Preprint, <pub-id pub-id-type="arxiv">arXiv:1909.04847</pub-id>, 2019.</mixed-citation>
</ref>
<ref id="ref088">
<label>[88]</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Jaradat</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Dokoohaki</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Matskin</surname></string-name> and <string-name><given-names>E.</given-names> <surname>Ferrari</surname></string-name>, <chapter-title>Trust and privacy correlations in social networks: A deep learning framework</chapter-title>, in: <source>2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.1109/ASONAM.2016.7752236</pub-id>.</mixed-citation>
</ref>
<ref id="ref089">
<label>[89]</label><mixed-citation publication-type="chapter"><string-name><given-names>G.</given-names> <surname>Jawaheer</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Szomszor</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Kostkova</surname></string-name>, <chapter-title>Comparison of implicit and explicit feedback from an online music recommendation service</chapter-title>, in: <source>Proceedings of the 1st International Workshop on Information Heterogeneity and Fusion in Recommender Systems</source>, <year>2010</year>, <publisher-name>ACM</publisher-name>, pp. <fpage>47</fpage>–<lpage>51</lpage>. doi:<pub-id pub-id-type="doi">10.1145/1869446.1869453</pub-id>.</mixed-citation>
</ref>
<ref id="ref090">
<label>[90]</label><mixed-citation publication-type="chapter"><string-name><given-names>N.</given-names> <surname>Jiang</surname></string-name> and <string-name><given-names>L.</given-names> <surname>Li</surname></string-name>, <chapter-title>Doubly robust off-policy value evaluation for reinforcement learning</chapter-title>, in: <source>International Conference on Machine Learning</source>, <year>2016</year>, pp. <fpage>652</fpage>–<lpage>661</lpage>. <uri>http://proceedings.mlr.press/v48/jiang16.html</uri>.</mixed-citation>
</ref>
<ref id="ref091">
<label>[91]</label><mixed-citation publication-type="other"><string-name><given-names>Z.</given-names> <surname>Jin</surname></string-name> and <string-name><given-names>Z.</given-names> <surname>Huajun</surname></string-name>, <chapter-title>Multi-objective reinforcement learning algorithm and its improved convergency method</chapter-title>, in: <source>2011 6th IEEE Conference on Industrial Electronics and Applications</source>, <year>2011</year>. doi:<pub-id pub-id-type="doi">10.1109/ICIEA.2011.5976002</pub-id>.</mixed-citation>
</ref>
<ref id="ref092">
<label>[92]</label><mixed-citation publication-type="chapter"><string-name><given-names>S.</given-names> <surname>Junges</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Jansen</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Dehnert</surname></string-name>, <string-name><given-names>U.</given-names> <surname>Topcu</surname></string-name> and <string-name><given-names>J.-P.</given-names> <surname>Katoen</surname></string-name>, <chapter-title>Safety-constrained reinforcement learning for MDPs</chapter-title>, in: <source>International Conference on Tools and Algorithms for the Construction and Analysis of Systems</source>, <publisher-name>Springer</publisher-name>, <year>2016</year>, pp. <fpage>130</fpage>–<lpage>146</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-662-49674-9_8</pub-id>.</mixed-citation>
</ref>
<ref id="ref093">
<label>[93]</label><mixed-citation publication-type="other"><string-name><given-names>A.A.</given-names> <surname>Kardan</surname></string-name> and <string-name><given-names>O.R.B.</given-names> <surname>Speily</surname></string-name>, <chapter-title>Smart lifelong learning system based on Q-learning</chapter-title>, in: <source>2010 Seventh International Conference on Information Technology: New Generations</source>, <year>2010</year>. doi:<pub-id pub-id-type="doi">10.1109/ITNG.2010.140</pub-id>.</mixed-citation>
</ref>
<ref id="ref094">
<label>[94]</label><mixed-citation publication-type="journal"><string-name><given-names>I.</given-names> <surname>Kastanis</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Slater</surname></string-name>, <article-title>Reinforcement learning utilizes proxemics</article-title>, <source>ACM Transactions on Applied Perception</source> <volume>9</volume>(<issue>1</issue>) (<year>2012</year>), <fpage>1</fpage>–<lpage>15</lpage>. doi:<pub-id pub-id-type="doi">10.1145/2134203.2134206</pub-id>.</mixed-citation>
</ref>
<ref id="ref095">
<label>[95]</label><mixed-citation publication-type="chapter"><string-name><given-names>S.</given-names> <surname>Keizer</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Rossignol</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Chandramohan</surname></string-name> and <string-name><given-names>O.</given-names> <surname>Pietquin</surname></string-name>, <chapter-title>User simulation in the development of statistical spoken dialogue systems</chapter-title>, in: <source>Data-Driven Methods for Adaptive Spoken Dialogue Systems</source>, <publisher-name>Springer</publisher-name>, <year>2012</year>, pp. <fpage>39</fpage>–<lpage>74</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-1-4614-4803-7_4</pub-id>.</mixed-citation>
</ref>
<ref id="ref096">
<label>[96]</label><mixed-citation publication-type="chapter"><string-name><given-names>M.K.</given-names> <surname>Khribi</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Jemni</surname></string-name> and <string-name><given-names>O.</given-names> <surname>Nasraoui</surname></string-name>, <chapter-title>Automatic recommendations for e-learning personalization based on web usage mining techniques and information retrieval</chapter-title>, in: <source>Eighth IEEE International Conference on Advanced Learning Technologies, ICALT’08</source>, <publisher-name>IEEE</publisher-name>, <year>2008</year>, pp. <fpage>241</fpage>–<lpage>245</lpage>. doi:<pub-id pub-id-type="doi">10.1109/ICALT.2008.198</pub-id>.</mixed-citation>
</ref>
<ref id="ref097">
<label>[97]</label><mixed-citation publication-type="chapter"><string-name><given-names>J.</given-names> <surname>Kober</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Peters</surname></string-name>, <chapter-title>Reinforcement learning in robotics: A survey</chapter-title>, in: <source>Reinforcement Learning</source>, <string-name><given-names>M.</given-names> <surname>Wiering</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Van Otterlo</surname></string-name>, eds, <series>Adaptation, Learning, and Optimization</series>, Vol. <volume>12</volume>, <publisher-name>Springer</publisher-name>, <year>2012</year>, pp. <fpage>596</fpage>–<lpage>597</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-642-27645-3_18</pub-id>.</mixed-citation>
</ref>
<ref id="ref098">
<label>[98]</label><mixed-citation publication-type="chapter"><string-name><given-names>V.R.</given-names> <surname>Konda</surname></string-name> and <string-name><given-names>J.N.</given-names> <surname>Tsitsiklis</surname></string-name>, <chapter-title>Actor-critic algorithms</chapter-title>, in: <source>Advances in Neural Information Processing Systems</source>, <year>2000</year>, pp. <fpage>1008</fpage>–<lpage>1014</lpage>. doi:<pub-id pub-id-type="doi">10.5555/3009657.3009799</pub-id>.</mixed-citation>
</ref>
<ref id="ref099">
<label>[99]</label><mixed-citation publication-type="other"><string-name><given-names>I.</given-names> <surname>Koukoutsidis</surname></string-name>, <chapter-title>A learning strategy for paging in mobile environments</chapter-title>, in: <source>5th European Personal Mobile Communications Conference 2003</source>, <year>2003</year>. doi:<pub-id pub-id-type="doi">10.1049/cp:20030322</pub-id>.</mixed-citation>
</ref>
<ref id="ref100">
<label>[100]</label><mixed-citation publication-type="chapter"><string-name><given-names>R.</given-names> <surname>Kozierok</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Maes</surname></string-name>, <chapter-title>A learning interface agent for scheduling meetings</chapter-title>, in: <source>Proceedings of the 1st International Conference on Intelligent User Interfaces</source>, <publisher-name>ACM</publisher-name>, <year>1993</year>, pp. <fpage>81</fpage>–<lpage>88</lpage>. doi:<pub-id pub-id-type="doi">10.1145/169891.169908</pub-id>.</mixed-citation>
</ref>
<ref id="ref101">
<label>[101]</label><mixed-citation publication-type="journal"><string-name><given-names>E.F.</given-names> <surname>Krakow</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hemmer</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Logan</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Arora</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Spellman</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Couriel</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Alousi</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Pidala</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Last</surname></string-name> <etal>et al.</etal>, <article-title>Tools for the precision medicine era: How to develop highly personalized treatment recommendations from cohort and registry data using Q-learning</article-title>, <source>American Journal of Epidemiology</source> <volume>186</volume>(<issue>2</issue>) (<year>2017</year>), <fpage>160</fpage>–<lpage>172</lpage>. doi:<pub-id pub-id-type="doi">10.1093/aje/kwx027</pub-id>.</mixed-citation>
</ref>
<ref id="ref102">
<label>[102]</label><mixed-citation publication-type="journal"><string-name><given-names>T.L.</given-names> <surname>Lai</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Robbins</surname></string-name>, <article-title>Asymptotically efficient adaptive allocation rules</article-title>, <source>Advances in Applied Mathematics</source> <volume>6</volume>(<issue>1</issue>) (<year>1985</year>), <fpage>4</fpage>–<lpage>22</lpage>. doi:<pub-id pub-id-type="doi">10.1016/0196-8858(85)90002-8</pub-id>.</mixed-citation>
</ref>
<ref id="ref103">
<label>[103]</label><mixed-citation publication-type="other"><string-name><given-names>A.S.</given-names> <surname>Lan</surname></string-name> and <string-name><given-names>R.G.</given-names> <surname>Baraniuk</surname></string-name>, <chapter-title>A contextual bandits framework for personalized learning action selection</chapter-title>, in: <source>EDM</source>, <year>2016</year>. <uri>http://www.educationaldatamining.org/EDM2016/proceedings/paper_18.pdf</uri>.</mixed-citation>
</ref>
<ref id="ref104">
<label>[104]</label><mixed-citation publication-type="chapter"><string-name><given-names>G.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Bauer</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Faratin</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Wroclawski</surname></string-name>, <chapter-title>Learning user preferences for wireless services provisioning</chapter-title>, in: <source>Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2004</source>, <year>2004</year>, pp. <fpage>480</fpage>–<lpage>487</lpage>. doi:<pub-id pub-id-type="doi">10.5555/1018409.1018782</pub-id>.</mixed-citation>
</ref>
<ref id="ref105">
<label>[105]</label><mixed-citation publication-type="chapter"><string-name><given-names>O.</given-names> <surname>Lemon</surname></string-name>, <chapter-title>Conversational interfaces</chapter-title>, in: <source>Data-Driven Methods for Adaptive Spoken Dialogue Systems</source>, <publisher-name>Springer</publisher-name>, <year>2012</year>, pp. <fpage>1</fpage>–<lpage>4</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-1-4614-4803-7</pub-id>.</mixed-citation>
</ref>
<ref id="ref106">
<label>[106]</label><mixed-citation publication-type="other"><string-name><given-names>K.</given-names> <surname>Li</surname></string-name> and <string-name><given-names>M.Q.-H.</given-names> <surname>Meng</surname></string-name>, <article-title>Personalizing a service robot by learning human habits from behavioral footprints</article-title>, <source>Engineering</source> <volume>1</volume>(<issue>1</issue>) (<year>2015</year>), <elocation-id>079</elocation-id>. doi:<pub-id pub-id-type="doi">10.15302/J-ENG-2015024</pub-id>.</mixed-citation>
</ref>
<ref id="ref107">
<label>[107]</label><mixed-citation publication-type="chapter"><string-name><given-names>L.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Chu</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Langford</surname></string-name> and <string-name><given-names>R.E.</given-names> <surname>Schapire</surname></string-name>, <chapter-title>A contextual-bandit approach to personalized news article recommendation</chapter-title>, in: <source>Proceedings of the 19th International Conference on World Wide Web</source>, <publisher-name>ACM</publisher-name>, <year>2010</year>, pp. <fpage>661</fpage>–<lpage>670</lpage>. doi:<pub-id pub-id-type="doi">10.1145/1772690.1772758</pub-id>.</mixed-citation>
</ref>
<ref id="ref108">
<label>[108]</label><mixed-citation publication-type="other"><string-name><given-names>Z.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Kiseleva</surname></string-name>, <string-name><given-names>M.</given-names> <surname>de Rijke</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Grotov</surname></string-name>, <chapter-title>Towards learning reward functions from user interactions</chapter-title>, in: <source>Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval – ICTIR ’17</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1145/3121050.3121098</pub-id>.</mixed-citation>
</ref>
<ref id="ref109">
<label>[109]</label><mixed-citation publication-type="other"><string-name><given-names>E.</given-names> <surname>Liebman</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Stone</surname></string-name>, <chapter-title>DJ-MC: A reinforcement-learning agent for music playlist recommendation</chapter-title>, in: <source>AAMAS</source>, <year>2015</year>. doi:<pub-id pub-id-type="doi">10.5555/2772879.2772954</pub-id>.</mixed-citation>
</ref>
<ref id="ref110">
<label>[110]</label><mixed-citation publication-type="other"><string-name><given-names>J.</given-names> <surname>Lim</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Son</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Lee</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Lee</surname></string-name>, <chapter-title>An MARL-based distributed learning scheme for capturing user preferences in a smart environment</chapter-title>, in: <source>2017 IEEE International Conference on Services Computing (SCC)</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1109/SCC.2017.24</pub-id>.</mixed-citation>
</ref>
<ref id="ref111">
<label>[111]</label><mixed-citation publication-type="journal"><string-name><given-names>L.-J.</given-names> <surname>Lin</surname></string-name>, <article-title>Self-improving reactive agents based on reinforcement learning, planning and teaching</article-title>, <source>Machine Learning</source> <volume>8</volume>(<issue>3–4</issue>) (<year>1992</year>), <fpage>293</fpage>–<lpage>321</lpage>. doi:<pub-id pub-id-type="doi">10.1007/BF00992699</pub-id>.</mixed-citation>
</ref>
<ref id="ref112">
<label>[112]</label><mixed-citation publication-type="chapter"><string-name><given-names>Q.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Cui</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Wei</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Peng</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Deng</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Hao</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Huang</surname></string-name> and <string-name><given-names>K.-F.</given-names> <surname>Wong</surname></string-name>, <chapter-title>Building personalized simulator for interactive search</chapter-title>, in: <source>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)</source>, <year>2019</year>, pp. <fpage>5109</fpage>–<lpage>5115</lpage>. doi:<pub-id pub-id-type="doi">10.24963/ijcai.2019/710</pub-id>.</mixed-citation>
</ref>
<ref id="ref113">
<label>[113]</label><mixed-citation publication-type="other"><string-name><given-names>Y.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Logan</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Tang</surname></string-name> and <string-name><given-names>Y.</given-names> <surname>Wang</surname></string-name>, <chapter-title>Deep reinforcement learning for dynamic treatment regimes on medical registry data</chapter-title>, in: <source>2017 IEEE International Conference on Healthcare Informatics (ICHI)</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1109/ICHI.2017.45</pub-id>.</mixed-citation>
</ref>
<ref id="ref114">
<label>[114]</label><mixed-citation publication-type="journal"><string-name><surname>Llorente</surname></string-name> and <string-name><given-names>S.E.</given-names> <surname>Guerrero</surname></string-name>, <article-title>Increasing retrieval quality in conversational recommenders</article-title>, <source>IEEE Transactions on Knowledge and Data Engineering</source> <volume>24</volume>(<issue>10</issue>) (<year>2012</year>), <fpage>1876</fpage>–<lpage>1888</lpage>. doi:<pub-id pub-id-type="doi">10.1109/TKDE.2011.116</pub-id>.</mixed-citation>
</ref>
<ref id="ref115">
<label>[115]</label><mixed-citation publication-type="journal"><string-name><given-names>H.M.S.</given-names> <surname>Lotfy</surname></string-name>, <string-name><given-names>S.M.S.</given-names> <surname>Khamis</surname></string-name> and <string-name><given-names>M.M.</given-names> <surname>Aboghazalah</surname></string-name>, <article-title>Multi-agents and learning: Implications for webusage mining</article-title>, <source>Journal of Advanced Research</source> <volume>7</volume>(<issue>2</issue>) (<year>2016</year>), <fpage>285</fpage>–<lpage>295</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jare.2015.06.005</pub-id>.</mixed-citation>
</ref>
<ref id="ref116">
<label>[116]</label><mixed-citation publication-type="other"><string-name><given-names>C.</given-names> <surname>Lowery</surname></string-name> and <string-name><given-names>A.A.</given-names> <surname>Faisal</surname></string-name>, <chapter-title>Towards efficient, personalized anesthesia using continuous reinforcement learning for propofol infusion control</chapter-title>, in: <source>2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)</source>, <year>2013</year>. doi:<pub-id pub-id-type="doi">10.1109/NER.2013.6696208</pub-id>.</mixed-citation>
</ref>
<ref id="ref117">
<label>[117]</label><mixed-citation publication-type="other"><string-name><given-names>O.</given-names> <surname>Madani</surname></string-name> and <string-name><given-names>D.</given-names> <surname>DeCoste</surname></string-name>, <chapter-title>Contextual recommender problems [extended abstract]</chapter-title>, in: <source>Proceedings of the 1st International Workshop on Utility-Based Data Mining – UBDM ’05</source>, <year>2005</year>. doi:<pub-id pub-id-type="doi">10.1145/1089827.1089838</pub-id>.</mixed-citation>
</ref>
<ref id="ref118">
<label>[118]</label><mixed-citation publication-type="chapter"><string-name><given-names>P.</given-names> <surname>Maes</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Kozierok</surname></string-name>, <chapter-title>Learning interface agents</chapter-title>, in: <source>AAAI</source>, Vol. <volume>93</volume>, <year>1993</year>, pp. <fpage>459</fpage>–<lpage>465</lpage>. <uri>https://www.aaai.org/Papers/AAAI/1993/AAAI93-069.pdf</uri>.</mixed-citation>
</ref>
<ref id="ref119">
<label>[119]</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Mahmood</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Mujtaba</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Venturini</surname></string-name>, <article-title>Dynamic personalization in conversational recommender systems</article-title>, <source>Information Systems and e-Business Management</source> <volume>12</volume>(<issue>2</issue>) (<year>2013</year>), <fpage>213</fpage>–<lpage>238</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s10257-013-0222-3</pub-id>.</mixed-citation>
</ref>
<ref id="ref120">
<label>[120]</label><mixed-citation publication-type="other"><string-name><given-names>T.</given-names> <surname>Mahmood</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Ricci</surname></string-name>, <chapter-title>Learning and adaptivity in interactive recommender systems</chapter-title>, in: <source>Proceedings of the Ninth International Conference on Electronic Commerce – ICEC ’07</source>, <year>2007</year>. doi:<pub-id pub-id-type="doi">10.1145/1282100.1282114</pub-id>.</mixed-citation>
</ref>
<ref id="ref121">
<label>[121]</label><mixed-citation publication-type="other"><string-name><given-names>A.</given-names> <surname>Malpani</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Ravindran</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Murthy</surname></string-name>, <chapter-title>Personalized intelligent tutoring system using reinforcement learning</chapter-title>, in: <source>FLAIRS Conference</source>, <year>2011</year>. <uri>https://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS11/paper/viewPaper/2597</uri>.</mixed-citation>
</ref>
<ref id="ref122">
<label>[122]</label><mixed-citation publication-type="journal"><string-name><given-names>U.</given-names> <surname>Manber</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Patel</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Robison</surname></string-name>, <article-title>Experience with personalization of Yahoo!</article-title>, <source>Communications of the ACM</source> <volume>43</volume>(<issue>8</issue>) (<year>2000</year>), <fpage>35</fpage>–<lpage>39</lpage>. doi:<pub-id pub-id-type="doi">10.1145/345124.345136</pub-id>.</mixed-citation>
</ref>
<ref id="ref123">
<label>[123]</label><mixed-citation publication-type="other"><string-name><given-names>I.</given-names> <surname>Manickam</surname></string-name>, <string-name><given-names>A.S.</given-names> <surname>Lan</surname></string-name> and <string-name><given-names>R.G.</given-names> <surname>Baraniuk</surname></string-name>, <chapter-title>Contextual multi-armed bandit algorithms for personalized learning action selection</chapter-title>, in: <source>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1109/ICASSP.2017.7953377</pub-id>.</mixed-citation>
</ref>
<ref id="ref124">
<label>[124]</label><mixed-citation publication-type="chapter"><string-name><given-names>K.N.</given-names> <surname>Martin</surname></string-name> and <string-name><given-names>I.</given-names> <surname>Arroyo</surname></string-name>, <chapter-title>AgentX: Using reinforcement learning to improve the effectiveness of intelligent tutoring systems</chapter-title>, in: <source>International Conference on Intelligent Tutoring Systems</source>, <publisher-name>Springer</publisher-name>, <year>2004</year>, pp. <fpage>564</fpage>–<lpage>572</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-540-30139-4_53</pub-id>.</mixed-citation>
</ref>
<ref id="ref125">
<label>[125]</label><mixed-citation publication-type="journal"><string-name><given-names>J.D.</given-names> <surname>Martín-Guerrero</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Gomez</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Soria-Olivas</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Schmidhuber</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Climente-Martí</surname></string-name> and <string-name><given-names>N.V.</given-names> <surname>Jiménez-Torres</surname></string-name>, <article-title>A reinforcement learning approach for individualizing erythropoietin dosages in hemodialysis patients</article-title>, <source>Expert Systems with Applications</source> <volume>36</volume>(<issue>6</issue>) (<year>2009</year>), <fpage>9737</fpage>–<lpage>9742</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.eswa.2009.02.041</pub-id>.</mixed-citation>
</ref>
<ref id="ref126">
<label>[126]</label><mixed-citation publication-type="chapter"><string-name><given-names>J.D.</given-names> <surname>Martín-Guerrero</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Soria-Olivas</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Martínez-Sober</surname></string-name>, <string-name><given-names>A.J.</given-names> <surname>Serrrano-López</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Magdalena-Benedito</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Gómez-Sanchis</surname></string-name>, <chapter-title>Use of reinforcement learning in two real applications</chapter-title>, in: <source>Recent Advances in Reinforcement Learning</source>, <year>2008</year>, pp. <fpage>191</fpage>–<lpage>204</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-540-89722-4_15</pub-id>.</mixed-citation>
</ref>
<ref id="ref127">
<label>[127]</label><mixed-citation publication-type="other"><string-name><given-names>D.</given-names> <surname>Massimo</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Elahi</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Ricci</surname></string-name>, <chapter-title>Learning user preferences by observing user-items interactions in an IoT augmented space</chapter-title>, in: <source>Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization – UMAP ’17</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1145/3099023.3099070</pub-id>.</mixed-citation>
</ref>
<ref id="ref128">
<label>[128]</label><mixed-citation publication-type="other"><string-name><given-names>K.</given-names> <surname>Masumitsu</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Echigo</surname></string-name>, <chapter-title>Video summarization using reinforcement learning in eigenspace</chapter-title>, in: <source>Proceedings 2000 International Conference on Image Processing (Cat. No. 00CH37101)</source>, <year>2000</year>. doi:<pub-id pub-id-type="doi">10.1109/ICIP.2000.899351</pub-id>.</mixed-citation>
</ref>
<ref id="ref129">
<label>[129]</label><mixed-citation publication-type="journal"><string-name><given-names>B.C.</given-names> <surname>May</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Korda</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Lee</surname></string-name> and <string-name><given-names>D.S.</given-names> <surname>Leslie</surname></string-name>, <article-title>Optimistic Bayesian sampling in contextual-bandit problems</article-title>, <source>Journal of Machine Learning Research</source> <volume>13</volume> (<year>2012</year>), <fpage>2069</fpage>–<lpage>2106</lpage>. <uri>http://jmlr.org/papers/v13/may12a.html</uri>.</mixed-citation>
</ref>
<ref id="ref130">
<label>[130]</label><mixed-citation publication-type="other"><string-name><given-names>E.</given-names> <surname>Mengelkamp</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Gärttner</surname></string-name> and <string-name><given-names>C.</given-names> <surname>Weinhardt</surname></string-name>, <chapter-title>Intelligent agent strategies for residential customers in local electricity markets</chapter-title>, in: <source>Proceedings of the Ninth International Conference on Future Energy Systems – e-Energy ’18</source>, <year>2018</year>. doi:<pub-id pub-id-type="doi">10.1145/3208903.3208907</pub-id>.</mixed-citation>
</ref>
<ref id="ref131">
<label>[131]</label><mixed-citation publication-type="other"><string-name><given-names>E.</given-names> <surname>Mengelkamp</surname></string-name> and <string-name><given-names>C.</given-names> <surname>Weinhardt</surname></string-name>, <chapter-title>Clustering household preferences in local electricity markets</chapter-title>, in: <source>Proceedings of the Ninth International Conference on Future Energy Systems – e-Energy ’18</source>, <year>2018</year>. doi:<pub-id pub-id-type="doi">10.1145/3208903.3214348</pub-id>.</mixed-citation>
</ref>
<ref id="ref132">
<label>[132]</label><mixed-citation publication-type="chapter"><string-name><given-names>N.</given-names> <surname>Merkle</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Zander</surname></string-name>, <chapter-title>Agent-based assistance in ambient assisted living through reinforcement learning and semantic technologies</chapter-title>, in: <source>OTM Confederated International Conferences “On the Move to Meaningful Internet Systems”</source>, <series>Lecture Notes in Computer Science</series>, Vol. <volume>10574</volume>, <year>2017</year>, pp. <fpage>180</fpage>–<lpage>188</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-319-69459-7_12</pub-id>.</mixed-citation>
</ref>
<ref id="ref133">
<label>[133]</label><mixed-citation publication-type="other"><string-name><given-names>V.</given-names> <surname>Mnih</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Kavukcuoglu</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Silver</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Graves</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Antonoglou</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Wierstra</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Riedmiller</surname></string-name>, Playing atari with deep reinforcement learning, Preprint, <pub-id pub-id-type="arxiv">arXiv:1312.5602</pub-id>, 2013.</mixed-citation>
</ref>
<ref id="ref134">
<label>[134]</label><mixed-citation publication-type="other"><string-name><given-names>K.</given-names> <surname>Mo</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Li</surname></string-name> and <string-name><given-names>Q.</given-names> <surname>Yang</surname></string-name>, <chapter-title>Personalizing a dialogue system with transfer reinforcement learning</chapter-title>, in: <source>AAAI</source>, <year>2018</year>. <uri>https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16104</uri>.</mixed-citation>
</ref>
<ref id="ref135">
<label>[135]</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Moher</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Liberati</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Tetzlaff</surname></string-name> and <string-name><given-names>D.G.</given-names> <surname>Altman</surname></string-name>, <article-title>Preferred reporting items for systematic reviews and meta-analyses: The PRISMA statement</article-title>, <source>Annals of Internal Medicine</source> <volume>151</volume>(<issue>4</issue>) (<year>2009</year>), <fpage>264</fpage>–<lpage>269</lpage>. doi:<pub-id pub-id-type="doi">10.7326/0003-4819-151-4-200908180-00135</pub-id>.</mixed-citation>
</ref>
<ref id="ref136">
<label>[136]</label><mixed-citation publication-type="other"><string-name><given-names>T.M.</given-names> <surname>Moldovan</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Abbeel</surname></string-name>, Safe exploration in Markov decision processes, Preprint, <pub-id pub-id-type="arxiv">arXiv:1205.4810</pub-id>, 2012.</mixed-citation>
</ref>
<ref id="ref137">
<label>[137]</label><mixed-citation publication-type="other"><string-name><given-names>O.</given-names> <surname>Moling</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Baltrunas</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Ricci</surname></string-name>, <chapter-title>Optimal radio channel recommendations with explicit and implicit feedback</chapter-title>, in: <source>Proceedings of the Sixth ACM Conference on Recommender Systems – RecSys ’12</source>, <year>2012</year>. doi:<pub-id pub-id-type="doi">10.1145/2365952.2365971</pub-id>.</mixed-citation>
</ref>
<ref id="ref138">
<label>[138]</label><mixed-citation publication-type="other"><string-name><given-names>A.</given-names> <surname>Moon</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Kang</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Kim</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Kim</surname></string-name>, <chapter-title>A service recommendation using reinforcement learning for network-based robots in ubiquitous computing environments</chapter-title>, in: <source>RO-MAN 2007 – The 16th IEEE International Symposium on Robot and Human Interactive Communication</source>, <year>2007</year>. doi:<pub-id pub-id-type="doi">10.1109/ROMAN.2007.4415198</pub-id>.</mixed-citation>
</ref>
<ref id="ref139">
<label>[139]</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Narvekar</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Sinapov</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Stone</surname></string-name>, <chapter-title>Autonomous task sequencing for customized curriculum design in reinforcement learning</chapter-title>, in: <source>IJCAI</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.5555/3172077.3172241</pub-id>.</mixed-citation>
</ref>
<ref id="ref140">
<label>[140]</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Nemati</surname></string-name>, <string-name><given-names>M.M.</given-names> <surname>Ghassemi</surname></string-name> and <string-name><given-names>G.D.</given-names> <surname>Clifford</surname></string-name>, <chapter-title>Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach</chapter-title>, in: <source>2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.1109/EMBC.2016.7591355</pub-id>.</mixed-citation>
</ref>
<ref id="ref141">
<label>[141]</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Neumann</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Mansi</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Itu</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Georgescu</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Kayvanpour</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Sedaghat-Hamedani</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Amr</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Haas</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Katus</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Meder</surname></string-name> <etal>et al.</etal>, <article-title>A self-taught artificial agent for multi-physics computational model personalization</article-title>, <source>Medical Image Analysis</source> <volume>34</volume> (<year>2016</year>), <fpage>52</fpage>–<lpage>64</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.media.2016.04.003</pub-id>.</mixed-citation>
</ref>
<ref id="ref142">
<label>[142]</label><mixed-citation publication-type="chapter"><string-name><given-names>D.</given-names> <surname>Neumann</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Mansi</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Itu</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Georgescu</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Kayvanpour</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Sedaghat-Hamedani</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Haas</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Katus</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Meder</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Steidl</surname></string-name> <etal>et al.</etal>, <chapter-title>Vito – A generic agent for multi-physics model personalization: Application to heart modeling</chapter-title>, in: <source>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015</source>, <year>2015</year>, pp. <fpage>442</fpage>–<lpage>449</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-319-24571-3_53</pub-id>.</mixed-citation>
</ref>
<ref id="ref143">
<label>[143]</label><mixed-citation publication-type="chapter"><string-name><given-names>D.W.</given-names> <surname>Oard</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Kim</surname></string-name> <etal>et al.</etal>, <chapter-title>Implicit feedback for recommender systems</chapter-title>, in: <source>Proceedings of the AAAI Workshop on Recommender Systems</source>, <publisher-name>AAAI Press</publisher-name>, <publisher-loc>Menlo Park, CA</publisher-loc>, <year>1998</year>, pp. <fpage>81</fpage>–<lpage>83</lpage>. <uri>https://www.aaai.org/Papers/Workshops/1998/WS-98-08/WS98-08-021.pdf</uri>.</mixed-citation>
</ref>
<ref id="ref144">
<label>[144]</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Oh</surname></string-name> and <string-name><given-names>C.L.</given-names> <surname>Tan</surname></string-name>, <article-title>Making better recommendations with online profiling agents</article-title>, <source>AI Magazine</source> <volume>26</volume> (<year>2004</year>), <fpage>29</fpage>–<lpage>40</lpage>. doi:<pub-id pub-id-type="doi">10.1609/aimag.v26i3.1823</pub-id>.</mixed-citation>
</ref>
<ref id="ref145">
<label>[145]</label><mixed-citation publication-type="other"><string-name><given-names>P.</given-names> <surname>Ondruska</surname></string-name> and <string-name><given-names>I.</given-names> <surname>Posner</surname></string-name>, <chapter-title>The route not taken: Driver-centric estimation of electric vehicle range</chapter-title>, in: <source>Twenty-Fourth International Conference on Automated Planning and Scheduling</source>, <year>2014</year>. <uri>https://www.aaai.org/ocs/index.php/ICAPS/ICAPS14/paper/viewPaper/7899</uri>.</mixed-citation>
</ref>
<ref id="ref146">
<label>[146]</label><mixed-citation publication-type="journal"><string-name><given-names>S.J.</given-names> <surname>Pan</surname></string-name> and <string-name><given-names>Q.</given-names> <surname>Yang</surname></string-name>, <article-title>A survey on transfer learning</article-title>, <source>IEEE Transactions on Knowledge and Data Engineering</source> <volume>22</volume>(<issue>10</issue>) (<year>2010</year>), <fpage>1345</fpage>–<lpage>1359</lpage>. doi:<pub-id pub-id-type="doi">10.1109/TKDE.2009.191</pub-id>.</mixed-citation>
</ref>
<ref id="ref147">
<label>[147]</label><mixed-citation publication-type="other"><string-name><given-names>V.</given-names> <surname>Pant</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Bhasin</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Jain</surname></string-name>, <chapter-title>Self-learning system for personalized e-learning</chapter-title>, in: <source>2017 International Conference on Emerging Trends in Computing and Communication Technologies (ICETCCT)</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1109/ICETCCT.2017.8280344</pub-id>.</mixed-citation>
</ref>
<ref id="ref148">
<label>[148]</label><mixed-citation publication-type="other"><string-name><given-names>P.</given-names> <surname>Patompak</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Jeong</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Nilkhamhang</surname></string-name> and <string-name><given-names>N.Y.</given-names> <surname>Chong</surname></string-name>, <chapter-title>Learning social relations for culture aware interaction</chapter-title>, in: <source>2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1109/URAI.2017.7992879</pub-id>.</mixed-citation>
</ref>
<ref id="ref149">
<label>[149]</label><mixed-citation publication-type="chapter"><string-name><given-names>M.</given-names> <surname>Pecka</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Svoboda</surname></string-name>, <chapter-title>Safe exploration techniques for reinforcement learning – An overview</chapter-title>, in: <source>International Workshop on Modelling and Simulation for Autonomous Systems</source>, <publisher-name>Springer</publisher-name>, <year>2014</year>, pp. <fpage>357</fpage>–<lpage>375</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-319-13823-7_31</pub-id>.</mixed-citation>
</ref>
<ref id="ref150">
<label>[150]</label><mixed-citation publication-type="other"><string-name><given-names>B.</given-names> <surname>Peng</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Jiao</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Kurner</surname></string-name>, <chapter-title>Angle of arrival estimation in dynamic indoor THz channels with Bayesian filter and reinforcement learning</chapter-title>, in: <source>2016 24th European Signal Processing Conference (EUSIPCO)</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.1109/EUSIPCO.2016.7760594</pub-id>.</mixed-citation>
</ref>
<ref id="ref151">
<label>[151]</label><mixed-citation publication-type="other"><string-name><given-names>C.</given-names> <surname>Peng</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Vuorimaa</surname></string-name>, <chapter-title>Automatic navigation among mobile DTV services</chapter-title>, in: <source>ICEIS</source>, <year>2004</year>. doi:<pub-id pub-id-type="doi">10.5220/0002629501400145</pub-id>.</mixed-citation>
</ref>
<ref id="ref152">
<label>[152]</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Perera</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Zaslavsky</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Christen</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Georgakopoulos</surname></string-name>, <article-title>Context aware computing for the Internet of things: A survey</article-title>, <source>IEEE Communications Surveys &amp; Tutorials</source> <volume>16</volume>(<issue>1</issue>) (<year>2014</year>), <fpage>414</fpage>–<lpage>454</lpage>. doi:<pub-id pub-id-type="doi">10.1109/SURV.2013.042313.00197</pub-id>.</mixed-citation>
</ref>
<ref id="ref153">
<label>[153]</label><mixed-citation publication-type="journal"><string-name><given-names>T.J.</given-names> <surname>Perkins</surname></string-name> and <string-name><given-names>A.G.</given-names> <surname>Barto</surname></string-name>, <article-title>Lyapunov design for safe reinforcement learning</article-title>, <source>Journal of Machine Learning Research</source> <volume>3</volume> (<year>2002</year>), <fpage>803</fpage>–<lpage>832</lpage>. <uri>http://www.jmlr.org/papers/v3/perkins02a.html</uri>.</mixed-citation>
</ref>
<ref id="ref154">
<label>[154]</label><mixed-citation publication-type="journal"><string-name><given-names>B.J.</given-names> <surname>Pine</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Victor</surname></string-name> and <string-name><given-names>A.C.</given-names> <surname>Boynton</surname></string-name>, <article-title>Making mass customization work</article-title>, <source>Harvard Business Review</source> <volume>71</volume>(<issue>5</issue>) (<year>1993</year>), <fpage>108</fpage>–<lpage>111</lpage>. <uri>https://hbr.org/1993/09/making-mass-customization-work</uri>.</mixed-citation>
</ref>
<ref id="ref155">
<label>[155]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Pineau</surname></string-name>, <string-name><given-names>M.G.</given-names> <surname>Bellemare</surname></string-name>, <string-name><given-names>A.J.</given-names> <surname>Rush</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Ghizaru</surname></string-name> and <string-name><given-names>S.A.</given-names> <surname>Murphy</surname></string-name>, <article-title>Constructing evidence-based treatment strategies using methods from computer science</article-title>, <source>Drug and Alcohol Dependence</source> <volume>88</volume> (<year>2007</year>), <fpage>S52</fpage>–<lpage>S60</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.drugalcdep.2007.01.005</pub-id>.</mixed-citation>
</ref>
<ref id="ref156">
<label>[156]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Pomprapa</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Leonhardt</surname></string-name> and <string-name><given-names>B.J.E.</given-names> <surname>Misgeld</surname></string-name>, <article-title>Optimal learning control of oxygen saturation using a policy iteration algorithm and a proof-of-concept in an interconnecting three-tank system</article-title>, <source>Control Engineering Practice</source> <volume>59</volume> (<year>2017</year>), <fpage>194</fpage>–<lpage>203</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.conengprac.2016.07.014</pub-id>.</mixed-citation>
</ref>
<ref id="ref157">
<label>[157]</label><mixed-citation publication-type="other"><string-name><given-names>N.</given-names> <surname>Prasad</surname></string-name>, <string-name><given-names>L.-F.</given-names> <surname>Cheng</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Chivers</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Draugelis</surname></string-name> and <string-name><given-names>B.E.</given-names> <surname>Engelhardt</surname></string-name>, A reinforcement learning approach to weaning of mechanical ventilation in intensive care units, Preprint, <pub-id pub-id-type="arxiv">arXiv:1704.06300</pub-id>, 2017.</mixed-citation>
</ref>
<ref id="ref158">
<label>[158]</label><mixed-citation publication-type="other"><string-name><given-names>M.</given-names> <surname>Preda</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Popescu</surname></string-name>, <chapter-title>Personalized web recommendations: Supporting epistemic information about end-users</chapter-title>, in: <source>The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI’05)</source>, <year>2005</year>. doi:<pub-id pub-id-type="doi">10.1109/WI.2005.115</pub-id>.</mixed-citation>
</ref>
<ref id="ref159">
<label>[159]</label><mixed-citation publication-type="chapter"><string-name><given-names>F.D.</given-names> <surname>Priscoli</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Fogliati</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Palo</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Pietrabissa</surname></string-name>, <chapter-title>Dynamic class of service mapping for quality of experience control in future networks</chapter-title>, in: <source>WTC 2014; World Telecommunications Congress 2014</source>, <publisher-name>VDE</publisher-name>, <year>2014</year>, pp. <fpage>1</fpage>–<lpage>6</lpage>. <uri>https://www.vde-verlag.de/proceedings-de/453602012.html</uri>.</mixed-citation>
</ref>
<ref id="ref160">
<label>[160]</label><mixed-citation publication-type="other"><string-name><given-names>Z.</given-names> <surname>Qin</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Rishabh</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Carnahan</surname></string-name>, <chapter-title>A scalable approach for periodical personalized recommendations</chapter-title>, in: <source>Proceedings of the 10th ACM Conference on Recommender Systems – RecSys ’16</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.1145/2959100.2959139</pub-id>.</mixed-citation>
</ref>
<ref id="ref161">
<label>[161]</label><mixed-citation publication-type="other"><string-name><given-names>V.R.</given-names> <surname>Raghuveer</surname></string-name>, <string-name><given-names>B.K.</given-names> <surname>Tripathy</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Singh</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Khanna</surname></string-name>, <chapter-title>Reinforcement learning approach towards effective content recommendation in MOOC environments</chapter-title>, in: <source>2014 IEEE International Conference on MOOC, Innovation and Technology in Education (MITE)</source>, <year>2014</year>. doi:<pub-id pub-id-type="doi">10.1109/MITE.2014.7020289</pub-id>.</mixed-citation>
</ref>
<ref id="ref162">
<label>[162]</label><mixed-citation publication-type="other"><string-name><given-names>E.</given-names> <surname>Rennison</surname></string-name>, <chapter-title>Personalized galaxies of information</chapter-title>, in: <source>Companion of the ACM Conference on Human Factors in Computing Systems (CHI’95)</source>, <year>1995</year>. doi:<pub-id pub-id-type="doi">10.1145/223355.223409</pub-id>.</mixed-citation>
</ref>
<ref id="ref163">
<label>[163]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Resnick</surname></string-name> and <string-name><given-names>H.R.</given-names> <surname>Varian</surname></string-name>, <article-title>Recommender systems</article-title>, <source>Communications of the ACM</source> <volume>40</volume>(<issue>3</issue>) (<year>1997</year>), <fpage>56</fpage>–<lpage>58</lpage>. doi:<pub-id pub-id-type="doi">10.1145/245108.245121</pub-id>.</mixed-citation>
</ref>
<ref id="ref164">
<label>[164]</label><mixed-citation publication-type="chapter"><string-name><given-names>F.</given-names> <surname>Ricci</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Rokach</surname></string-name> and <string-name><given-names>B.</given-names> <surname>Shapira</surname></string-name>, <chapter-title>Introduction to recommender systems handbook</chapter-title>, in: <source>Recommender Systems Handbook</source>, <publisher-name>Springer</publisher-name>, <year>2011</year>, pp. <fpage>14</fpage>–<lpage>17</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-0-387-85820-3</pub-id>.</mixed-citation>
</ref>
<ref id="ref165">
<label>[165]</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Riecken</surname></string-name>, <article-title>Personalized views of personalization</article-title>, <source>Communications of the ACM</source> <volume>43</volume>(<issue>8</issue>) (<year>2000</year>), <fpage>26</fpage>–<lpage>28</lpage>. doi:<pub-id pub-id-type="doi">10.1145/345124.345133</pub-id>.</mixed-citation>
</ref>
<ref id="ref166">
<label>[166]</label><mixed-citation publication-type="chapter"><string-name><given-names>M.</given-names> <surname>Riedmiller</surname></string-name>, <chapter-title>Neural fitted Q iteration – First experiences with a data efficient neural reinforcement learning method</chapter-title>, in: <source>European Conference on Machine Learning</source>, <publisher-name>Springer</publisher-name>, <year>2005</year>, pp. <fpage>317</fpage>–<lpage>328</lpage>. doi:<pub-id pub-id-type="doi">10.1007/11564096_32</pub-id>.</mixed-citation>
</ref>
<ref id="ref167">
<label>[167]</label><mixed-citation publication-type="other"><string-name><given-names>H.</given-names> <surname>Ritschel</surname></string-name> and <string-name><given-names>E.</given-names> <surname>André</surname></string-name>, <chapter-title>Real-time robot personality adaptation based on reinforcement learning and social signals</chapter-title>, in: <source>Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human–Robot Interaction – HRI ’17</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1145/3029798.3038381</pub-id>.</mixed-citation>
</ref>
<ref id="ref168">
<label>[168]</label><mixed-citation publication-type="journal"><string-name><given-names>I.</given-names> <surname>Rivas-Blanco</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Lopez-Casado</surname></string-name>, <string-name><given-names>C.J.</given-names> <surname>Perez-del-Pulgar</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Garcia-Vacas</surname></string-name>, <string-name><given-names>J.C.</given-names> <surname>Fraile</surname></string-name> and <string-name><given-names>V.F.</given-names> <surname>Munoz</surname></string-name>, <article-title>Smart cable-driven camera robotic assistant</article-title>, <source>IEEE Transactions on Human–Machine Systems</source> <volume>48</volume>(<issue>2</issue>) (<year>2018</year>), <fpage>183</fpage>–<lpage>196</lpage>. doi:<pub-id pub-id-type="doi">10.1109/THMS.2017.2767286</pub-id>.</mixed-citation>
</ref>
<ref id="ref169">
<label>[169]</label><mixed-citation publication-type="chapter"><string-name><given-names>M.</given-names> <surname>Rudary</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Singh</surname></string-name> and <string-name><given-names>M.E.</given-names> <surname>Pollack</surname></string-name>, <chapter-title>Adaptive cognitive orthotics: Combining reinforcement learning and constraint-based temporal reasoning</chapter-title>, in: <source>Proceedings of the Twenty-First International Conference on Machine Learning</source>, <publisher-name>ACM</publisher-name>, <year>2004</year>, p. <fpage>91</fpage>. doi:<pub-id pub-id-type="doi">10.1145/2959100.2959139</pub-id>.</mixed-citation>
</ref>
<ref id="ref170">
<label>[170]</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Russell</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Norvig</surname></string-name>, <source>Artificial Intelligence: A Modern Approach</source>, <publisher-name>Prentice-Hall</publisher-name>, <publisher-loc>Egnlewood Cliffs, NJ</publisher-loc>, <year>1995</year>. ISBN <isbn>0136042597</isbn>.</mixed-citation>
</ref>
<ref id="ref171">
<label>[171]</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Saha</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Quazi</surname></string-name>, <chapter-title>Emotion-driven learning agent for setting rich presence in mobile telephony</chapter-title>, in: <source>2008 11th International Conference on Computer and Information Technology</source>, <year>2008</year>. doi:<pub-id pub-id-type="doi">10.1109/ICCITECHN.2008.4803023</pub-id>.</mixed-citation>
</ref>
<ref id="ref172">
<label>[172]</label><mixed-citation publication-type="chapter"><string-name><given-names>J.B.</given-names> <surname>Schafer</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Frankowski</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Herlocker</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Sen</surname></string-name>, <chapter-title>Collaborative filtering recommender systems</chapter-title>, in: <source>The Adaptive Web</source>, <publisher-name>Springer</publisher-name>, <year>2007</year>, pp. <fpage>291</fpage>–<lpage>324</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-540-72079-9_9</pub-id>.</mixed-citation>
</ref>
<ref id="ref173">
<label>[173]</label><mixed-citation publication-type="other"><string-name><given-names>Y.A.</given-names> <surname>Sekhavat</surname></string-name>, <chapter-title>MPRL: Multiple-periodic reinforcement learning for difficulty adjustment in rehabilitation games</chapter-title>, in: <source>2017 IEEE 5th International Conference on Serious Games and Applications for Health (SeGAH)</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1109/SeGAH.2017.7939260</pub-id>.</mixed-citation>
</ref>
<ref id="ref174">
<label>[174]</label><mixed-citation publication-type="chapter"><string-name><given-names>Y.-W.</given-names> <surname>Seo</surname></string-name> and <string-name><given-names>B.-T.</given-names> <surname>Zhang</surname></string-name>, <chapter-title>A reinforcement learning agent for personalized information filtering</chapter-title>, in: <source>Proceedings of the 5th International Conference on Intelligent User Interfaces</source>, <publisher-name>ACM</publisher-name>, <year>2000</year>, pp. <fpage>248</fpage>–<lpage>251</lpage>. doi:<pub-id pub-id-type="doi">10.1145/325737.325859</pub-id>.</mixed-citation>
</ref>
<ref id="ref175">
<label>[175]</label><mixed-citation publication-type="chapter"><string-name><given-names>Y.-W.</given-names> <surname>Seo</surname></string-name> and <string-name><given-names>B.-T.</given-names> <surname>Zhang</surname></string-name>, <chapter-title>Learning user’s preferences by analyzing web-browsing behaviors</chapter-title>, in: <source>Proceedings of the Fourth International Conference on Autonomous Agents</source>, <publisher-name>ACM</publisher-name>, <year>2000</year>, pp. <fpage>381</fpage>–<lpage>387</lpage>. doi:<pub-id pub-id-type="doi">10.1145/336595.337546</pub-id>.</mixed-citation>
</ref>
<ref id="ref176">
<label>[176]</label><mixed-citation publication-type="chapter"><string-name><given-names>D.</given-names> <surname>Shawky</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Badawi</surname></string-name>, <chapter-title>A reinforcement learning-based adaptive learning system</chapter-title>, in: <source>Advances in Intelligent Systems and Computing</source>, <year>2018</year>, pp. <fpage>221</fpage>–<lpage>231</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-319-74690-6_22</pub-id>.</mixed-citation>
</ref>
<ref id="ref177">
<label>[177]</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Shen</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Chi</surname></string-name>, <chapter-title>Reinforcement learning: the Sooner the Better, or the Later the Better?</chapter-title>, in: <source>Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization – UMAP ’16</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.1145/2930238.2930247</pub-id>.</mixed-citation>
</ref>
<ref id="ref178">
<label>[178]</label><mixed-citation publication-type="journal"><string-name><given-names>S.M.</given-names> <surname>Shortreed</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Laber</surname></string-name>, <string-name><given-names>D.J.</given-names> <surname>Lizotte</surname></string-name>, <string-name><given-names>T.S.</given-names> <surname>Stroup</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Pineau</surname></string-name> and <string-name><given-names>S.A.</given-names> <surname>Murphy</surname></string-name>, <article-title>Informing sequential clinical decision-making through reinforcement learning: An empirical study</article-title>, <source>Machine Learning</source> <volume>84</volume>(<issue>1–2</issue>) (<year>2011</year>), <fpage>109</fpage>–<lpage>136</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s10994-010-5229-0</pub-id>.</mixed-citation>
</ref>
<ref id="ref179">
<label>[179]</label><mixed-citation publication-type="journal"><string-name><given-names>G.E.</given-names> <surname>Simon</surname></string-name> and <string-name><given-names>R.H.</given-names> <surname>Perlis</surname></string-name>, <article-title>Personalized medicine for depression: Can we match patients with treatments?</article-title>, <source>American Journal of Psychiatry</source> <volume>167</volume>(<issue>12</issue>) (<year>2010</year>), <fpage>1445</fpage>–<lpage>1455</lpage>. doi:<pub-id pub-id-type="doi">10.1176/appi.ajp.2010.09111680</pub-id>.</mixed-citation>
</ref>
<ref id="ref180">
<label>[180]</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Song</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Hsu</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Xu</surname></string-name> and <string-name><given-names>M.</given-names> <surname>van der Schaar</surname></string-name>, <article-title>Using contextual learning to improve diagnostic accuracy: Application in breast cancer screening</article-title>, <source>IEEE Journal of Biomedical and Health Informatics</source> <volume>20</volume>(<issue>3</issue>) (<year>2016</year>), <fpage>902</fpage>–<lpage>914</lpage>. doi:<pub-id pub-id-type="doi">10.1109/JBHI.2015.2414934</pub-id>.</mixed-citation>
</ref>
<ref id="ref181">
<label>[181]</label><mixed-citation publication-type="other"><string-name><given-names>N.</given-names> <surname>Sprague</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Ballard</surname></string-name>, Multiple-goal reinforcement learning with modular sarsa (0), 2003. doi:<pub-id pub-id-type="doi">10.5555/1630659.1630892</pub-id>.</mixed-citation>
</ref>
<ref id="ref182">
<label>[182]</label><mixed-citation publication-type="other"><string-name><given-names>A.R.</given-names> <surname>Srinivasan</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Chakraborty</surname></string-name>, <chapter-title>Path planning with user route preference – A reward surface approximation approach using orthogonal Legendre polynomials</chapter-title>, in: <source>2016 IEEE International Conference on Automation Science and Engineering (CASE)</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.1109/COASE.2016.7743527</pub-id>.</mixed-citation>
</ref>
<ref id="ref183">
<label>[183]</label><mixed-citation publication-type="other"><string-name><given-names>A.</given-names> <surname>Srivihok</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Sukonmanee</surname></string-name>, <chapter-title>Intelligent agent for e-tourism: Personalization travel support agent using reinforcement learning</chapter-title>, in: <source>WWW 2005</source>, <year>2005</year>. <uri>http://ceur-ws.org/Vol-143/paper12.pdf</uri>.</mixed-citation>
</ref>
<ref id="ref184">
<label>[184]</label><mixed-citation publication-type="other"><string-name><given-names>P.</given-names> <surname>Su</surname></string-name>, <string-name><given-names>Y.-B.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Yu</surname></string-name> and <string-name><given-names>L.</given-names> <surname>Lee</surname></string-name>, <chapter-title>A dialogue game framework with personalized training using reinforcement learning for computer-assisted language learning</chapter-title>, in: <source>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</source>, <year>2013</year>. doi:<pub-id pub-id-type="doi">10.1109/ICASSP.2013.6639266</pub-id>.</mixed-citation>
</ref>
<ref id="ref185">
<label>[185]</label><mixed-citation publication-type="other"><string-name><given-names>P.-H.</given-names> <surname>Su</surname></string-name>, <string-name><given-names>C.-H.</given-names> <surname>Wu</surname></string-name> and <string-name><given-names>L.-S.</given-names> <surname>Lee</surname></string-name>, <chapter-title>A recursive dialogue game for personalized computer-aided pronunciation training</chapter-title>, in: <source>IEEE/ACM Transactions on Audio, Speech, and Language Processing</source>, <year>2014</year>. doi:<pub-id pub-id-type="doi">10.1109/TASLP.2014.2375572</pub-id>.</mixed-citation>
</ref>
<ref id="ref186">
<label>[186]</label><mixed-citation publication-type="chapter"><string-name><given-names>R.S.</given-names> <surname>Sutton</surname></string-name>, <chapter-title>Integrated architectures for learning, planning, and reacting based on approximating dynamic programming</chapter-title>, in: <source>Machine Learning Proceedings 1990</source>, <publisher-name>Elsevier</publisher-name>, <year>1990</year>, pp. <fpage>216</fpage>–<lpage>224</lpage>. doi:<pub-id pub-id-type="doi">10.1016/B978-1-55860-141-3.50030-4</pub-id>.</mixed-citation>
</ref>
<ref id="ref187">
<label>[187]</label><mixed-citation publication-type="chapter"><string-name><given-names>R.S.</given-names> <surname>Sutton</surname></string-name>, <chapter-title>Generalization in reinforcement learning: Successful examples using sparse coarse coding</chapter-title>, in: <source>Advances in Neural Information Processing Systems</source>, <year>1996</year>, pp. <fpage>1038</fpage>–<lpage>1044</lpage>. doi:<pub-id pub-id-type="doi">10.5555/2998828.2998974</pub-id>.</mixed-citation>
</ref>
<ref id="ref188">
<label>[188]</label><mixed-citation publication-type="other"><string-name><given-names>R.S.</given-names> <surname>Sutton</surname></string-name> and <string-name><given-names>A.G.</given-names> <surname>Barto</surname></string-name>, <source>Reinforcement Learning: An Introduction</source>, <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, MA</publisher-loc>, <year>2018</year>. ISBN <isbn>9780262193986</isbn>.</mixed-citation>
</ref>
<ref id="ref189">
<label>[189]</label><mixed-citation publication-type="chapter"><string-name><given-names>R.S.</given-names> <surname>Sutton</surname></string-name>, <string-name><given-names>D.A.</given-names> <surname>McAllester</surname></string-name>, <string-name><given-names>S.P.</given-names> <surname>Singh</surname></string-name> and <string-name><given-names>Y.</given-names> <surname>Mansour</surname></string-name>, <chapter-title>Policy gradient methods for reinforcement learning with function approximation</chapter-title>, in: <source>Advances in Neural Information Processing Systems</source>, <year>2000</year>, pp. <fpage>1057</fpage>–<lpage>1063</lpage>. doi:<pub-id pub-id-type="doi">10.5555/3009657.3009806</pub-id>.</mixed-citation>
</ref>
<ref id="ref190">
<label>[190]</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Szepesvári</surname></string-name>, <article-title>Algorithms for reinforcement learning</article-title>, <source>Synthesis Lectures on Artificial Intelligence and Machine Learning</source> <volume>4</volume>(<issue>1</issue>) (<year>2010</year>), <fpage>1</fpage>–<lpage>103</lpage>. doi:<pub-id pub-id-type="doi">10.2200/S00268ED1V01Y201005AIM009</pub-id>.</mixed-citation>
</ref>
<ref id="ref191">
<label>[191]</label><mixed-citation publication-type="chapter"><string-name><given-names>S.A.</given-names> <surname>Tabatabaei</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hoogendoorn</surname></string-name> and <string-name><given-names>A.</given-names> <surname>van Halteren</surname></string-name>, <chapter-title>Narrowing reinforcement learning: Overcoming the cold start problem for personalized health interventions</chapter-title>, in: <source>International Conference on Principles and Practice of Multi-Agent Systems</source>, <publisher-name>Springer</publisher-name>, <year>2018</year>, pp. <fpage>312</fpage>–<lpage>327</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-030-03098-8_19</pub-id>.</mixed-citation>
</ref>
<ref id="ref192">
<label>[192]</label><mixed-citation publication-type="other"><string-name><given-names>N.</given-names> <surname>Taghipour</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Kardan</surname></string-name>, <chapter-title>A hybrid web recommender system based on Q-learning</chapter-title>, in: <source>Proceedings of the 2008 ACM Symposium on Applied Computing – SAC ’08</source>, <year>2008</year>. doi:<pub-id pub-id-type="doi">10.1145/1363686.1363954</pub-id>.</mixed-citation>
</ref>
<ref id="ref193">
<label>[193]</label><mixed-citation publication-type="other"><string-name><given-names>N.</given-names> <surname>Taghipour</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Kardan</surname></string-name> and <string-name><given-names>S.S.</given-names> <surname>Ghidary</surname></string-name>, <chapter-title>Usage-based web recommendations</chapter-title>, in: <source>Proceedings of the 2007 ACM Conference on Recommender Systems – RecSys ’07</source>, <year>2007</year>. doi:<pub-id pub-id-type="doi">10.1145/1297231.1297250</pub-id>.</mixed-citation>
</ref>
<ref id="ref194">
<label>[194]</label><mixed-citation publication-type="other"><string-name><given-names>L.</given-names> <surname>Tang</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Jiang</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Li</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Li</surname></string-name>, <chapter-title>Ensemble contextual bandits for personalized recommendation</chapter-title>, in: <source>Proceedings of the 8th ACM Conference on Recommender Systems – RecSys ’14</source>, <year>2014</year>. doi:<pub-id pub-id-type="doi">10.1145/2645710.2645732</pub-id>.</mixed-citation>
</ref>
<ref id="ref195">
<label>[195]</label><mixed-citation publication-type="other"><string-name><given-names>L.</given-names> <surname>Tang</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Jiang</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Zeng</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Li</surname></string-name>, <chapter-title>Personalized recommendation via parameter-free contextual bandits</chapter-title>, in: <source>Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval – SIGIR ’15</source>, <year>2015</year>. doi:<pub-id pub-id-type="doi">10.1145/2766462.2767707</pub-id>.</mixed-citation>
</ref>
<ref id="ref196">
<label>[196]</label><mixed-citation publication-type="other"><string-name><given-names>L.</given-names> <surname>Tang</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Rosales</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Singh</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Agarwal</surname></string-name>, <chapter-title>Automatic ad format selection via contextual bandits</chapter-title>, in: <source>Proceedings of the 22nd ACM International Conference on Conference on Information &amp; Knowledge Management – CIKM ’13</source>, <year>2013</year>. doi:<pub-id pub-id-type="doi">10.1145/2505515.2514700</pub-id>.</mixed-citation>
</ref>
<ref id="ref197">
<label>[197]</label><mixed-citation publication-type="chapter"><string-name><given-names>M.</given-names> <surname>Tavakol</surname></string-name> and <string-name><given-names>U.</given-names> <surname>Brefeld</surname></string-name>, <chapter-title>A unified contextual bandit framework for long- and short-term recommendations</chapter-title>, in: <source>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</source>, <series>Lecture Notes in Computer Science</series>, Vol. <volume>10535</volume>, <year>2017</year>, pp. <fpage>269</fpage>–<lpage>284</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-319-71246-8_17</pub-id>.</mixed-citation>
</ref>
<ref id="ref198">
<label>[198]</label><mixed-citation publication-type="other"><string-name><given-names>B.</given-names> <surname>Tegelund</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Son</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Lee</surname></string-name>, <chapter-title>A task-oriented service personalization scheme for smart environments using reinforcement learning</chapter-title>, in: <source>2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.1109/PERCOMW.2016.7457110</pub-id>.</mixed-citation>
</ref>
<ref id="ref199">
<label>[199]</label><mixed-citation publication-type="journal"><string-name><given-names>G.</given-names> <surname>Tesauro</surname></string-name>, <article-title>Temporal difference learning and TD-Gammon</article-title>, <source>Communications of the ACM</source> <volume>38</volume>(<issue>3</issue>) (<year>1995</year>), <fpage>58</fpage>–<lpage>68</lpage>. doi:<pub-id pub-id-type="doi">10.1145/203330.203343</pub-id>.</mixed-citation>
</ref>
<ref id="ref200">
<label>[200]</label><mixed-citation publication-type="other"><string-name><given-names>G.</given-names> <surname>Theocharous</surname></string-name>, <string-name><given-names>P.S.</given-names> <surname>Thomas</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Ghavamzadeh</surname></string-name>, <chapter-title>Personalized ad recommendation systems for life-time value optimization with guarantees</chapter-title>, in: <source>Twenty-Fourth International Joint Conference on Artificial Intelligence</source>, <year>2015</year>. doi:<pub-id pub-id-type="doi">10.5555/2832415.2832500</pub-id>.</mixed-citation>
</ref>
<ref id="ref201">
<label>[201]</label><mixed-citation publication-type="other"><string-name><given-names>G.</given-names> <surname>Theocharous</surname></string-name>, <string-name><given-names>P.S.</given-names> <surname>Thomas</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Ghavamzadeh</surname></string-name>, <chapter-title>Ad recommendation systems for life-time value optimization</chapter-title>, in: <source>Proceedings of the 24th International Conference on World Wide Web – WWW ’15 Companion</source>, <year>2015</year>. doi:<pub-id pub-id-type="doi">10.1145/2740908.2741998</pub-id>.</mixed-citation>
</ref>
<ref id="ref202">
<label>[202]</label><mixed-citation publication-type="chapter"><string-name><given-names>P.</given-names> <surname>Thomas</surname></string-name> and <string-name><given-names>E.</given-names> <surname>Brunskill</surname></string-name>, <chapter-title>Data-efficient off-policy policy evaluation for reinforcement learning</chapter-title>, in: <source>International Conference on Machine Learning</source>, <year>2016</year>, pp. <fpage>2139</fpage>–<lpage>2148</lpage>. <uri>http://proceedings.mlr.press/v48/thomasa16.html</uri>.</mixed-citation>
</ref>
<ref id="ref203">
<label>[203]</label><mixed-citation publication-type="other"><string-name><given-names>P.S.</given-names> <surname>Thomas</surname></string-name>, Safe reinforcement learning, 2015. <uri>https://scholarworks.umass.edu/dissertations_2/514</uri>.</mixed-citation>
</ref>
<ref id="ref204">
<label>[204]</label><mixed-citation publication-type="other"><string-name><given-names>P.S.</given-names> <surname>Thomas</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Theocharous</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Ghavamzadeh</surname></string-name>, <chapter-title>High-confidence off-policy evaluation</chapter-title>, in: <source>Twenty-Ninth AAAI Conference on Artificial Intelligence</source>, <year>2015</year>. doi:<pub-id pub-id-type="doi">10.5555/2888116.2888134</pub-id>.</mixed-citation>
</ref>
<ref id="ref205">
<label>[205]</label><mixed-citation publication-type="chapter"><string-name><given-names>S.</given-names> <surname>Triki</surname></string-name> and <string-name><given-names>C.</given-names> <surname>Hanachi</surname></string-name>, <chapter-title>A self-adaptive system for improving autonomy and public spaces accessibility for elderly</chapter-title>, in: <source>Smart Innovation, Systems and Technologies</source>, <year>2017</year>, pp. <fpage>53</fpage>–<lpage>66</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-319-59394-4_6</pub-id>.</mixed-citation>
</ref>
<ref id="ref206">
<label>[206]</label><mixed-citation publication-type="journal"><string-name><given-names>H.-H.</given-names> <surname>Tseng</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Luo</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Cui</surname></string-name>, <string-name><given-names>J.-T.</given-names> <surname>Chien</surname></string-name>, <string-name><given-names>R.K.</given-names> <surname>Ten Haken</surname></string-name> and <string-name><given-names>I.E.</given-names> <surname>Naqa</surname></string-name>, <article-title>Deep reinforcement learning for automated radiation adaptation in lung cancer</article-title>, <source>Medical Physics</source> <volume>44</volume>(<issue>12</issue>) (<year>2017</year>), <fpage>6690</fpage>–<lpage>6705</lpage>. doi:<pub-id pub-id-type="doi">10.1002/mp.12625</pub-id>.</mixed-citation>
</ref>
<ref id="ref207">
<label>[207]</label><mixed-citation publication-type="other"><string-name><given-names>K.</given-names> <surname>Tsiakas</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Abellanoza</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Makedon</surname></string-name>, <chapter-title>Interactive learning and adaptation for robot assisted therapy for people with dementia</chapter-title>, in: <source>Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments – PETRA ’16</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.1145/2910674.2935849</pub-id>.</mixed-citation>
</ref>
<ref id="ref208">
<label>[208]</label><mixed-citation publication-type="other"><string-name><given-names>K.</given-names> <surname>Tsiakas</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Huber</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Makedon</surname></string-name>, <chapter-title>A multimodal adaptive session manager for physical rehabilitation exercising</chapter-title>, in: <source>Proceedings of the 8th ACM International Conference on PErvasive Technologies Related to Assistive Environments – PETRA ’15</source>, <year>2015</year>. doi:<pub-id pub-id-type="doi">10.1145/2769493.2769507</pub-id>.</mixed-citation>
</ref>
<ref id="ref209">
<label>[209]</label><mixed-citation publication-type="other"><string-name><given-names>K.</given-names> <surname>Tsiakas</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Papakostas</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Chebaa</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Ebert</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Karkaletsis</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Makedon</surname></string-name>, <chapter-title>An interactive learning and adaptation framework for adaptive robot assisted therapy</chapter-title>, in: <source>Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments – PETRA ’16</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.1145/2910674.2935857</pub-id>.</mixed-citation>
</ref>
<ref id="ref210">
<label>[210]</label><mixed-citation publication-type="other"><string-name><given-names>K.</given-names> <surname>Tsiakas</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Papakostas</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Theofanidis</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Bell</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Mihalcea</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Burzo</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Makedon</surname></string-name>, <chapter-title>An interactive multisensing framework for personalized human robot collaboration and assistive training using reinforcement learning</chapter-title>, in: <source>Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments – PETRA ’17</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1145/3056540.3076191</pub-id>.</mixed-citation>
</ref>
<ref id="ref211">
<label>[211]</label><mixed-citation publication-type="other"><string-name><given-names>D.</given-names> <surname>Urieli</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Stone</surname></string-name>, <chapter-title>TacTex’13: A champion adaptive power trading agent</chapter-title>, in: <source>AAMAS</source>, <year>2014</year>. doi:<pub-id pub-id-type="doi">10.5555/2615731.2617516</pub-id>.</mixed-citation>
</ref>
<ref id="ref212">
<label>[212]</label><mixed-citation publication-type="other"><string-name><given-names>H.</given-names> <surname>Van Hasselt</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Guez</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Silver</surname></string-name>, <chapter-title>Deep reinforcement learning with double q-learning</chapter-title>, in: <source>Thirtieth AAAI Conference on Artificial Intelligence</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.5555/3016100.3016191</pub-id>.</mixed-citation>
</ref>
<ref id="ref213">
<label>[213]</label><mixed-citation publication-type="other"><string-name><given-names>G.</given-names> <surname>Vasan</surname></string-name> and <string-name><given-names>P.M.</given-names> <surname>Pilarski</surname></string-name>, <chapter-title>Learning from demonstration: Teaching a myoelectric prosthesis with an intact limb via reinforcement learning</chapter-title>, in: <source>2017 International Conference on Rehabilitation Robotics (ICORR)</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1109/ICORR.2017.8009453</pub-id>.</mixed-citation>
</ref>
<ref id="ref214">
<label>[214]</label><mixed-citation publication-type="other"><string-name><given-names>L.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Gao</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Cao</surname></string-name> and <string-name><given-names>L.</given-names> <surname>Wang</surname></string-name>, <chapter-title>Towards a general supporting framework for self-adaptive software systems</chapter-title>, in: <source>2012 IEEE 36th Annual Computer Software and Applications Conference Workshops</source>, <year>2012</year>. doi:<pub-id pub-id-type="doi">10.1109/COMPSACW.2012.38</pub-id>.</mixed-citation>
</ref>
<ref id="ref215">
<label>[215]</label><mixed-citation publication-type="chapter"><string-name><given-names>P.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Rowe</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Mott</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Lester</surname></string-name>, <chapter-title>Decomposing drama management in educational interactive narrative: A modular reinforcement learning approach</chapter-title>, in: <source>International Conference on Interactive Digital Storytelling</source>, <series>Lecture Notes in Computer Science</series>, Vol. <volume>10045</volume>, <year>2016</year>, pp. <fpage>270</fpage>–<lpage>282</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-319-48279-8_24</pub-id>.</mixed-citation>
</ref>
<ref id="ref216">
<label>[216]</label><mixed-citation publication-type="other"><string-name><given-names>P.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>J.P.</given-names> <surname>Rowe</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Min</surname></string-name>, <string-name><given-names>B.W.</given-names> <surname>Mott</surname></string-name> and <string-name><given-names>J.C.</given-names> <surname>Lester</surname></string-name>, <chapter-title>Interactive narrative personalization with deep reinforcement learning</chapter-title>, in: <source>IJCAI</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.5555/3172077.3172427</pub-id>.</mixed-citation>
</ref>
<ref id="ref217">
<label>[217]</label><mixed-citation publication-type="other"><string-name><given-names>X.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Hsu</surname></string-name> and <string-name><given-names>Y.</given-names> <surname>Wang</surname></string-name>, <article-title>Exploration in interactive personalized music recommendation: A reinforcement learning approach</article-title>, <source>ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</source> <volume>11</volume>(<issue>1</issue>) (<year>2014</year>), <elocation-id>7</elocation-id>. doi:<pub-id pub-id-type="doi">10.1145/2648583</pub-id>.</mixed-citation>
</ref>
<ref id="ref218">
<label>[218]</label><mixed-citation publication-type="other"><string-name><given-names>X.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Ren</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Ito</surname></string-name>, <chapter-title>GongBroker: A broker model for power trading in smart grid markets</chapter-title>, in: <source>2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)</source>, <year>2015</year>. doi:<pub-id pub-id-type="doi">10.1109/WI-IAT.2015.108</pub-id>.</mixed-citation>
</ref>
<ref id="ref219">
<label>[219]</label><mixed-citation publication-type="journal"><string-name><given-names>C.J.</given-names> <surname>Watkins</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Dayan</surname></string-name>, <article-title>Q-learning</article-title>, <source>Machine Learning</source> <volume>8</volume>(<issue>3–4</issue>) (<year>1992</year>), <fpage>279</fpage>–<lpage>292</lpage>. doi:<pub-id pub-id-type="doi">10.1007/BF00992698</pub-id>.</mixed-citation>
</ref>
<ref id="ref220">
<label>[220]</label><mixed-citation publication-type="other"><string-name><given-names>M.</given-names> <surname>Wiering</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Van Otterlo</surname></string-name>, <chapter-title>Reinforcement learning</chapter-title>, in: <source>Reinforcement Learning</source>, <string-name><given-names>M.</given-names> <surname>Wiering</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Van Otterlo</surname></string-name>, eds, <series>Adaptation, Learning, and Optimization</series>, Vol. <volume>12</volume>, <publisher-name>Springer</publisher-name>, <year>2012</year>. doi:<pub-id pub-id-type="doi">10.1007/978-3-642-27645-3</pub-id>.</mixed-citation>
</ref>
<ref id="ref221">
<label>[221]</label><mixed-citation publication-type="other"><string-name><given-names>G.</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Luo</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Zhang</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Fu</surname></string-name>, <chapter-title>Data-driven inverse learning of passenger preferences in urban public transits</chapter-title>, in: <source>2017 IEEE 56th Annual Conference on Decision and Control (CDC)</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1109/CDC.2017.8264410</pub-id>.</mixed-citation>
</ref>
<ref id="ref222">
<label>[222]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Xing</surname></string-name> and <string-name><given-names>M.</given-names> <surname>van der Schaar</surname></string-name>, <article-title>Personalized course sequence recommendations</article-title>, <source>IEEE Transactions on Signal Processing</source> <volume>64</volume>(<issue>20</issue>) (<year>2016</year>), <fpage>5340</fpage>–<lpage>5352</lpage>. doi:<pub-id pub-id-type="doi">10.1109/TSP.2016.2595495</pub-id>.</mixed-citation>
</ref>
<ref id="ref223">
<label>[223]</label><mixed-citation publication-type="chapter"><string-name><given-names>M.</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Qu</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Lei</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Zhu</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Chen</surname></string-name> and <string-name><given-names>J.Z.</given-names> <surname>Huang</surname></string-name>, <chapter-title>Investigating deep reinforcement learning techniques in personalized dialogue generation</chapter-title>, in: <source>Proceedings of the 2018 SIAM International Conference on Data Mining</source>, <publisher-name>SIAM</publisher-name>, <year>2018</year>, pp. <fpage>630</fpage>–<lpage>638</lpage>. doi:<pub-id pub-id-type="doi">10.1137/1.9781611975321.71</pub-id>.</mixed-citation>
</ref>
<ref id="ref224">
<label>[224]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Tu</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Qu</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Chen</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Zhu</surname></string-name>, <article-title>Personalized response generation by dual-learning based domain adaptation</article-title>, <source>Neural Networks</source> <volume>103</volume> (<year>2018</year>), <fpage>72</fpage>–<lpage>82</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neunet.2018.03.009</pub-id>.</mixed-citation>
</ref>
<ref id="ref225">
<label>[225]</label><mixed-citation publication-type="other"><string-name><given-names>M.</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Zhu</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Zhou</surname></string-name> and <string-name><given-names>Z.</given-names> <surname>Cao</surname></string-name>, <chapter-title>Personalized response generation via domain adaptation</chapter-title>, in: <source>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval – SIGIR ’17</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1145/3077136.3080706</pub-id>.</mixed-citation>
</ref>
<ref id="ref226">
<label>[226]</label><mixed-citation publication-type="journal"><string-name><given-names>S.-T.</given-names> <surname>Yuan</surname></string-name>, <article-title>A personalized and integrative comparison-shopping engine and its applications</article-title>, <source>Decision Support Systems</source> <volume>34</volume>(<issue>2</issue>) (<year>2003</year>), <fpage>139</fpage>–<lpage>156</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0167-9236(02)00077-5</pub-id>.</mixed-citation>
</ref>
<ref id="ref227">
<label>[227]</label><mixed-citation publication-type="chapter"><string-name><given-names>Y.</given-names> <surname>Yue</surname></string-name>, <string-name><given-names>S.A.</given-names> <surname>Hong</surname></string-name> and <string-name><given-names>C.</given-names> <surname>Guestrin</surname></string-name>, <chapter-title>Hierarchical exploration for accelerating contextual bandits</chapter-title>, in: <source>Proceedings of the 29th International Coference on International Conference on Machine Learning</source>, <publisher-name>Omnipress</publisher-name>, <year>2012</year>, pp. <fpage>979</fpage>–<lpage>986</lpage>. doi:<pub-id pub-id-type="doi">10.5555/3042573.3042700</pub-id>.</mixed-citation>
</ref>
<ref id="ref228">
<label>[228]</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Zaidenberg</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Reignier</surname></string-name>, <chapter-title>Reinforcement learning of user preferences for a ubiquitous personal assistant</chapter-title>, in: <source>Advances in Reinforcement Learning</source>, <publisher-name>IntechOpen</publisher-name>, <year>2011</year>. doi:<pub-id pub-id-type="doi">10.5772/13723</pub-id>.</mixed-citation>
</ref>
<ref id="ref229">
<label>[229]</label><mixed-citation publication-type="chapter"><string-name><given-names>S.</given-names> <surname>Zaidenberg</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Reignier</surname></string-name> and <string-name><given-names>J.L.</given-names> <surname>Crowley</surname></string-name>, <chapter-title>Reinforcement learning of context models for a ubiquitous personal assistant</chapter-title>, in: <source>3rd Symposium of Ubiquitous Computing and Ambient Intelligence 2008</source>, <year>2008</year>, pp. <fpage>254</fpage>–<lpage>264</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-540-85867-6_30</pub-id>.</mixed-citation>
</ref>
<ref id="ref230">
<label>[230]</label><mixed-citation publication-type="other"><string-name><given-names>C.</given-names> <surname>Zeng</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Mokhtari</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Li</surname></string-name>, <chapter-title>Online context-aware recommendation with time varying multi-armed bandit</chapter-title>, in: <source>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining – KDD ’16</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.1145/2939672.2939878</pub-id>.</mixed-citation>
</ref>
<ref id="ref231">
<label>[231]</label><mixed-citation publication-type="journal"><string-name><given-names>B.-T.</given-names> <surname>Zhang</surname></string-name> and <string-name><given-names>Y.-W.</given-names> <surname>Seo</surname></string-name>, <article-title>Personalized web-document filtering using reinforcement learning</article-title>, <source>Applied Artificial Intelligence</source> <volume>15</volume>(<issue>7</issue>) (<year>2001</year>), <fpage>665</fpage>–<lpage>685</lpage>. doi:<pub-id pub-id-type="doi">10.1080/088395101750363993</pub-id>.</mixed-citation>
</ref>
<ref id="ref232">
<label>[232]</label><mixed-citation publication-type="other"><string-name><given-names>Y.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Tang</surname></string-name>, <string-name><given-names>W.F.</given-names> <surname>Stewart</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Sun</surname></string-name>, <chapter-title>LEAP: Learning to prescribe effective and safe treatment combinations for multimorbidity</chapter-title>, in: <source>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining – KDD ’17</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1145/3097983.3098109</pub-id>.</mixed-citation>
</ref>
<ref id="ref233">
<label>[233]</label><mixed-citation publication-type="chapter"><string-name><given-names>T.</given-names> <surname>Zhao</surname></string-name> and <string-name><given-names>I.</given-names> <surname>King</surname></string-name>, <chapter-title>Locality-sensitive linear bandit model for online social recommendation</chapter-title>, in: <source>International Conference on Neural Information Processing</source>, <series>Lecture Notes in Computer Science</series>, Vol. <volume>9947</volume>, <year>2016</year>, pp. <fpage>80</fpage>–<lpage>90</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-319-46687-3_9</pub-id>.</mixed-citation>
</ref>
<ref id="ref234">
<label>[234]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>M.R.</given-names> <surname>Kosorok</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Zeng</surname></string-name>, <article-title>Reinforcement learning design for cancer clinical trials</article-title>, <source>Statistics in Medicine</source> <volume>28</volume>(<issue>26</issue>) (<year>2009</year>), <fpage>3294</fpage>–<lpage>3315</lpage>. doi:<pub-id pub-id-type="doi">10.1002/sim.3720</pub-id>.</mixed-citation>
</ref>
<ref id="ref235">
<label>[235]</label><mixed-citation publication-type="other"><string-name><given-names>Y.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Zou</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Ng</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Ng</surname></string-name>, <chapter-title>Automatically learning user preferences for personalized service composition</chapter-title>, in: <source>2017 IEEE International Conference on Web Services (ICWS)</source>, <year>2017</year>. doi:<pub-id pub-id-type="doi">10.1109/ICWS.2017.93</pub-id>.</mixed-citation>
</ref>
<ref id="ref236">
<label>[236]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Zeng</surname></string-name>, <string-name><given-names>M.A.</given-names> <surname>Socinski</surname></string-name> and <string-name><given-names>M.R.</given-names> <surname>Kosorok</surname></string-name>, <article-title>Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer</article-title>, <source>Biometrics</source> <volume>67</volume>(<issue>4</issue>) (<year>2011</year>), <fpage>1422</fpage>–<lpage>1433</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1541-0420.2011.01572.x</pub-id>.</mixed-citation>
</ref>
<ref id="ref237">
<label>[237]</label><mixed-citation publication-type="other"><string-name><given-names>Y.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Xia</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Cheng</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Wang</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Song</surname></string-name>, <chapter-title>A unified control framework of HVAC system for thermal and acoustic comforts in office building</chapter-title>, in: <source>2013 IEEE International Conference on Automation Science and Engineering (CASE)</source>, <year>2013</year>. doi:<pub-id pub-id-type="doi">10.1109/CoASE.2013.6653964</pub-id>.</mixed-citation>
</ref>
<ref id="ref238">
<label>[238]</label><mixed-citation publication-type="other"><string-name><given-names>G.</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Xiang</surname></string-name>, <string-name><given-names>N.J.</given-names> <surname>Yuan</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Xie</surname></string-name> and <string-name><given-names>Z.</given-names> <surname>Li</surname></string-name>, <chapter-title>DRN: A deep reinforcement learning framework for news recommendation</chapter-title>, in: <source>Proceedings of the 2018 World Wide Web Conference on World Wide Web – WWW ’18</source>, <year>2018</year>. doi:<pub-id pub-id-type="doi">10.1145/3178876.3185994</pub-id>.</mixed-citation>
</ref>
<ref id="ref239">
<label>[239]</label><mixed-citation publication-type="other"><string-name><given-names>H.</given-names> <surname>Zheng</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Jumadinova</surname></string-name>, <chapter-title>OWLS: Observational wireless life-enhancing system (extended abstract)</chapter-title>, in: <source>AAMAS</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.5555/2936924.2937192</pub-id>.</mixed-citation>
</ref>
<ref id="ref240">
<label>[240]</label><mixed-citation publication-type="other"><string-name><given-names>L.</given-names> <surname>Zhou</surname></string-name> and <string-name><given-names>E.</given-names> <surname>Brunskill</surname></string-name>, <chapter-title>Latent contextual bandits and their application to personalized recommendations for new users</chapter-title>, in: <source>IJCAI</source>, <year>2016</year>. doi:<pub-id pub-id-type="doi">10.5555/3061053.3061129</pub-id>.</mixed-citation>
</ref>
<ref id="ref241">
<label>[241]</label><mixed-citation publication-type="other"><string-name><given-names>M.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>Y.D.</given-names> <surname>Mintz</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Fukuoka</surname></string-name>, <string-name><given-names>K.Y.</given-names> <surname>Goldberg</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Flowers</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Kaminsky</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Castillejo</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Aswani</surname></string-name>, <chapter-title>Personalizing mobile fitness apps using reinforcement learning</chapter-title>, in: <source>IUI Workshops</source>, <year>2018</year>. <uri>http://ceur-ws.org/Vol-2068/humanize7.pdf</uri>.</mixed-citation>
</ref>
<ref id="ref242">
<label>[242]</label><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Zhu</surname></string-name>, <string-name><given-names>Y.-Q.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Ma</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Zhao</surname></string-name>, <article-title>Greedy outcome weighted tree learning of optimal personalized treatment rules</article-title>, <source>Biometrics</source> <volume>73</volume>(<issue>2</issue>) (<year>2016</year>), <fpage>391</fpage>–<lpage>400</lpage>. doi:<pub-id pub-id-type="doi">10.1111/biom.12593</pub-id>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>
